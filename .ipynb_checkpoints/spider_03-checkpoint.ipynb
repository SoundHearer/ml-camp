{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 本次课程scrapy项目代码在sc_demo.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 用户名/密码登陆\n",
    "# 使用seleniumn模拟用户名/密码输入的时候，很多工作浏览器调用js完成。\n",
    "# 使用requests虽然更高效，但是需要处理的工作也更多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 豆瓣模拟登陆\n",
    "# 使用浏览器开发者工具查看登陆调用的接口与提交的表单信息\n",
    "# 如果有验证码，并不像selenium一样直接填了验证码就好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 豆瓣模拟登陆，使用cookie。\n",
    "# 使用cookie登陆\n",
    "cookies = {'cookie': 'gr_user_id=0b04b3f6-fab4-4b70-86df-38982c6d0535; __yadk_uid=6X9uTW4XzeoQErRmSRzwy0DuKnot3o6l; bid=cQHg3XeWrzU; viewed=\"5338399_26859889_25923638_3081930_3924175_20471211_1057390_26829015_25716519_3715623\"; _vwo_uuid_v2=64E0E442544CB2FE2D322C59F01F1115|026be912d24071903cb0ed891ae9af65; ll=\"108296\"; _ga=GA1.2.1329310863.1477654711; ap=1; push_noty_num=0; push_doumail_num=0; __utmz=30149280.1513506485.88.52.utmcsr=google|utmccn=(organic)|utmcmd=organic|utmctr=(not%20provided); __utmc=30149280; _gid=GA1.2.989173222.1517394860; ps=y; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1517401258%2C%22https%3A%2F%2Fwww.google.com.sg%2F%22%5D; _pk_ses.100001.8cb4=*; __utma=30149280.1329310863.1477654711.1517394849.1517401259.93; __utmt=1; ue=\"t.t.panda@hotmail.com\"; dbcl2=\"2625855:0e8GlBEqJ54\"; ck=XLba; _pk_id.100001.8cb4=40c3cee75022c8e1.1477654710.69.1517401441.1517394879.; __utmv=30149280.262; __utmb=30149280.4.10.1517401259; ct=y'}\n",
    "url = 'http://www.douban.com'\n",
    "resp = requests.get(url, cookies=cookies, headers=headers)\n",
    "with open('douban.txt', 'w+', encoding='utf-8') as f:\n",
    "    f.write(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapy框架\n",
    "# 为什么需要爬虫框架\n",
    "# - 提供并整合了实现爬虫的基础服务：HTTP请求发送，网页解析，底层的并发和异步支持。\n",
    "# - 提供了各种接口供用户扩展和定制功能，比如管道和中间件。\n",
    "# 安装scrapy如果遇到依赖库因为需要编译等原因安装失败，\n",
    "# Windows用户可以直接去https://www.lfd.uci.edu/~gohlke/pythonlibs/下载安装包。\n",
    "# 或者直接下载https://download.lfd.uci.edu/pythonlibs/n1rrk3iq/Scrapy-1.5.0-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapy常用命令\n",
    "# 新建项目：scrapy startproject projectname\n",
    "# 新建爬虫：scrapy genspider spidername domainname\n",
    "# - spidername是爬虫名称，会在spiders目录下生成对应的文件spidername.py。\n",
    "# - domainname是该爬虫爬取时的域名限制。\n",
    "# 执行爬虫：scrapy crawl spidername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scrapy genspider baidu www.baidu.com爬虫文件示范\n",
    "# baidu.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class BaiduSpider(scrapy.Spider):\n",
    "    name = 'baidu'\n",
    "    allowed_domains = ['www.baidu.com'] # 用来检查url域名是否满足要求\n",
    "    start_urls = ['http://www.baidu.com/'] # 要爬的url数组\n",
    "\n",
    "    # 有同学可能会问，有了start_urls，谁来负责发送请求，怎么就直接到parse了？\n",
    "    # 注意，框架的作用就是把很多日常工作封装掉，如果只是发送请求这么简单的事，框架就处理了。\n",
    "    # 那么有同学会继续问，我怎么知道这个response是那个url的啊？response有个url属性，看一下就知道了。\n",
    "    # 当然这个请求也可以自己控制，后面演示代码会处理。\n",
    "    def parse(self, response):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 如何自己处理请求\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'My'\n",
    "    allowed_domains = ['my.com']\n",
    "    \n",
    "    def start_requests(self): # 不要试图改名字\n",
    "        urls = ['a.my.com', 'b.my.com']\n",
    "        for url in urls:\n",
    "            # yield说明请求是异步的\n",
    "            yield Scrapy.Request(url, callback=self.parse) # 其他更多信息可以通过meta参数传递\n",
    "\n",
    "    def parse(self, response): # Scrapy.Request(...)的结果通过callback参数设置的回调函数跑到这里\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 例子，zipcode用Scrapy来写\n",
    "# scrapy genspider zip_code zip_code生成爬虫zip_code.py\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class ZipCodeSpider(scrapy.Spider):\n",
    "    name = 'zip_code'\n",
    "    allowed_domains = ['ip138.com']\n",
    "    start_urls = ['http://www.ip138.com/post']\n",
    "\n",
    "    def parse(self, response):\n",
    "        text = response.text\n",
    "        bs = BeautifulSoup(text, 'html5lib') # 感觉bs4的css定位查询更好用\n",
    "        for item in bs.select('div#newAlexa > table.t4 > tbody > tr > td > a'):\n",
    "            if item.text in ['上海市', '广东省']:\n",
    "                url = 'http://www.ip138.com' + item.get('href')\n",
    "                yield scrapy.Request(url, callback=self.fetch_zipcode, meta={'province': item.text})\n",
    "\n",
    "    def fetch_zipcode(self, response):\n",
    "        province = response.meta['province']\n",
    "        text = response.text\n",
    "        bs = BeautifulSoup(text, 'html5lib')\n",
    "        rows = bs.select('table.t12 > tbody > tr')\n",
    "        for i in range(1, len(rows)):\n",
    "            items = rows[i].select('td')\n",
    "            t_1 = items[0].text.strip()\n",
    "            if t_1:\n",
    "                yield {'province': province, 'dist': t_1, 'zip': items[1].text.strip()}\n",
    "            if len(items) > 4:\n",
    "                t_2 = items[3].text.strip()\n",
    "                if t_2:\n",
    "                    yield {'province': province, 'dist': t_2, 'zip': items[4].text.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 那么问题来了，如果我们数据要保存到csv文件该怎么办？\n",
    "# 利用数据管道可以解决这个问题，在pipelines.py里新加一个类，用于处理上面yield出来的数据。\n",
    "# spider.name这个判断很重要，因为你的确定你要处理的结果是某个特定爬虫扔出来的。\n",
    "class ZipCodePipeline(object):\n",
    "    def __init__(self):\n",
    "        self.filename = 'zip.csv'\n",
    "        self.f = None\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        if spider.name == 'zip_code': # 确认数据来自你要处理的spider\n",
    "            self.f = open(self.filename, 'w+', encoding='utf-8')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        if self.f:\n",
    "            self.f.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if spider.name == 'zip_code':\n",
    "            province = item['province']\n",
    "            dist = item['dist']\n",
    "            zip_ = item['zip']\n",
    "            self.f.write('%s, %s, %s\\n' % (province, dist, zip_))\n",
    "            \n",
    "# 同时在settings.py里打开pipeline的注释，添加如下代码：\n",
    "ITEM_PIPELINES = {\n",
    "#    'jyx.pipelines.SomePipeline': 300,\n",
    "    'jyx.pipelines.ZipCodePipeline': 300\n",
    "}\n",
    "# 数字越小越早执行。\n",
    "# 根据数字排序，得到一个pipelines数组，执行伪代码：\n",
    "# for pl in pipelines:\n",
    "#     item = pl.process_item(item)\n",
    "#     if not item:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scrapy如何整合selenium？\n",
    "# 使用selenium替代scrapy自带的Request，中间件登场了！\n",
    "# 在middlewares.py中添加以下代码\n",
    "from scrapy.http import HtmlResponse\n",
    "# sc_demo根据实际项目名称修改\n",
    "from sc_demo.extras import utils # 在spiders同级目录下建extras目录，放置utils。\n",
    "\n",
    "class SeleniumMiddleWare(object):\n",
    "    def process_request(self, request, spider):\n",
    "        if spider.name == 'xxx': # 你希望用selenium处理请求的爬虫名字\n",
    "            driver = utils.create_chrome_driver()\n",
    "            driver.get(request.url)\n",
    "            response = HtmlResponse(url=request.url)    \n",
    "            response.driver = driver\n",
    "            return response\n",
    "        else:\n",
    "            return None # 交给系统默认处理\n",
    "    \n",
    "# 在settings里打开DOWNLOADER_MIDDLEWARES的注释，添加如下代码。\n",
    "DOWNLOADER_MIDDLEWARES = {\n",
    "#    'sc_demo.middlewares.MyCustomDownloaderMiddleware': 543,\n",
    "    'sc_demo.middlewares.SeleniumMiddleWare': 543\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utils.py内容\n",
    "from selenium import webdriver\n",
    "\n",
    "def sleep(seconds):\n",
    "    time.sleep(seconds)\n",
    "\n",
    "def create_chrome_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # 浏览器开着方便调试\n",
    "    # options.add_argument('--headless')\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument('--ignore-ssl-errors')\n",
    "    driver = webdriver.Chrome(chrome_options=options)\n",
    "    driver.maximize_window()\n",
    "    return driver\n",
    "\n",
    "def find_element_by_css_selector(element, selector):\n",
    "    try:\n",
    "        return element.find_element_by_css_selector(selector)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def find_elements_by_css_selector(element, selector):\n",
    "    try:\n",
    "        return element.find_elements_by_css_selector(selector)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scrapy genspider ferragamo www.ferragamo.cn\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from sc_demo.extras import utils\n",
    "\n",
    "class FerragamoSpider(scrapy.Spider):\n",
    "    name = \"ferragamo\"\n",
    "    allowed_domains = [\"www.ferragamo.cn\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.ferragamo.cn/woman/shoes/moccasins-drivers.html']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        driver = response.driver\n",
    "        products = []\n",
    "        elements = utils.find_elements_by_css_selector(driver, 'article.cpd-product > div > a.ga-product-detail')\n",
    "        for element in elements:\n",
    "            products.append(element.get_attribute('href').strip())\n",
    "        driver.quit()\n",
    "        return {'url': ','.join(products)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 分布式爬虫案例：https://github.com/yingl/sc_fashion\n",
    "# 作业：上面这个爬虫存在一个问题，下拉刷新后有更多数据，要求实现下拉刷新获取所有数据，并且每条url一行写到文本文件保持。\n",
    "#       另外，原来的爬虫输入写在start_urls，现在我要拿一个文本文件作为输入，云平台教师目录下的ferragamo.txt。\n",
    "#       https://www.ferragamo.cn/man/smallleather-1.html商品越拉越多。你怎么知道拉到底了？\n",
    "#       404也要安全的处理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
