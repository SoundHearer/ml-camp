{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL是架在Spark core之上允许我们用SQL去操作大型数据库的高级层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先你需要了解的是，**我们需要在之前的`SparkContext`基础上架上一层`SQLContext`，然后才能完成后续的SQL。**<br>\n",
    "如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyspark",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e80712876fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msqlCtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pyspark"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext, Row\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL里有一种新的RDD，叫做**`SchemaRDD`**，其实你可以把他看作一个table。<br>\n",
    "schema RDD类似一种图表的RDD(想象成Excel那样)，每个item都是一个`Row`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的初始化`SchemaRDD`方法是先用`sc.parallelize`产出一个RDD，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([\n",
    "        Row(name='John', home='Brussels', age=35),\n",
    "        Row(name='Jack', home='Brussels', age=32),\n",
    "        Row(name='Jane', home='Leuven', age=42),\n",
    "        Row(name='Jill', home='Mechelen', age=53),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后通过下面的方式把RDD转成一个SchemaRDD："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaRDD = sqlCtx.inferSchema(rdd)\n",
    "schemaRDD.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，你也可以在Hive表里使用Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚才的table建好后，你就可以在上面跑各种SQL了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at mapPartitions at SerDeUtil.scala:143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgAge = sqlCtx.sql(\n",
    "    \"\"\"SELECT\n",
    "           home,\n",
    "           AVG(age) AS mean\n",
    "       FROM people\n",
    "       GROUP BY home\"\"\"\n",
    ")\n",
    "avgAge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家都知道在一般的数据表里执行SQL之后得到的还是一份表格式的数据，在Spark SQL执行完之后，你得到的依旧是一个SchemaRDD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(home=u'Brussels', mean=33.5),\n",
       " Row(home=u'Mechelen', mean=53.0),\n",
       " Row(home=u'Leuven', mean=42.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgAge.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为SchemaRDD也是一种RDD，所以你之前学到的所有RDD上的transform或者action等operation都可以用，**同时你可以用`row.fieldname`取出来某个field**，如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average age in Brussels is 33.5 years\n",
      "Average age in Mechelen is 53.0 years\n",
      "Average age in Leuven is 42.0 years\n"
     ]
    }
   ],
   "source": [
    "print(avgAge\n",
    "      .map(lambda row: \"Average age in {0} is {1} years\"\n",
    "                        .format(row.home, row.mean))\n",
    "      .reduce(lambda x, y: x + \"\\n\" + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>练习作业</font><br>\n",
    "**回顾一下，我们可以在pandas里定义这样一个dataframe**\n",
    "```\n",
    "data = {\n",
    "    'country': ['BE', 'BE', 'BE', 'NL', 'NL', 'NL'],\n",
    "    'year': [1913, 1950, 2003, 1913, 1950, 2003],\n",
    "    'gdp_per_capita': [4220, 5462, 21205, 4049, 5996, 21480]\n",
    "}\n",
    "frame = DataFrame(data)\n",
    "```\n",
    "**请大家在Spark中生成一个SchemaRDD，再做一个简单的SQL分析：求这3年各首都的平均GDP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次提醒一下：`sqlCtx.sql(\"...\")`得到的结果本身也是一个`SchemaRDD`，所以你也可以把它注册成一张新的表，再上门做后续的操作！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>练习作业</font><br>\n",
    "**`sc.textFile`, `union` 和 `map`练习。 把提供的`names`数据集构建成一个SchemaRDD，包含`year`, `name`, `sex`和`births`，把它注册成一个表`\"names\"`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 求从1939到1945年美国总出生的人口**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 统计从1880到2014每一年叫`\"Mary\"`的宝宝出生数目，并用matplotlib绘制成图像**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 统计从1880到2014每一年男孩和女孩的出生数，并绘制在一张图中。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 统计出来每一年出生的宝宝频次最高的前1000个名字，以及它们的占比，并用绘图的方式去展示出来**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多知识\n",
    "到此为止，应该Spark核心知识与Spark SQL知识你都有一些了解了，其实有更多的Spark SQL知识，比如:\n",
    "* Complex Joins\n",
    "* Subqueries\n",
    "* Basic math functions\n",
    "* User-defined functions (UDFs)\n",
    "* ...\n",
    "\n",
    "完整的文档可以看[这里](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#compatibility-with-apache-hive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
