{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第六周(机器学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月16日至3月18日期间完成，最晚提交时间本周日（3月18日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam6后，进行作答。例如wangwei-exam6\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>彭志雄</u>  \n",
    "- 批改人： David\n",
    "- 最终得分: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共4题，每题5分，共计20分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写出你了解的机器学习特征工程操作，以及它的意义（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "   #### （1）数据预处理\n",
    "   - 处理缺失值\n",
    "     - 缺失值较多的特征处理：一般如果某特征的缺失量过大，我们会直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。\n",
    "     - 缺失值较少的特征处理：用规则或模型将缺失数据补全，这种做法的缺点是可能会引入噪声。  \n",
    "     ①把NaN直接作为一个特征，假设用0表示；  \n",
    "     ②用均值填充；有时候还有可能遇到这样一种情况，那就是训练集train中有缺失值，而test中无缺失值 ，这时候最适合的处理方式应该是对缺失值取    条件均值或条件中值，即即根据该用户的label值类别，取所有该label下用户该属性的均值或中值。  \n",
    "     ③用前后数据进行填充；  \n",
    "     ④用插值法填充；插值法就是通过两点(x0,y0),(x1,y1)估计中间点的值（pd.interpolate()）  \n",
    "     - 忽略：有一些模型，如随机森林，自身能够处理数据缺失的情况，在这种情况下不需要对缺失数据做任何的处理，这种做法的缺点是在模型的选择上有局限。\n",
    "   - 处理脏数据\n",
    "     - 不可信的样本直接丢掉，比如一个人身高3米的人、一个人一个月买了10万元的发卡。 \n",
    "   - 数据采样\n",
    "     - 若数据的正负样本不均衡，则要进行数据采样。采样的方法有随机采样和分层抽样。但是随机采样会有隐患，因为可能某次随机采样得到的数据很不均匀，更多的是根据特征采用分层抽样。\n",
    "     \n",
    "#### （2）特征处理\n",
    "  - 数值型\n",
    "    - 幅度调整/归一化：python中会有一些函数比如preprocessing.MinMaxScaler()，可以消除样本不同属性具有不同量级时的影响：  \n",
    "    ①数量级的差异将导致量级较大的属性占据主导地位，譬如SVM，可能会出现数值过大导致c,g取值超过寻优范围；  \n",
    "    ②数量级的差异将导致迭代收敛速度减慢甚至不收敛，例如LR和神经网络。\n",
    "    - 统计值：包括max, min, mean, std等。python中用pandas库序列化数据后，可以得到数据的统计值。 \n",
    "    - 离散化：把连续值转成非线性数据。例如电商会有各种连续的价格表，从0.03到100元，假如以一元钱的间距分割成99个区间，用99维的向量代表每一个价格所处的区间，1.2元和1.6元的向量都是 [0,1,0,…,0]。pd.cut() 可以直接把数据分成若干段。\n",
    "    - 柱状分布：离散化后统计每个区间的个数做柱状图。\n",
    "  - 类别型\n",
    "  类别型一般是文本信息，比如颜色是红色、黄色还是蓝色，我们存储数据的时候就需要先处理数据。处理方法有： \n",
    "    - one-hot编码，编码后得到哑变量。能够处理非数值属性，避免引入大小关系；在一定程度上扩充了特征；编码后的属性是稀疏的，存在大量的零元分量。。 \n",
    "    - Hash编码成词向量\n",
    "    - Histogram映射：把每一列的特征拿出来，根据target内容做统计，把target中的每个内容对应的百分比填到对应的向量的位置。优点是把两个特征联系起来。 \n",
    "  - 时间型\n",
    "    - 时间型特征的用处特别大，既可以看做连续值（持续时间、间隔时间），也可以看做离散值（星期几、几月份）。数据挖掘中经常会用时间作为重要特征，比如电商可以分析节假日和购物的关系，一天中用户喜好的购物时间等。\n",
    "  - 文本型\n",
    "    - 词袋：文本数据预处理后，去掉停用词，剩下的词组成的list，在词库中的映射稀疏向量。Python中用CountVectorizer处理词袋。\n",
    "    - 把词袋中的词扩充到n-gram：n-gram代表n个词的组合。比如“我喜欢你”、“你喜欢我”这两句话如果用词袋表示的话，分词后包含相同的三个词，组成一样的向量：“我 喜欢 你”。显然两句话不是同一个意思，用n-gram可以解决这个问题。如果用2-gram，那么“我喜欢你”的向量中会加上“我喜欢”和“喜欢你”，“你喜欢我”的向量中会加上“你喜欢”和“喜欢我”。这样就区分开来了。\n",
    "    - 使用TF-IDF特征：TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF(t) = (词t在当前文中出现次数) / (t在全部文档中出现次数)，IDF(t) = ln(总文档数/ 含t的文档数)，TF-IDF权重 = TF(t) * IDF(t)。自然语言处理中经常会用到。\n",
    "  - 统计型\n",
    "    - 历届的Kaggle/天池比赛， 天猫/京东排序和推荐业务线里模型用到的特征。统计的内容包括加减平均、分位线、次序型、比例类等。\n",
    "\n",
    "#### （3）特征选择  \n",
    "特征选择，就是从多个特征中，挑选出一些对结果预测最有用的特征。因为原始的特征中可能会有冗余和噪声。  \n",
    "进行特征选择的意义有：①减轻维数灾难问题；②降低学习任务的难度。    \n",
    "常见的特征选择方法分为如下三类：    \n",
    "   - 过滤式（filter）方法评估单个特征和结果值之间的相关程度，主要采用Pearson相关系数，排序留下Top相关的特征部分。缺点是只评估了单个特征对结果的影响，没有考虑到特征之间的关联作用， 可能把有用的关联特征误踢掉。因此工业界使用比较少。\n",
    "   - 包裹式（wrapper）的经典算法是递归特征消除法（RFE）：使用一个基模型在全量特征上进行多轮训练，每轮训练后，消除若干权值系数低的特征，再基于新的特征集进行下一轮训练。其优点是直接针对特定学习器进行优化，因此通常包裹式特征选择比过滤式特征选择更好，缺点是由于特征选择过程需要多次训练学习器，故计算开销要比过滤式特征选择要大得多。\n",
    "   - 嵌入式（embedding）方法的主要思想是在模型既定的情况下学习出对提高模型准确性最好的属性。\n",
    "     - 基于惩罚项的特征选择法：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。实际上，L1惩罚项降维的原理在于选择多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。\n",
    "     - 基于树模型的特征选择法：树模型中GBDT也可用来作为基模型进行特征选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请写出上述特征工程操作的sklearn或者pandas实现方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('./train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            714 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       889 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     22.000000\n",
      "1     38.000000\n",
      "2     26.000000\n",
      "3     35.000000\n",
      "4     35.000000\n",
      "5     29.699118\n",
      "6     54.000000\n",
      "7      2.000000\n",
      "8     27.000000\n",
      "9     14.000000\n",
      "10     4.000000\n",
      "11    58.000000\n",
      "12    20.000000\n",
      "13    39.000000\n",
      "14    14.000000\n",
      "15    55.000000\n",
      "16     2.000000\n",
      "17    29.699118\n",
      "18    31.000000\n",
      "19    29.699118\n",
      "Name: Age, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 22.        ],\n",
       "       [ 38.        ],\n",
       "       [ 26.        ],\n",
       "       [ 35.        ],\n",
       "       [ 35.        ],\n",
       "       [ 29.69911765],\n",
       "       [ 54.        ],\n",
       "       [  2.        ],\n",
       "       [ 27.        ],\n",
       "       [ 14.        ],\n",
       "       [  4.        ],\n",
       "       [ 58.        ],\n",
       "       [ 20.        ],\n",
       "       [ 39.        ],\n",
       "       [ 14.        ],\n",
       "       [ 55.        ],\n",
       "       [  2.        ],\n",
       "       [ 29.69911765],\n",
       "       [ 31.        ],\n",
       "       [ 29.69911765]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#缺失值填充，使用pandas的fillna函数\n",
    "print(df_train['Age'].fillna(value=df_train['Age'].mean()).head(20))\n",
    "\n",
    "## 使用sklearn当中的Imputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy='mean', axis=0)\n",
    "age = imp.fit_transform(df_train[['Age']].values)\n",
    "age[:20]#取出前20项数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数值型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （1）幅度缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.091042\n",
       "1    3.637586\n",
       "2    3.258097\n",
       "3    3.555348\n",
       "4    3.555348\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取log缩放\n",
    "import numpy as np\n",
    "log_age = df_train['Age'].apply(np.log)\n",
    "log_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01415106]\n",
      " [ 0.13913574]\n",
      " [ 0.01546857]\n",
      " [ 0.1036443 ]\n",
      " [ 0.01571255]\n",
      " [ 0.0165095 ]\n",
      " [ 0.10122886]\n",
      " [ 0.04113566]\n",
      " [ 0.02173075]\n",
      " [ 0.05869429]\n",
      " [ 0.03259623]\n",
      " [ 0.05182215]\n",
      " [ 0.01571255]\n",
      " [ 0.06104473]\n",
      " [ 0.01533038]\n",
      " [ 0.03122992]\n",
      " [ 0.05684821]\n",
      " [ 0.02537431]\n",
      " [ 0.03513366]\n",
      " [ 0.01410226]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.50244517],\n",
       "       [ 0.78684529],\n",
       "       [-0.48885426],\n",
       "       [ 0.42073024],\n",
       "       [-0.48633742],\n",
       "       [-0.47811643],\n",
       "       [ 0.39581356],\n",
       "       [-0.22408312],\n",
       "       [-0.42425614],\n",
       "       [-0.0429555 ],\n",
       "       [-0.31217238],\n",
       "       [-0.11384571],\n",
       "       [-0.48633742],\n",
       "       [-0.01870931],\n",
       "       [-0.49027979],\n",
       "       [-0.32626666],\n",
       "       [-0.06199889],\n",
       "       [-0.38667072],\n",
       "       [-0.28599728],\n",
       "       [-0.50294854]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最大最小值缩放\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()\n",
    "fare_trans = mm_scaler.fit_transform(df_train[['Fare']])\n",
    "print(fare_trans[:20])\n",
    "#标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "fare_std_trans = std_scaler.fit_transform(df_train[['Fare']])\n",
    "fare_std_trans[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （2）统计结果作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n",
      "0.42\n"
     ]
    }
   ],
   "source": [
    "# max min\n",
    "print(df_train[\"Age\"].max())\n",
    "print(df_train[\"Age\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.125"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第1四分位数 (Q1)\n",
    "age_quarter_1 = df_train['Age'].quantile(0.25)\n",
    "age_quarter_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#第3四分位数 (Q3)\n",
    "age_quarter_3 = df_train['Age'].quantile(0.75)\n",
    "age_quarter_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （3）四则运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>family_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  family_size  \n",
       "0      0         A/5 21171   7.2500   NaN        S            2  \n",
       "1      0          PC 17599  71.2833   C85        C            2  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S            1  \n",
       "3      0            113803  53.1000  C123        S            2  \n",
       "4      0            373450   8.0500   NaN        S            1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[:,'family_size'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （4）构造高次特征和交叉特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   1.00000000e+00,   7.25000000e+00,\n",
       "          1.00000000e+00,   7.25000000e+00,   5.25625000e+01],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   7.12833000e+01,\n",
       "          1.00000000e+00,   7.12833000e+01,   5.08130886e+03],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   7.92500000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.28056250e+01],\n",
       "       [  1.00000000e+00,   1.00000000e+00,   5.31000000e+01,\n",
       "          1.00000000e+00,   5.31000000e+01,   2.81961000e+03],\n",
       "       [  1.00000000e+00,   0.00000000e+00,   8.05000000e+00,\n",
       "          0.00000000e+00,   0.00000000e+00,   6.48025000e+01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "poly_fea = poly.fit_transform(df_train[['SibSp', 'Fare']])\n",
    "poly_fea[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （5）离散化/分箱/分桶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.512, 102.466]     838\n",
       "(102.466, 204.932]     33\n",
       "(204.932, 307.398]     17\n",
       "(409.863, 512.329]      3\n",
       "(307.398, 409.863]      0\n",
       "Name: fare_cut, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas cut \n",
    "df_train.loc[:,'fare_cut'] = pd.cut(df_train['Fare'],5)\n",
    "df_train.fare_cut.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.854, 10.5]        184\n",
       "(21.679, 39.688]     180\n",
       "(-0.001, 7.854]      179\n",
       "(39.688, 512.329]    176\n",
       "(10.5, 21.679]       172\n",
       "Name: fare_qcut, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas qcut\n",
    "df_train.loc[:,'fare_qcut'] = pd.qcut(df_train['Fare'],5)\n",
    "df_train.fare_qcut.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （6）独热向量编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>family_size</th>\n",
       "      <th>fare_cut</th>\n",
       "      <th>fare_qcut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>(-0.512, 102.466]</td>\n",
       "      <td>(-0.001, 7.854]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "      <td>(-0.512, 102.466]</td>\n",
       "      <td>(39.688, 512.329]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "\n",
       "   Parch     Ticket     Fare Cabin Embarked  family_size           fare_cut  \\\n",
       "0      0  A/5 21171   7.2500   NaN        S            2  (-0.512, 102.466]   \n",
       "1      0   PC 17599  71.2833   C85        C            2  (-0.512, 102.466]   \n",
       "\n",
       "           fare_qcut  \n",
       "0    (-0.001, 7.854]  \n",
       "1  (39.688, 512.329]  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Embarked_C  Embarked_Q  Embarked_S\n",
       "0           0           0           1\n",
       "1           1           0           0\n",
       "2           0           0           1\n",
       "3           0           0           1\n",
       "4           0           0           1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embarked_oht = pd.get_dummies(df_train[['Embarked']])\n",
    "embarked_oht.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- （7）时间型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>20180314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>20180311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>20180227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>20180114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cnt      date\n",
       "0   21  20180314\n",
       "1   25  20180311\n",
       "2   16  20180227\n",
       "3   19  20180114"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time = pd.DataFrame({'date':['20180314', '20180311', '20180227','20180114'],'cnt':[21,25,16,19]})\n",
    "df_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>date</th>\n",
       "      <th>dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>20180314</td>\n",
       "      <td>2018-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>20180311</td>\n",
       "      <td>2018-03-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>20180227</td>\n",
       "      <td>2018-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>20180114</td>\n",
       "      <td>2018-01-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cnt      date         dt\n",
       "0   21  20180314 2018-03-14\n",
       "1   25  20180311 2018-03-11\n",
       "2   16  20180227 2018-02-27\n",
       "3   19  20180114 2018-01-14"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.loc[:,'dt'] = pd.to_datetime(df_time['date'])\n",
    "df_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 3 columns):\n",
      "cnt     4 non-null int64\n",
      "date    4 non-null object\n",
      "dt      4 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(1)\n",
      "memory usage: 176.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_time.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 过滤式/Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.4,  0.2],\n",
       "       [ 1.4,  0.2],\n",
       "       [ 1.3,  0.2],\n",
       "       [ 1.5,  0.2],\n",
       "       [ 1.4,  0.2],\n",
       "       [ 1.7,  0.4]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = SelectKBest(k=2).fit_transform(X,y)\n",
    "print(X_new.shape)\n",
    "X_new[:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 包裹式/wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4,  0.2],\n",
       "       [ 1.4,  0.2],\n",
       "       [ 1.3,  0.2],\n",
       "       [ 1.5,  0.2],\n",
       "       [ 1.4,  0.2],\n",
       "       [ 1.7,  0.4]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rfe = RFE(estimator=rf, n_features_to_select=2)\n",
    "X_rfe = rfe.fit_transform(X,y)\n",
    "X_rfe[:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 嵌入式/Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4],\n",
       "       [ 4.9,  3. ,  1.4],\n",
       "       [ 4.7,  3.2,  1.3],\n",
       "       [ 4.6,  3.1,  1.5],\n",
       "       [ 5. ,  3.6,  1.4],\n",
       "       [ 5.4,  3.9,  1.7]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(C=0.01, penalty='l1', dual=False).fit(X,y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_embedd = model.transform(X)\n",
    "X_embedd[:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.模型评估中的留一法，留出法，交叉验证分别是什么操作？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 留出法：直接将数据集D划分成两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。\n",
    "- 留一法：假设有N个样本，将每一个样本作为测试样本，其它N-1个样本作为训练样本。这样得到N个分类器，N个测试结果。用这N个结果的平均值来衡量模型的性能。\n",
    "- 交叉验证：将全部训练集 S分成 k个不相交的子集，假设 S中的训练样例个数为 m，那么每一个子 集有 m/k 个训练样例，，相应的子集称作 {s1,s2,……,sk}；每次从分好的子集中里面，拿出一个作为测试集，其它k-1个作为训练集；根据训练训练出模型或者假设函数，把这个模型放到测试集上，得到分类率；计算k次求得的分类率的平均值，作为该模型或者假设函数的真实分类率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.如何理解模型的过拟合与欠拟合，以及如何解决？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 欠拟合  \n",
    "欠拟合就是模型没有很好地捕捉到数据特征，不能够很好地拟合数据。  \n",
    "解决方法：  \n",
    "  - 添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。\n",
    "  - 添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。\n",
    "  - 减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。  \n",
    "  \n",
    "  \n",
    "- 过拟合  \n",
    "过拟合就是模型把数据学习的太彻底，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差。  \n",
    "解决方法：  \n",
    "  - 重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。\n",
    "  - 增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。\n",
    "  - 采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则。\n",
    "  - 采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作题(共1题，共计80分)\n",
    "\n",
    "- note: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信用卡欺诈项目(共7项，前5项每题10分，6，7题每题15分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 前期数据导入,预览及处理(此部分勿修改，涉及的数据文件无需复制移动)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0                 1                              0.766127  45.0   \n",
       "1                 0                              0.957151  40.0   \n",
       "2                 0                              0.658180  38.0   \n",
       "3                 0                              0.233810  30.0   \n",
       "4                 0                              0.907239  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0   0.802982         9120.0   \n",
       "1                                   0.0   0.121876         2600.0   \n",
       "2                                   1.0   0.085113         3042.0   \n",
       "3                                   0.0   0.036050         3300.0   \n",
       "4                                   1.0   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip', 'r') as z:\n",
    "    f = z.open('KaggleCredit2.csv')\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112915, 11)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeriousDlqin2yrs                           0\n",
       "RevolvingUtilizationOfUnsecuredLines       0\n",
       "age                                     4267\n",
       "NumberOfTime30-59DaysPastDueNotWorse       0\n",
       "DebtRatio                                  0\n",
       "MonthlyIncome                              0\n",
       "NumberOfOpenCreditLinesAndLoans            0\n",
       "NumberOfTimes90DaysLate                    0\n",
       "NumberRealEstateLoansOrLines               0\n",
       "NumberOfTime60-89DaysPastDueNotWorse       0\n",
       "NumberOfDependents                      4267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shapey = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06742876076872101"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 1.把数据切分成训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "if Version(sklearn_version) < '0.18':\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "else:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76053, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32595, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76053,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32595,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.使用logistic regression建模，并且输出一下系数，分析重要度。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先对数值型特征进行标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std=StandardScaler()\n",
    "X_train_std=std.fit_transform(X_train)\n",
    "X_test_std=std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建模\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(penalty='l1',C=10, random_state=0)\n",
    "LR.fit(X_train_std, y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "逻辑回归训练集准确度：0.933126\n"
     ]
    }
   ],
   "source": [
    "print('逻辑回归训练集准确度：%f'%LR.score(X_train_std,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  NumberOfTime30-59DaysPastDueNotWorse  1.728235\n",
      "1  NumberOfTimes90DaysLate  1.688007\n",
      "2  DebtRatio  0.312083\n",
      "3  NumberOfDependents  0.116360\n",
      "4  RevolvingUtilizationOfUnsecuredLines  -0.014233\n",
      "5  NumberOfOpenCreditLinesAndLoans  -0.091907\n",
      "6  MonthlyIncome  -0.114985\n",
      "7  NumberRealEstateLoansOrLines  -0.196426\n",
      "8  age  -0.364293\n",
      "9  NumberOfTime60-89DaysPastDueNotWorse  -3.246326\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels=data.columns[1:]\n",
    "coefs=LR.coef_      #coef_存放回归系数，intercept_存放截距\n",
    "indexs=np.argsort(coefs[0])[::-1]\n",
    "for i in range(X_train.shape[1]):\n",
    "    print ('%d  %s  %f'%(i,labels[indexs[i]],coefs[0,indexs[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重要度分析：系数绝对值越大，则对应特征的重要度越大，重要度排名前三的特征都是逾期还款但不糟糕的次数，可见人的信用情况对于债务违约的概率具有至关重要的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.使用决策树/SVM/KNN...等sklearn分类算法进行分类，尝试了解参数含义，调整不同的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#决策树\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "DT.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树训练集准确度：0.946458\n"
     ]
    }
   ],
   "source": [
    "print ('决策树训练集准确度：%f'%DT.score(X_train_std,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pengzx\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:218: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=4, gamma='auto', kernel='rbf',\n",
       "  max_iter=5000, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "SVM = SVC(degree=4,max_iter=5000)\n",
    "SVM.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM训练集准确度：0.936018\n"
     ]
    }
   ],
   "source": [
    "print ('SVM训练集准确度：%f'%SVM.score(X_train_std,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors=4)\n",
    "KNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN训练集准确度：0.932218\n"
     ]
    }
   ],
   "source": [
    "print ('KNN训练集准确度：%f'%KNN.score(X_train_std,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.在测试集上进行预测，计算准确度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR错误分类数: 2171\n",
      "LR测试集准确度：0.933855\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "y_pred_lr=LR.predict(X_test)\n",
    "print('LR错误分类数: %d' % (y_test != y_pred_lr).sum())\n",
    "print('LR测试集准确度：%f'%LR.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树错误分类数: 2186\n",
      "决策树测试集准确度：0.931922\n"
     ]
    }
   ],
   "source": [
    "#决策树\n",
    "y_pred_tree=DT.predict(X_test)\n",
    "print('决策树错误分类数: %d' % (y_test != y_pred_tree).sum())\n",
    "print('决策树测试集准确度：%f'%DT.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错误分类数: 2171\n",
      "测试集准确度：0.936432\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "y_pred_svm=SVM.predict(X_test)\n",
    "print('错误分类数: %d' % (y_test != y_pred_svm).sum())\n",
    "print('测试集准确度：%f'%SVM.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN错误分类数: 2195\n",
      "KNN测试集准确度：0.932658\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "y_pred_knn=KNN.predict(X_test)\n",
    "print('KNN错误分类数: %d' % (y_test != y_pred_knn).sum())\n",
    "print('KNN测试集准确度：%f'%KNN.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.查看sklearn的官方说明，了解混淆矩阵等评估标准，并对此例进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 评估标准：\n",
    "  - AccuracyRate(准确率): (TP+TN)/(TP+TN+FN+FP)\n",
    "  - ErrorRate(误分率): (FN+FP)/(TP+TN+FN+FP)\n",
    "  - Recall(召回率，查全率,击中概率): TP/(TP+FN), 在所有真正的正样本中有多少被识别为正样本了\n",
    "  - Precision(查准率):TP/(TP+FP),在所有识别成正样本中有多少是真正的正样本\n",
    "  - FAR(FalseAcceptance Rate)或FPR(False Positive Rate)：FP/(FP+TN)， 错误接收率，误报率，在所有标记为负样本中有多少被识别为正样本了\n",
    "  - FRR(FalseRejection Rate): FN/(TP+FN)，错误拒绝率，拒真率，在所有标记为正样本中有多少被识别为负样本了，它等于1-Recall\n",
    "  - ROC曲线（receiver operatingcharacteristic curve）：  \n",
    "  ①横轴是FAR,纵轴是Recall；  \n",
    "  ②每个阈值的识别结果对应一个点(FPR，TPR),当阈值最大时，所有样本都被识别成负样本，对应于右上角的点(0,0)，当阈值最小时，所有样本都被识别成正样本，对应于右上角的点(1,1)，随着阈值从最大变化到最小，TP和FP都逐渐增大；  \n",
    "  ③可以使用ROC曲线下方的面积AUC（AreaUnder roc Curve）值来度量算法好坏：如果模型是完美的，那么它的AUG = 1，如果模型是个简单的随机猜测模型，那么它的AUG = 0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大；  \n",
    "  ④一个好的分类模型应尽可能位于图像的左上角，而一个随机猜测模型应位于连接点（TPR=0,FPR=0）和（TPR=1,FPR=1）的主对角线上；  \n",
    "  ⑤ERR（Equal Error Rate,相等错误率）：FAR和FRR是同一个算法系统的两个参数，把它放在同一个坐标中。FAR是随阈值增大而减小的，FRR是随阈值增大而增大的。因此它们一定有交点。这个点是在某个阈值下的FAR与FRR等值的点。习惯上用这一点的值来衡量算法的综合性能。对于一个更优的指纹算法，希望在相同阈值情况下，FAR和FRR都越小越好。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30424,     0],\n",
       "       [ 2171,     0]], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对LR模型评估\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_lr=confusion_matrix(y_test, y_pred_lr)\n",
    "confusion_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30259,   165],\n",
       "       [ 2021,   150]], dtype=int64)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对决策树模型评估\n",
    "confusion_tree=confusion_matrix(y_test, y_pred_tree)\n",
    "confusion_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30424,     0],\n",
       "       [ 2171,     0]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对SVM模型评估\n",
    "confusion_svm=confusion_matrix(y_test, y_pred_svm)\n",
    "confusion_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30383,    41],\n",
       "       [ 2154,    17]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对KNN模型评估\n",
    "confusion_knn=confusion_matrix(y_test, y_pred_knn)\n",
    "confusion_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6.银行通常会有更严格的要求，因为fraud带来的后果通常比较严重，一般我们会调整模型的标准。   \n",
    "比如在logistic regression当中，一般我们的概率判定边界为0.5，但是我们可以把阈值设定低一些，来提高模型的“敏感度”   \n",
    "试试看把阈值设定为0.3，再看看这个时候的混淆矩阵等评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight={1: 0.3, 0: 0.7}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=0,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2',C=10, random_state=0,class_weight={1:0.3,0:0.7})#由于误分类，故适当提高负样本的权重\n",
    "lr.fit(X_train, y_train)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错误分类数: 2169\n",
      "测试集准确度：0.933456\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr=lr.predict(X_test)\n",
    "print('错误分类数: %d' % (y_test != y_pred_lr).sum())\n",
    "print('测试集准确度：%f'%lr.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30410,    14],\n",
       "       [ 2155,    16]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_lr=confusion_matrix(y_test, y_pred_lr)\n",
    "confusion_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见召回率由100%下降到99.954%，查准率由93.339%上升到93.382%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7.尝试对不同特征的重要度进行排序，通过特征选择的方式，对特征进行筛选。并重新建模，观察此时的模型准确率等评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用随机森林模型对特征进行筛选\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "feat_labels = data.columns[1:]\n",
    "forest=RandomForestClassifier(n_estimators=1000,random_state=0,n_jobs=-1)\n",
    "forest.fit(X_train,y_train)\n",
    "importances=forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RevolvingUtilizationOfUnsecuredLines 0.188899\n",
      "1 DebtRatio 0.172926\n",
      "2 MonthlyIncome 0.165889\n",
      "3 age 0.122358\n",
      "4 NumberOfOpenCreditLinesAndLoans 0.089148\n",
      "5 NumberOfTimes90DaysLate 0.087252\n",
      "6 NumberOfTime30-59DaysPastDueNotWorse 0.051195\n",
      "7 NumberOfDependents 0.046049\n",
      "8 NumberOfTime60-89DaysPastDueNotWorse 0.044431\n",
      "9 NumberRealEstateLoansOrLines 0.031852\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels=data.columns[1:]\n",
    "indexs=np.argsort(importances)[::-1]\n",
    "for i in range(X_train.shape[1]):\n",
    "    print('%d %s %f'%(i,labels[indexs[i]],importances[indexs[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#选取按重要度排序的前4个特征，建立逻辑回归模型（很奇怪为啥LR与forest模型训练出来的特征重要度排序相差这么大，不知是树太少还是LR不行）\n",
    "X_train_4=X_train[['RevolvingUtilizationOfUnsecuredLines','DebtRatio','MonthlyIncome','age']]\n",
    "X_test_4=X_test[['RevolvingUtilizationOfUnsecuredLines','DebtRatio','MonthlyIncome','age']]\n",
    "#对特征进行标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std=StandardScaler()\n",
    "X_train_std=std.fit_transform(X_train_4)\n",
    "X_test_std=std.transform(X_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重新建模\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l1',C=10.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集准确度：0.932218\n"
     ]
    }
   ],
   "source": [
    "print('训练集准确度：%f'%lr.score(X_train_std,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "错误分类数: 2171\n",
      "测试集准确度：0.933395\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr=lr.predict(X_test_std)\n",
    "print('错误分类数: %d' % (y_test != y_pred_lr).sum())\n",
    "print ('测试集准确度：%f'%lr.score(X_test_std,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30424,     0],\n",
       "       [ 2171,     0]], dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_lr=confusion_matrix(y_test, y_pred_lr)\n",
    "confusion_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过对比可看出，特征筛选前与筛选后，模型各项评估指标没有明显变化，但是降低了特征数量，加快了训练时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    对特征工程的基本流程有一定的认识，对各种模型和学习算法有一定的了解，对sklearn中一些常见API的参数有一些了解；完成作业用时过长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    这周课程涉及的知识点较多，还得多理一理思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
