{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第七周(机器学习-模型调优与融合)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月23日至3月25日期间完成，最晚提交时间本周日（3月25日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam7后，进行作答。例如wangwei-exam7\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/7/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>张少洋</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共6题，1，2题每题5分，后4题每题10分，共计50分)\n",
    "\n",
    "- note: 完美  50分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 机器学习任务中，通常会将给定数据切分为训练集，验证集，测试集，请回答这三类数据集各自的用途。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "数据分为两大类，一类是有标签的，一类是无标签的。有监督学习的数据都是有标签的，有监督学习的数据可分为训练集，验证集，测试集。\n",
    "- 训练集：学习样本数据集，用来估计模型；\n",
    "- 验证集：用来对估计出来的模型进行调参，从而找到最优参数和最优模型，例如利用网格搜索交叉验证来进行参数的选择；\n",
    "- 测试集：用来检验最终选择最优的模型的性能如何。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.如何将数据转换成xgboost内置用法的libsvm数据格式，以及该格式数据是如何解读的?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 转换方式\n",
    "使用xgboost内置的xgboost.DMatrix类将数据转换成libsvm的数据格式，xgboost.DMatrix(data)，例如xgb.DMatrix('../data/agaricus.txt.train')\n",
    "#### 如何解读\n",
    "数据格式如下<br>\n",
    "\n",
    "[label] [index1]:[value1] [index2]:[value2] …\n",
    "\n",
    "[label] [index1]:[value1] [index2]:[value2] …\n",
    "\n",
    "- label 是类别；\n",
    "- index 是特征的索引；\n",
    "- value 是特征值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 对于自动特征选择，通常有哪三类方法？试写出每类方式的思想，以及在sklearn中的代码实现（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (1)单变量特征选择\n",
    "单变量的特征选择是通过基于单变量的统计测试来选择最好的特征。它可以当做是评估器的预处理步骤。sklearn将特征选择的内容作为实现了 transform 方法的对象，例如：\n",
    "- SelectKBest 选择评分最高的 K 个特征\n",
    "- SelectPercentile 选择用户指定的最高得分百分比的特征\n",
    "- GenericUnivariateSelect 允许使用可配置方法来进行单变量特征选择。它允许超参数搜索评估器来选择最好的单变量特征（默认mode='percentile'）\n",
    "\n",
    "以上三种对象将得分函数作为输入，返回单变量的得分和 p 值。得分函数即判定得分高低的依据，得分函数包括：\n",
    "- 对于回归: f_regression , mutual_info_regression\n",
    "- 对于分类: chi2（卡方统计） , f_classif（ANOVA F检验） , mutual_info_classif（估计离散目标变量的互信息）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "#代码实现\n",
    "#1.SelectKBest\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y) #k=2 Number of top features to select. \n",
    "print(X.shape)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284, 80)\n",
      "(284, 40)\n"
     ]
    }
   ],
   "source": [
    "#2.SelectPercentile\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# get deterministic random numbers\n",
    "rng = np.random.RandomState(42)\n",
    "noise = rng.normal(size=(len(cancer.data), 50))\n",
    "# add noise features to the data\n",
    "# the first 30 features are from the dataset, the next 50 are noise\n",
    "X_w_noise = np.hstack([cancer.data, noise])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    " X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
    "# use f_classif (the default) and SelectPercentile to select 10% of features:\n",
    "select = SelectPercentile(percentile=50) #SelectPercentile 移除除了用户指定的最高得分百分比之外的所有特征\n",
    "select.fit(X_train, y_train)\n",
    "# transform training set:\n",
    "X_train_selected = select.transform(X_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_selected.shape)\n",
    "#最终选择了得分最高的前50%的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 2)\n"
     ]
    }
   ],
   "source": [
    "#代码实现\n",
    "#3.GenericUnivariateSelect\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "#X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new = GenericUnivariateSelect(score_func = chi2,mode = 'k_best', param=2).fit_transform(X, y)\n",
    "print(X.shape)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2)基于模型的特征选择\n",
    "SelectFromModel 是一个 meta-transformer（元转换器） ，它可以用来处理任何带有 coef_ 或者 feature_importances_ 属性的训练之后的评估器。 如果相关的``coef_`` 或者 featureimportances 属性值低于预先设置的阈值，这些特征将会被认为不重要并且移除掉。除了指定数值上的阈值之外，还可以通过给定字符串参数来使用内置的启发式方法找到一个合适的阈值。可以使用的启发式方法有 mean 、 median 以及使用浮点数乘以这些（例如，0.1*mean ）。\n",
    "- 基于 L1 的特征选取<br>\n",
    "Linear models 使用 L1 正则化的线性模型会得到稀疏解：他们的许多系数为 0。 当目标是降低使用另一个分类器的数据集的维度， 它们可以与 feature_selection.SelectFromModel 一起使用来选择非零系数。特别的，可以用于此目的的稀疏评估器有用于回归的 linear_model.Lasso , 以及用于分类的 linear_model.LogisticRegression 和 svm.LinearSVC\n",
    "- 基于 Tree（树）的特征选取<br>\n",
    "基于树的 estimators可以用来计算特征的重要性，然后可以消除不相关的特征（当与 sklearn.feature_selection.SelectFromModel 等元转换器一同使用时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X.shape)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284, 80)\n",
      "(284, 40)\n"
     ]
    }
   ],
   "source": [
    "#代码实现\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=\"median\")\n",
    "\n",
    "select.fit(X_train, y_train)\n",
    "X_train_select = select.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_select.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)递归式特征消除\n",
    "给定一个外部的估计器，可以对特征赋予一定的权重（比如，线性模型的相关系数），recursive feature elimination ( RFE ) 通过考虑越来越小的特征集合来递归的选择特征。 首先，评估器在初始的特征集合上面训练并且每一个特征的重要程度是通过一个 coef_ 属性 或者 feature_importances_ 属性来获得。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为止。 RFECV 在一个交叉验证的循环中执行 RFE 来找到最优的特征数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd09627358>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAAsCAYAAABlluU8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efd097442e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)\n",
    "#select = RFE(LogisticRegression(penalty=\"l1\"), n_features_to_select=40)\n",
    "\n",
    "select.fit(X_train, y_train)\n",
    "# visualize the selected features:\n",
    "mask = select.get_support()\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9508771929824561"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rfe = select.transform(X_train)\n",
    "X_test_rfe = select.transform(X_test)\n",
    "select.score(X_test, y_test)  #Reduce X to the selected features and then return the score of the underlying estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9508771929824561"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285, 40)\n",
      "(285, 80)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_rfe.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.网格搜索交叉验证的作用是什么，并简述网格搜索交叉验证是如何运行的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 作用：\n",
    "网格搜索交叉验证的作用是结合网格搜索的原理和交叉验证的原理来选择模型的最优参数。交叉验证是以比较高的可信度去评估模型本组参数的效果，网格搜索通过遍历所有候选参数去选择模型的参数，通过以上两个阶段去选择最优参数。网格搜索交叉验证在sklearn中用的是sklearn.model_selection.GridSearchCV。\n",
    "\n",
    "#### 网格搜索交叉验证的运行原理：\n",
    "- 1.准备好参数网格字典，该网格字典是所有参数的候选列表；\n",
    "- 2.取定一组参数，然后利用交叉验证，对每折验证集的评价结果取平均，并把该结果作为本组参数的评价结果；\n",
    "- 3.遍历所有的参数组合，选出参数评价结果最好的一组参数作为最优的参数组合，从而得到最好的得分和最优模型；\n",
    "- 4.利用上述得到的最优模型在测试集上进行预测评估。\n",
    "\n",
    "以上过程是并行的，GridSearchCV 内部实现了并行，数据是共享的，也可以指定使用多少个核。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.工业界上模型整合有三大类方式？试简述每类方式其思想？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (1)集体智慧(投票器/BAGGING/随机森林（样本特征随机）\n",
    "- Voting投票器：单个模型很难控制过拟合，采取多数表决的方式，少数服从多数。\n",
    "- bagging：\n",
    "    - 从样本集中重采样（有重复的）选出n个样本；\n",
    "    - 在所有特征上，对这n个样本建立基本分类器，分类器之间要有差异度；\n",
    "    - 重复以上两步m次，即获得m个分类器；\n",
    "    - 将数据喂给这m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。\n",
    "- 随机森林：在bagging基础上做了修改，对样本和特征都随机选取。\n",
    "    - 从样本集中有放回地抽样，选出n个样本；\n",
    "    - 从所有特征中随机选择k个特征，然后选择最佳分割属性作为节点建立CART树；\n",
    "    - 重复以上两步m次，即建立了m颗CART决策树；\n",
    "    - 这m颗CART形成随机森林，通过投票表决结果，决定数据属于哪一类。\n",
    "\n",
    "#### (2)STACKING和BLENDING\n",
    "- Stacking：用上一层模型输出的结果作为下一层的特征，再学习一个模型；\n",
    "- Blending：弱化版的stacking，对结果做线性加权，上一层的输出结果作为下一层的特征，然后再喂给一个线性模型（回归问题-LinearRegression；分类问题-LogisticRegression）。\n",
    "\n",
    "#### (3)BOOSTING\n",
    "- Boosting是基于参差进行学习，逐步缩小残差；\n",
    "- 每一步产生一个弱预测模型，并加权累加到总模型中；\n",
    "- 如果每一步的弱预测模型生成都是依据损失函数的梯度方向，则称之为梯度提升；\n",
    "- 核心理论意义：如果一个问题存在弱分类器，则可以通过提升的办法得到强分类器。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  我们可以将xgboost的中众多参数分类为哪三类？请写出哪些参数可以用什么方式用来控制过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 总共有3类参数：\n",
    "\n",
    "- 通用参数/general parameters：通用参数和模型本身无关，属于全局的一个设定，例如：booster [default=gbtree]（常用）、silent [default=0]（0表示输出信息， 1表示安静模式）、nthread（跑xgboost的线程数，默认最大线程数）\n",
    "\n",
    "\n",
    "- 集成(增强)参数/booster parameters :\n",
    "    - eta [default=0.3, 可以视作学习率]\n",
    "    - gamma [default=0, alias: min_split_loss]\n",
    "    - max_depth [default=6]\n",
    "    - min_child_weight [default=1]\n",
    "    - 等等\n",
    "- 任务参数/task parameters\n",
    "    - objective [ default=reg:linear ] 这个参数定义需要被最小化的损失函数。最常用的值有\"reg:linear\"（回归）、\"reg:logistic\"（二分类）、\"multi:softmax\" （多分类）等等；\n",
    "    - eval_metric [ 默认是根据 损失函数/目标函数 自动选定的 ]评估指标。\n",
    "    - 等等\n",
    "\n",
    "#### 控制过拟合方式：\n",
    "- eat：为防止过拟合，eta通过缩减特征的权重使提升计算过程更加保守；\n",
    "- gamma：对树的叶子节点做进一步的分割而必须设置的损失减少的最小值，该值越大，算法越保守；\n",
    "- max_depth：用于设置树的最大深度，每棵树在生长的过程当中所允许达到的最大树深（一般设置成小于9的数比较好）；\n",
    "- min_child_weight：如果树的生长时的某一步所生成的叶子结点，若该叶子节点上的样本数之和小于min_child_weight，那么可以放弃该步生长；\n",
    "- subsample：表示观测的子样本的比率，将其设置为0.5意味着xgboost将随机抽取一半观测用于数的生长，这将有助于防止过拟合现象；\n",
    "- colsample_bytree：表示用于构造每棵树时，对特征进行采样；\n",
    "- scale_pos_weight：在各类别样本十分不平衡时，对样本少的类别进行加权，增加模型对类别的重视程度。\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，每题25分，共计50分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用XGBoost内置方式，导入iris数据完成分类问题（要求以不同参数设置xgboost运行并比对），最后给出实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "dtrain = xgb.DMatrix(X_train, label = y_train)\n",
    "dtest = xgb.DMatrix(X_test, label = y_test)\n",
    "watch_list = [(dtest, 'eval'), (dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以下对树深和学习率两个参数调参对比，其他参数可参考这种方式，暂且省略其他参数的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running0\n",
      "[0]\teval-merror:0.105263\ttrain-merror:0.035714\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.026786\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.026786\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.026786\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.026786\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.026786\n",
      "正确率:\t 0.9736842105263158\n",
      "running1\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "正确率:\t 0.9736842105263158\n",
      "running2\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "正确率:\t 0.9736842105263158\n",
      "running3\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "正确率:\t 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#固定其他参数，调整树深，观察数据结果\n",
    "params = [{'max_depth': 2, 'eta': 0.3, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 3, 'eta': 0.3, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 4, 'eta': 0.3, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 6, 'eta': 0.3, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3}]\n",
    "for i, param in enumerate(params):\n",
    "    print ('running'+ str(i))\n",
    "    bst = xgb.train(param, data_train, num_boost_round=6, evals=watch_list)  #num_boost_round=6 代表进行六轮  \n",
    "    y_hat = bst.predict(dtest)\n",
    "    result = y_test.reshape(1, -1) == y_hat\n",
    "    print ('正确率:\\t', float(np.sum(result)) / len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running0\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "正确率:\t 0.9736842105263158\n",
      "running1\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "正确率:\t 0.9736842105263158\n",
      "running2\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[3]\teval-merror:0.026316\ttrain-merror:0.008929\n",
      "[4]\teval-merror:0.026316\ttrain-merror:0.008929\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0.008929\n",
      "正确率:\t 0.9736842105263158\n",
      "running3\n",
      "[0]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[1]\teval-merror:0.026316\ttrain-merror:0.017857\n",
      "[2]\teval-merror:0.052632\ttrain-merror:0\n",
      "[3]\teval-merror:0.052632\ttrain-merror:0\n",
      "[4]\teval-merror:0.052632\ttrain-merror:0\n",
      "[5]\teval-merror:0.026316\ttrain-merror:0\n",
      "正确率:\t 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#固定其他参数，调整学习率，观察数据结果\n",
    "params = [{'max_depth': 3, 'eta': 0.01, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 3, 'eta': 0.1, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 3, 'eta': 0.5, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3},\n",
    "         {'max_depth': 3, 'eta': 1, 'silent': 1, 'objective': 'multi:softmax', 'num_class': 3}]\n",
    "for i, param in enumerate(params):\n",
    "    print ('running'+ str(i))\n",
    "    bst = xgb.train(param, data_train, num_boost_round=6, evals=watch_list)  #num_boost_round=6 代表进行六轮  \n",
    "    y_hat = bst.predict(dtest)\n",
    "    result = y_test.reshape(1, -1) == y_hat\n",
    "    print ('正确率:\\t', float(np.sum(result)) / len(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "- #### 固定其他参数，调整树深 \n",
    "    - 'max_depth'的值是2时，测试集和验证集的错误率在第二轮迭代开始就趋向于稳定\n",
    "    - 'max_depth'能够防止过拟合的上限是3\n",
    "- #### 固定其他参数，调整学习率，仅从以上结果发现：在该样本量的前提下，学习率的大小对该模型没有太大的影响\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用XGBoost的sklearn接口，对KaggleCredit2数据完成信用卡欺诈项目的建模及分析（要求以不同参数设置xgboost运行并比对），最后给出实验总结\n",
    "\n",
    "- KaggleCredit2数据文件 位于/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip，请勿复制或移动该文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0                 1                              0.766127  45.0   \n",
       "1                 0                              0.957151  40.0   \n",
       "2                 0                              0.658180  38.0   \n",
       "3                 0                              0.233810  30.0   \n",
       "4                 0                              0.907239  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0   0.802982         9120.0   \n",
       "1                                   0.0   0.121876         2600.0   \n",
       "2                                   1.0   0.085113         3042.0   \n",
       "3                                   0.0   0.036050         3300.0   \n",
       "4                                   1.0   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip', 'r') as z:\n",
    "    f = z.open('KaggleCredit2.csv')\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shapey = data['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06742876076872101"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#切分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=1, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [0.1,0.3,0.5], \n",
    "              'n_estimators': [3,3,3], \n",
    "              'max_depth': [2,3,5], \n",
    "              'min_samples_leaf':[2,3,1]}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [3, 3, 3], 'learning_rate': [0.1, 0.3, 0.5], 'max_depth': [2, 3, 5], 'min_samples_leaf': [2, 3, 1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9341923607915325"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 3, 'learning_rate': 0.3, 'max_depth': 2, 'min_samples_leaf': 2}\n",
      "0.9353740154891983\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.3, loss='deviance', max_depth=2,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=2, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=3, presort='auto',\n",
       "              random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note:如果能加上对调整参数，如果树深等参数，对结果的影响的分析会更棒~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结：\n",
    "- 在给定的参数列表中，最优参数：{'n_estimators': 3, 'learning_rate': 0.3, 'max_depth': 2, 'min_samples_leaf': 2}\n",
    "- 在上述的参数组合条件下，5折交叉验证的最优评分是：0.9353740154891983"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 收获：到此为止，机器学习的算法较上周有更深刻的认识，并且学习了几个数据科学比赛的案例，对特征工程和特征提起也有了更深刻的认识；\n",
    "- 欠缺：需要学习的东西还很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "给寒老师的教学点赞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
