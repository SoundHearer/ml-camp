{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第七周(机器学习-模型调优与融合)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月23日至3月25日期间完成，最晚提交时间本周日（3月25日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam7后，进行作答。例如wangwei-exam7\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/7/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王玺 \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共6题，1，2题每题5分，后4题每题10分，共计50分)\n",
    "\n",
    "- note :50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 机器学习任务中，通常会将给定数据切分为训练集，验证集，测试集，请回答这三类数据集各自的用途。\n",
    "\n",
    "- note: 一般数据科学比赛时的测试集是不带标签，防止做BI，但本地做项目时候，测试集本就是数据集中的一部分哈。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "训练集(train set) 是带标签的，用来拟合，训练模型，确定模型参数的(如LR中的w等),从而用这部分数据来建立模型。  \n",
    "\n",
    "验证集（validation set）也是带标签的，用来做模型选择（model selection），即做模型的最终优化及确定的；常用的方法是是留少部分做测试集。然后对其余N个样本采用K折交叉验证法。就是将样本打乱，然后均匀分成K份，轮流选择其中K－1份训练，剩余的一份做验证，计算预测误差平方和，最后把K次的预测误差平方和再做平均作为选择最优模型结构的依据。K取N，就是留一法（leave one out）。  \n",
    "\n",
    "测试集（test set）也是带标签的，是为了测试已经训练好的模型的推广能力。当然，test set这并不能保证模型的正确性，他只是展示了相似的数据用此模型会得出的结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.如何将数据转换成xgboost内置用法的libsvm数据格式，以及该格式数据是如何解读的?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "libsvm格式十分适合存储稀疏的数据集。最左边的一列是标签的值。其余位置存放的是有值的特征的下标和它对应的值。\n",
    "label index1:value1 index2:value2……\n",
    "下面是将数据转换成libsvm的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "dtrain = xgb.DMatrix(X_train, y_train)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 对于自动特征选择，通常有哪三类方法？试写出每类方式的思想，以及在sklearn中的代码实现（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.过滤式(filter)            \n",
    "Filter特征选择方法是一种启发式方法，其基本思想是：制定一个准则，用来衡量每个特征/属性，对目标属性的重要性程度，以此来对所有特征/属性进行排序，或者进行优选操作。常用的衡量准则有假设检验的p值、相关系数、互信息、信息增益等。\n",
    "\n",
    "1）方差选择法：使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：    \n",
    "from sklearn.feature_selection import VarianceThreshold       \n",
    "VarianceThreshold(threshold=3).fit_transform(iris.data)     \n",
    "\n",
    "2） 相关系数法：使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：     \n",
    "from sklearn.feature_selection import SelectKBest       \n",
    "from scipy.stats import pearsonr      \n",
    "SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)   \n",
    "\n",
    "3）卡方检验：经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用feature_selection库的SelectKBest类结合卡方检验来选择特征的代码如下：       \n",
    "from sklearn.feature_selection import SelectKBest       \n",
    "from sklearn.feature_selection import chi2      \n",
    "SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n",
    "\n",
    "4）互信息法：经典的互信息也是评价定性自变量对定性因变量的相关性的，为了处理定量数据，最大信息系数法被提出，使用feature_selection库的SelectKBest类结合最大信息系数法来选择特征的代码如下：       \n",
    "from sklearn.feature_selection import SelectKBest      \n",
    "from minepy import MINE      \n",
    " def mic(x, y):    \n",
    "     m = MINE()     \n",
    "     m.compute_score(x, y)     \n",
    "     return (m.mic(), 0.5)      \n",
    "SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform        \n",
    "\n",
    "2.包裹式（wrapper）       \n",
    "其主要思想是：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA，PSO，DE，ABC等。  \n",
    "\n",
    "1）使用feature_selection库的RFE类来选择特征的代码如下：       \n",
    "from sklearn.feature_selection import RFE        \n",
    "from sklearn.linear_model import LogisticRegression                   \n",
    "RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)              \n",
    "\n",
    "3.嵌入式（embedding）    \n",
    "嵌入式特征选择将特征选择过程和机器训练过程融合为一体。两者在同一优化过程中完成，即在学习器训练过程中自动进行了特征选择。。比如lasso回归，岭回归，弹性网络（Elastic Net）等。\n",
    "\n",
    "1）基于惩罚项的特征选择法:使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：     \n",
    "from sklearn.feature_selection import SelectFromModel             \n",
    "from sklearn.linear_model import LogisticRegression                       \n",
    "SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)    \n",
    "\n",
    "2）使用feature_selection库的SelectFromModel类结合带L1以及L2惩罚项的逻辑回归模型，来选择特征的代码如下：              \n",
    "from sklearn.feature_selection import SelectFromModel           \n",
    "SelectFromModel(LR(threshold=0.5, C=0.1)).fit_transform(iris.data, iris.target)    \n",
    "\n",
    "3）基于树模型的特征选择法：树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：                 \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier               \n",
    "SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.网格搜索交叉验证的作用是什么，并简述网格搜索交叉验证是如何运行的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1）交叉验证是评估方法，将数据集D划分为k个大小相似的互斥子集每个子集Di都尽可能保持数据分布的一致性，即从D中通过分层采样所得。训练时，每次用k-1个子集的并集作为训练集，余下的一个子集作为测试集；如此，可获得k组训练集和测试集，从而进行k次训练和测试，最终返回k次测试结果的均值。     \n",
    "\n",
    "2）网格搜索法是参数选择的方法，通过穷举法选择出一系列的参数组。它将各个参数可能的取值进行排列组合，列出所有可能的组合结果生成“网格”。    \n",
    "\n",
    "3）网格搜索加交叉验证就构成了一个不错的调参方式，首先由网格搜索选出各种参数组合，然后将各组合用于模型训练，并使用交叉验证对表现进行评估。在拟合函数尝试了所有的参数组合后，返回一个合适的分类器，自动调整至最佳参数组合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.工业界上模型整合有三大类方式？试简述每类方式其思想？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.集体智慧(投票器/BAGGING/随机森林（样本特征随机）。           \n",
    "这一类方式的思想是每个模型的学习原理是不同的，集合各种模型的学习结果在一起，最终的学习效果也应该会更好些，是并行的算法。        \n",
    "1）投票器是让融合的各个模型投标表决从而控制过拟合。          \n",
    "2）BAGGING会对数据做采样从而其中每个模型看到的数据都是不一样的。进而使各个模型的差异化增大。              \n",
    "3）随机森林比bagging更随机更多样化，更能缓解过拟合。 \n",
    "\n",
    "2.站在巨人的肩膀看得更远STACKING和BLENDING（用上层模型输出 做为线性模型的特征）       \n",
    "这类方式是分两层的结构。      \n",
    "1）Stacking会在上一层模型输出的结果的基础上再去学习一个模型。用上一层estimator的结果作为下一层的特征。     \n",
    "2）Blending是一个弱化版的stacking它第二层的模型是线性的。      \n",
    "\n",
    "3.一万小时定律BOOSTING（特点：反复训练，分配不同权重，简单模型叠加adaboost，串行运行）             \n",
    "1）adaboost 串行运行的，每次运行调整学的不好的部分。      \n",
    "2）boosting 串行运行的，用比较弱的模型通过多次迭代处理上一轮学习的不好的数据来调整权重，最终达到比较不错的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  我们可以将xgboost的中众多参数分类为哪三类？请写出哪些参数可以用什么方式用来控制过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.General Parameters(通用参数)：用来设置XGboost的宏观功能。 包括booster，silent，nthread等\n",
    "\n",
    "2.Boosterparameters：基于模型的参数。     \n",
    "max_depth,min_child_weight和gamma都是通过控制模型的复杂度来控制过拟合的。   \n",
    "&emsp;1）gamma参数：表示在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守。同时也就一定程度上抵抗了过拟合。   \n",
    "&emsp;2）max_depth：树深太深的话模型就过拟合了，所以通过设置树的深度也可以控制过拟合。 \n",
    "\n",
    "subsample,colsample_bytree是通过增加随机性来使训练更能抵抗噪声增强健壮性来控制过拟合的。    \n",
    "&emsp;1） subsample:通过随机抽取部分样本来进行训练可以防止过拟合。   \n",
    "&emsp;2） eta：降低学习率也是控制过拟合的一种方法，但是要注意增加num_round（学习轮数）。\n",
    "\n",
    "3.task parameters学习任务参数：这个参数用来控制理想的优化目标和每一步结果的度量方法。     \n",
    "objective：代价函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，每题25分，共计50分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用XGBoost内置方式，导入iris数据完成分类问题（要求以不同参数设置xgboost运行并比对），最后给出实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[2]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "训练集准确率等于 0.9619047619047619\n",
      "测试集准确率等于 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris   \n",
    "from sklearn.model_selection import train_test_split  \n",
    "import xgboost as xgb  \n",
    "from matplotlib import pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)  \n",
    "                                                              \n",
    "dtrain = xgb.DMatrix(X_train, y_train)  \n",
    "dtest = xgb.DMatrix(X_test, y_test)  \n",
    "\n",
    "\n",
    "def xgboost(params,num_rounds,dtrain,dtest):  \n",
    "    watch_list = [(dtrain, 'train'),(dtest, 'eval')]  \n",
    "    model = xgb.train(params, dtrain, num_rounds,watch_list)  \n",
    "\n",
    "    pred = model.predict(dtrain)  \n",
    "    labels = dtrain.get_label()\n",
    "    e=0\n",
    "    for index in range(len(pred)):\n",
    "        if int(pred[index]==labels[index]):\n",
    "            e += 1        \n",
    "    print('训练集准确率等于',e/len(pred))\n",
    "\n",
    "    pred = model.predict(dtest)  \n",
    "    labels = dtest.get_label()\n",
    "    e=0\n",
    "    for index in range(len(pred)):\n",
    "        if int(pred[index]==labels[index]):\n",
    "            e += 1        \n",
    "    print('测试集准确率等于',e/len(pred))\n",
    "##参数    \n",
    "params={  \n",
    "    'booster':'gbtree',  \n",
    "    'silent': 1,                           # 设置成1则没有运行信息输出，最好是设置为0.  \n",
    "    'eta': 0.6,                           # 如同学习率(等价于learn_rate)  \n",
    "    'min_child_weight':3,                  # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言，假设 h 在 0.01 附  \n",
    "                                           # 近，min_child_weight 为 1 意味着叶子节点中最少需要包含100个样本。   \n",
    "                                           # 这个参数控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。  \n",
    "    'max_depth':3,                         # 构建树的深度，越大越容易过拟合  \n",
    "    'gamma':0.1,                           # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守  \n",
    "    'subsample':0.7,                       # 随机采样训练样本  \n",
    "    'colsample_bytree':0.7,                # 生成树时进行的列采样   \n",
    "    'lambda':2,                            # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。  \n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':3,\n",
    "#     'seed':1000\n",
    "\n",
    "}  \n",
    "num_rounds = 3                     # 迭代次数  \n",
    "xgboost(params,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调参与分析：\n",
    "(有一句mmp我一定要讲，这数据集太小了，动不动就过拟合了。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.首先来看 max_depth 对模型的影响：         \n",
    "原例子中max_depth为3，现在设置max_depth为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.066667\n",
      "[2]\ttrain-merror:0.047619\teval-merror:0.066667\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "p1 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':0.1, 'subsample':0.7,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3   \n",
    "xgboost(p1,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可见虽然训练集准确率下降了但是测试集的准确率却上升了，这说明树的深度由3变为了1以后对训练集的描述泛化了，有效的减少了过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 然后再看gamma对模型的影响        \n",
    "Gamma指定了节点分裂所需的最小损失函数下降值。现在保持'max_depth'依然为1,现在设置gamma为10看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[2]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "p2 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.7,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3 \n",
    "xgboost(p2,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上可见虽然训练集准确率不变的情况下测试集的准确率上升了，gamma越大节点越难分裂，算法越保守。同时也就一定程度上抵抗了过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.接下来来调整subsample参数          \n",
    "分别设置subsample为0.9和0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[2]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "p3 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.9,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3 \n",
    "xgboost(p3,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.628571\teval-merror:0.755556\n",
      "[1]\ttrain-merror:0.628571\teval-merror:0.755556\n",
      "[2]\ttrain-merror:0.628571\teval-merror:0.755556\n",
      "训练集准确率等于 0.37142857142857144\n",
      "测试集准确率等于 0.24444444444444444\n"
     ]
    }
   ],
   "source": [
    "p3 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.2,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3 \n",
    "xgboost(p3,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别设置subsample为0.2和0.9时，模型分别为过拟合与欠拟合的状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.经过测试colsample_bytree 参数也可达到与subsample类似的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.228571\teval-merror:0.311111\n",
      "[1]\ttrain-merror:0.228571\teval-merror:0.311111\n",
      "[2]\ttrain-merror:0.228571\teval-merror:0.311111\n",
      "训练集准确率等于 0.7714285714285715\n",
      "测试集准确率等于 0.6888888888888889\n"
     ]
    }
   ],
   "source": [
    "p4 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.9,\\\n",
    "    'colsample_bytree':0.2,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3 \n",
    "xgboost(p4,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[2]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "p4 = { 'booster':'gbtree','silent': 1,'eta': 0.6, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.9,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 3 \n",
    "xgboost(p4,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.最后调整学习率\n",
    "初始学习率为0.6，现在分别设置为0.01和10："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "p5 = { 'booster':'gbtree','silent': 1,'eta': 0.001, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.7,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 2 \n",
    "xgboost(p5,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "训练集准确率等于 0.9523809523809523\n",
      "测试集准确率等于 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "p6= { 'booster':'gbtree','silent': 1,'eta': 10, 'min_child_weight':3, 'max_depth':1,'gamma':10, 'subsample':0.7,\\\n",
    "    'colsample_bytree':0.7,  'lambda':2, 'objective':'multi:softmax','num_class':3,}  \n",
    "num_rounds = 2 \n",
    "xgboost(p5,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在迭代次数都为2的时候学习率为10的过拟合了，下面维持学习率不变，增加迭代次数num_rounds为30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.038095\teval-merror:0.088889\n",
      "[1]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[2]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[3]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[4]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[5]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[6]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[7]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[8]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[9]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[10]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[11]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[12]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[13]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[14]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[15]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[16]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[17]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[18]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[19]\ttrain-merror:0.047619\teval-merror:0.044444\n",
      "[20]\ttrain-merror:0.038095\teval-merror:0.044444\n",
      "[21]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[22]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[23]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[24]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[25]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[26]\ttrain-merror:0.057143\teval-merror:0.044444\n",
      "[27]\ttrain-merror:0.057143\teval-merror:0.022222\n",
      "[28]\ttrain-merror:0.057143\teval-merror:0.022222\n",
      "[29]\ttrain-merror:0.057143\teval-merror:0.022222\n",
      "训练集准确率等于 0.9428571428571428\n",
      "测试集准确率等于 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "num_rounds=30\n",
    "xgboost(p5,num_rounds,dtrain,dtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集准确率提升了由上可见学习率减小的同时增加迭代次数的话，也可以提高模型表现效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用XGBoost的sklearn接口，对KaggleCredit2数据完成信用卡欺诈项目的建模及分析（要求以不同参数设置xgboost运行并比对），最后给出实验总结\n",
    "\n",
    "- KaggleCredit2数据文件 位于/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip，请勿复制或移动该文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split      \n",
    "from sklearn import metrics  \n",
    "from xgboost.sklearn import XGBClassifier  \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip', 'r') as z:\n",
    "    f = z.open('KaggleCredit2.csv')\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "y = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(  \n",
    "silent=0 ,                                      #设置成1则没有运行信息输出，最好是设置为0.是否在运  \n",
    "                                                #行升级时打印消息。  \n",
    "#nthread=4,                                     # cpu 线程数 默认最大  \n",
    "learning_rate= 0.01,                             # 如同学习率(eta)  \n",
    "min_child_weight=1,   \n",
    "                                                # 这个参数默认是 1，是每个叶子里面 h 的和至少是多  \n",
    "                                                # 少，对正负样本不均衡时的 0-1 分类而言  \n",
    "                                                #，假设 h 在 0.01 附近，min_child_weight 为 1 意味  \n",
    "                                                # 着叶子节点中最少需要包含 100 个样本。  \n",
    "                                                # 这个参数非常影响结果，控制叶子节点中二阶导的和的  \n",
    "                                                # 最小值，该参数值越小，越容易 overfitting。  \n",
    "max_depth=6,                                    # 构建树的深度，越大越容易过拟合  \n",
    "gamma=0,                                        # 树的叶子节点上作进一步分区所需的最小损失减少,越大  \n",
    "                                                # 越保守，一般0.1、0.2这样子。  \n",
    "subsample=1,                                    # 随机采样训练样本 训练实例的子采样比  \n",
    "max_delta_step=0,                               # 最大增量步长，我们允许每个树的权重估计。  \n",
    "colsample_bytree=1,                             # 生成树时进行的列采样   \n",
    "reg_lambda=1,                                   # 控制模型复杂度的权重值的L2正则化项参数，参数越  \n",
    "                                                # 大，模型越不容易过拟合。  \n",
    "#reg_alpha=0,                                   # L1 正则项参数  \n",
    "#scale_pos_weight=1,                            # 如果取值大于0的话，在类别样本不平衡的情况下有助于     \n",
    "                                                # 快速收敛，平衡正负权重  \n",
    "#objective= 'multi:softmax',                    # 多分类的问题 指定学习任务和相应的学习目标  \n",
    "#num_class=10,                                  # 类别数，多分类与 multisoftmax 并用  \n",
    "n_estimators=100,                               # 树的个数  \n",
    "seed=1000                                       # 随机种子  \n",
    "#eval_metric= 'auc'  \n",
    ")  \n",
    "clf.fit(X_train,y_train,eval_metric='auc')  \n",
    "y_true, y_pred = y_test, clf.predict(X_test)  \n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_true, y_pred)) #显示精度，越大越好  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用网格搜索来进行调参：     \n",
    "1.首先对 max_depth 进行设置分别为1,2,5,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.849481 (0.001344) with: {'max_depth': 1}\n",
      "0.854458 (0.000561) with: {'max_depth': 2}\n",
      "0.854802 (0.000341) with: {'max_depth': 5}\n",
      "0.841760 (0.001650) with: {'max_depth': 10}\n",
      "Best: 0.854802 using {'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = XGBClassifier() \n",
    "param_grid= {'max_depth':[1,2,5,10]}\n",
    "grid_search = GridSearchCV(estimator = XGBClassifier(silent=0,max_depth=6,gamma=0,n_estimators=100,subsample=1,reg_lambda=1,seed=10), param_grid=param_grid, scoring='roc_auc')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示树深度选5的效果是最好的，大了会过拟合，小了则为欠拟合。    \n",
    "2.下面进行gamma参数调优Gamma参数取值范围可以很大，我这里取值范围设置为6，10，15.（不敢设太多怕卡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855295 (0.000656) with: {'gamma': 6}\n",
      "0.855476 (0.000593) with: {'gamma': 10}\n",
      "0.854653 (0.000437) with: {'gamma': 15}\n",
      "Best: 0.855476 using {'gamma': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = XGBClassifier() \n",
    "param_grid= {'gamma':[6, 10, 15]}\n",
    "grid_search = GridSearchCV(estimator = XGBClassifier(silent=0,max_depth=6,gamma=0,n_estimators=100,subsample=1,reg_lambda=1,seed=1000), param_grid=param_grid, scoring='roc_auc')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上所示gamma为10的得分最高。         \n",
    "3.下面subsample 和 colsample_bytree 俩参数一起调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.850987 (0.001560) with: {'subsample': 0.3, 'colsample_bytree': 0.3}\n",
      "0.853690 (0.001188) with: {'subsample': 0.6, 'colsample_bytree': 0.3}\n",
      "0.854334 (0.001176) with: {'subsample': 0.9, 'colsample_bytree': 0.3}\n",
      "0.851424 (0.000463) with: {'subsample': 0.3, 'colsample_bytree': 0.6}\n",
      "0.853037 (0.000586) with: {'subsample': 0.6, 'colsample_bytree': 0.6}\n",
      "0.854497 (0.000496) with: {'subsample': 0.9, 'colsample_bytree': 0.6}\n",
      "0.851143 (0.000570) with: {'subsample': 0.3, 'colsample_bytree': 0.9}\n",
      "0.852407 (0.000987) with: {'subsample': 0.6, 'colsample_bytree': 0.9}\n",
      "0.853312 (0.000522) with: {'subsample': 0.9, 'colsample_bytree': 0.9}\n",
      "Best: 0.854497 using {'subsample': 0.9, 'colsample_bytree': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = XGBClassifier() \n",
    "param_grid= {'subsample':[0.3, 0.6, 0.9], 'colsample_bytree':[0.3, 0.6, 0.9]}\n",
    "grid_search = GridSearchCV(estimator = XGBClassifier(silent=0,max_depth=6,gamma=0,n_estimators=100,subsample=1,reg_lambda=1,seed=1000), param_grid=param_grid, scoring='roc_auc')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最佳选择为'colsample_bytree'= 0.6与 'subsample'= 0.9            \n",
    "4.最后依然是调学习率  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.845564 (0.004012) with: {'learning_rate': 0.01}\n",
      "0.853172 (0.000652) with: {'learning_rate': 0.1}\n",
      "0.787670 (0.002530) with: {'learning_rate': 1}\n",
      "0.556187 (0.014270) with: {'learning_rate': 10}\n",
      "Best: 0.853172 using {'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = XGBClassifier() \n",
    "param_grid= {'learning_rate':[0.01, 0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(estimator = XGBClassifier(silent=0,max_depth=6,gamma=0,n_estimators=100,subsample=1,reg_lambda=1,seed=1000), param_grid=param_grid, scoring='roc_auc')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最佳选择为0.1    \n",
    "综上所得，在用xboost模型时合理调整参数的取值可以在很大程度上通过使模型避免过拟合与欠拟合的方式，改善模型对数据集的表示效果。sklearn和xgboost本身也都提供了很多相应的调参和评估方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
