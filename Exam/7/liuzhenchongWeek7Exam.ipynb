{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第七周(机器学习-模型调优与融合)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月23日至3月25日期间完成，最晚提交时间本周日（3月25日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam7后，进行作答。例如wangwei-exam7\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/7/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>刘振冲</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共6题，1，2题每题5分，后4题每题10分，共计50分)\n",
    "\n",
    "- note: 47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 机器学习任务中，通常会将给定数据切分为训练集，验证集，测试集，请回答这三类数据集各自的用途。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    \n",
    "        在机器学习中，我们主要是为了防止模型的过拟合，通常会把数据集划分为三类，如果仅仅把所有的数据集只仅仅用于训练模型，则结果可能会很好低拟合数据，但是给定未知的数据集时，会出现无法预料的结果。划分比例一般为0.6:0.2:0.2；\n",
    "    训练集：在模型的建立的时候，需要让算法首先去通过训练达到我们需要的效果，拟合模型，通过设置分类器的参数，训练分类模型，让机器“知道”什么是对的，什么是错的。这就相当于一个小学生要学习新的知识点，训练集相当于练习册；\n",
    "    验证集：我们已经什么是对的，什么是错的，但是让机器去做，看看能不能达到预期的效果，与实际发生多少的偏差。相当于小学生做完练习题，要进行周测，验证学到的效果如何，如svn中的参数c和核函数等，一般验证集在交叉验证里应用的比较多；\n",
    "    测试集：机器已经通过训练集和验证集的测试，已经更改了之前的偏差，要进行最后阶段的测试，衡量该最优模型的性能和分类能力，相当于小学生进行最后的期末考，但是如果还出现较大的出入，说明出现了问题，需要再重新去审视之前的过程。\n",
    "      \n",
    "        简言之，training set是用来训练模型或确定模型参数的，如ANN中权值等； validation set是用来做模型选择（model selection），即做模型的最终优化及确定的，如ANN的结构；而 test set则纯粹是为了测试已经训练好的模型的推广能力。但是，test set这并不能保证模型的正确性，相似的数据用此模型会得出相似的结果。但实际应用中，一般只将数据集分成两类，即training set 和test set，大多数的时候并不涉及validation set。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.如何将数据转换成xgboost内置用法的libsvm数据格式，以及该格式数据是如何解读的?\n",
    "\n",
    "- 后面的答案也不需要，也对不上题目的要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    xgboost利用libsvm进行数据格式化，用来训练和预测，列如下面的例子：\n",
    "    \n",
    "    train.txt\n",
    "    1 101:1.2 102:0.03\n",
    "    0 1:2.1 10001:300 10002:400\n",
    "    0 0:1.3 1:0.3\n",
    "    1 0:0.01 1:0.3\n",
    "    0 0:0.2 1:0.3\n",
    "    \n",
    "    每一行代表一个单独的例子， 在第一行中的“1”是例子标签， “101”、“102”是特征指数，“1.2”、“0.03”是特征值，在二进制分类例子中，“1”被用来代表正样本，“0”则用来代表负样本，我们也可以用可能性值[0， 1]作为一个标签，来表示例子呈正样本的可能性。 \n",
    "    xgboost支持完成排序任务，我们支持按组输入格式，在真实世界的排序任务中，例子被分为成不同的小部分。例如，以上的例子文件，train.txt，部分文件被分为：\n",
    "    \n",
    "    train.txt.group\n",
    "    2\n",
    "    3\n",
    "    这意味着，数据集包含5个例子，第一部分的2为一组，之后的3为一组。在组文件中的数字实际上代表了每一组中的几个例子，在此配置下，你无需指定组文件的路径。如果某个文件的名字是“XXX”，那么xgboost就会自动去检查是否有个文件名字叫“XXX.group”在同样的路径下，再决定是否要读入数据。\n",
    "    \n",
    "    样例权重文件（Instance Weight File）\n",
    "    xgboost支持为不同样例的重要度提供不同的权重，例如，如果我们为下面的例子“train.txt”提供权值文件：\n",
    "    \n",
    "    train.txt.weight\n",
    "    1\n",
    "    0.5\n",
    "    0.5\n",
    "    1\n",
    "    0.5\n",
    "    这意味着，xgboost将会强调第一个文件和第四个文件，这也就是说，在训练的时候是正样本，这种情况与组文件很类似，如果文件中有个叫“XXX”，xgboost将会检查是否有个文件名叫“XXX.weight”。权重值将会被包含在“xxx.buffer”文件中。如果想更新权值，需要去删除原来的“xxx.buffer”文件，或者重新启动xgboost。\n",
    "    \n",
    "    Initial Margin file\n",
    "    xgboost支持为每一个例子提供一个初始化的边界预测。例如，如果我们有一个用逻辑回归为“train.txt”的初始化的预测，我们产生如下的文件：\n",
    "    train.txt.base_margin\n",
    "    -0.4\n",
    "    1.0\n",
    "    3.4\n",
    "    XGBoost 将把这些值作为初始化的边界预测，重要的一点是，base_margin在转换之前应当是边界预测，所以，如果你正在做逻辑损失，你需要在进行逻辑转换之前去输入值，如果用xgboost预测，应当用pred_margin=1来输出边界值。\n",
    "    \n",
    "    libsvm中总共有五种方法，对应如下：\n",
    "    svm.libsvm.cross_validation\t    交叉验证\n",
    "    svm.libsvm.decision_function\t预测边界(libsvm name for this is predict_values)\n",
    "    svm.libsvm.fit\t                训练使用libsvm的模型(low-level method)\n",
    "    svm.libsvm.predict\t            预测给定模型的值(low-level method)\n",
    "    svm.libsvm.predict_proba\t    预测可能性\n",
    "    \n",
    "    LIBSVM 使用的一般步骤是:\n",
    "    1) 按照LIBSVM软件包所要求的格式准备数据集;\n",
    "    2) 对数据进行简单的缩放操作;\n",
    "    3) 考虑选用RBF 核函数;\n",
    "    4) 采用交叉验证选择最佳参数C与g ;\n",
    "    5) 采用最佳参数C与g 对整个训练集进行训练获取支持向量机模型;\n",
    "    6) 利用获取的模型进行测试与预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 对于自动特征选择，通常有哪三类方法？试写出每类方式的思想，以及在sklearn中的代码实现（面试题）\n",
    "\n",
    "- note: 基于模型的思想，没写全，代码实现很好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    \n",
    "    自动特征选类有以下三类：\n",
    "    1.单变量分析：针对一个变量或者多个独立的变量进行分析，以了解某一现象的本质或者发展规律.单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。\n",
    "    # 导入包\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.feature_selection import SelectPercentile\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    cancer = load_breast_cancer() # 载入数据\n",
    "\n",
    "    # 得到确定的随机率\n",
    "    rng = np.random.RandomState(42)\n",
    "    noise = rng.normal(size=(len(cancer.data), 50))\n",
    "    # 为数据增加噪音\n",
    "    # the first 30 features are from the dataset, the next 50 are noise\n",
    "    X_w_noise = np.hstack([cancer.data, noise])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
    "    # use f_classif (the default) and SelectPercentile to select 10% of features:\n",
    "    select = SelectPercentile(percentile=50)\n",
    "    select.fit(X_train, y_train)\n",
    "    # transform training set:\n",
    "    X_train_selected = select.transform(X_train)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_train_selected.shape)\n",
    "    \n",
    "    from sklearn.feature_selection import f_classif, f_regression, chi2\n",
    "    F, p = f_classif(X_train, y_train)\n",
    "    plt.figure()\n",
    "    plt.plot(p, 'o')\n",
    "    \n",
    "    mask = select.get_support()\n",
    "    print(mask)\n",
    "    # visualize the mask. black is True, white is False\n",
    "    plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # transform test data:\n",
    "    X_test_selected = select.transform(X_test)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    print(\"Score with all features: %f\" % lr.score(X_test, y_test))\n",
    "    lr.fit(X_train_selected, y_train)\n",
    "    print(\"Score with only selected features: %f\" % lr.score(X_test_selected, y_test))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    2.基于模型的特征选择\n",
    "    \n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), threshold=\"median\")\n",
    "    select.fit(X_train, y_train)\n",
    "    X_train_l1 = select.transform(X_train)\n",
    "    print(X_train.shape)\n",
    "    print(X_train_l1.shape)\n",
    "    \n",
    "    mask = select.get_support()\n",
    "    # visualize the mask. black is True, white is False\n",
    "    plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "    \n",
    "    X_test_l1 = select.transform(X_test)\n",
    "    LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    3.RFE，逐步特征删除\n",
    "    \n",
    "    from sklearn.feature_selection import RFE\n",
    "    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)\n",
    "    #select = RFE(LogisticRegression(penalty=\"l1\"), n_features_to_select=40)\n",
    "    \n",
    "    select.fit(X_train, y_train)\n",
    "    # visualize the selected features:\n",
    "    mask = select.get_support()\n",
    "    plt.matshow(mask.reshape(1, -1), cmap='gray_r')\n",
    "    \n",
    "    X_train_rfe = select.transform(X_train)\n",
    "    X_test_rfe = select.transform(X_test)\n",
    "    \n",
    "    LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n",
    "    \n",
    "    select.score(X_test, y_test)\n",
    "    \n",
    "    sklearn 介绍两大类特征选择方法：\n",
    "    单变量特征选择方法，这里面主要包括皮尔森相关系数、最大信息系数、距离相关系数等，主要思想是衡量特征和标签变量之间的相关性；\n",
    "    基于模型的特征选择方法，这主要是指模型在训练过程中对特征的排序，如基于随机森林的特征选择、基于逻辑回归的特征选择等\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.网格搜索交叉验证的作用是什么，并简述网格搜索交叉验证是如何运行的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    网格搜索交叉验证的主要作用是寻找出相对的局部最优点，给定的任务比较重，无法直接找到最优点；\n",
    "    \n",
    "    网格搜索交叉验证的运行机制是首先将数据分成一个个小份，再在每个小份中找到一个相对最优点，接着通过不断的迭代，再比较一个个的相对最优点，最后找到无限逼近的点，这个点就几乎是全局最优点。（也是，相对来说，在实际工程中，无法找到绝对最优点）。网格搜索一般是找出最优参数从而使模型最优，属于暴力破解算法。例如，模型有三个参数A，B，C需要调节，每个参数分别有r，s，t个可能的取值，那么三个参数一共组合r\\times s \\times t 个情况，其中每一种组合都可能是最优参数。将每种组合带入模型，采用交叉验证的方法对模型进行评估，所以一共有 r\\times s \\times t 个对应的评估，选取其中最好的评估，其参数就是要找的最优参数。\n",
    "    网格搜索，简言之就是手动的给出一个模型中想要改动的所用的参数，程序自动的帮你使用穷举法来将所用的参数都运行一遍。然后，我们需要一种可靠的评分方法，对每个最大深度的决策树模型都进行评分，这其中非常经典的一种方法就是交叉验证；\n",
    "    交叉验证，将数据集进行分割，将原始数据集分为训练集和测试集。\n",
    "    以K折交叉验证为例，在K折交叉验证中，我们用到的数据是训练集中的所有数据。我们将训练集的所有数据平均划分成K份（通常选择K=10），取第K份作为验证集，它的作用就像我们用来估计高考分数的模拟题，余下的K-1份作为交叉验证的训练集。\n",
    "    运行方式：\n",
    "    （1）从feature_format.py文件中导入featureFormat和targetFeatureSplit两个模块\n",
    "    （2）利用pickle.load(open(\"\", \"r\"))打开文件扔给数据字典data-dict\n",
    "    （3）读入数据，并将数据格式化为标签和特征的列表\n",
    "    （4）创建一个决策树分类器\n",
    "    （5）计算准确度\n",
    "    \n",
    "    下面是流程的代码实现:\n",
    "    # 导入包\n",
    "    from sklearn.svm import SVC\n",
    "    # 切分数据集、训练集\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "    # 验证集\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=1)\n",
    "    \n",
    "        print(\"Size of training set: %d   size of validation set: %d   size of test set: %d\" % (X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\n",
    "    best_score = 0\n",
    "    # gamma 和 C是最重要的参数\n",
    "    for gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        # for each combination of parameters\n",
    "        # train an SVC\n",
    "        svm = SVC(gamma=gamma, C=C)\n",
    "        svm.fit(X_train, y_train)\n",
    "        # 在测试集上评估\n",
    "        score = svm.score(X_valid, y_valid)\n",
    "        # 如果得到相对最好的结果，保存下来\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "    # 重新建模\n",
    "    svm = SVC(**best_parameters)\n",
    "    svm.fit(X_trainval, y_trainval)\n",
    "    # 计算准确度\n",
    "    test_score = svm.score(X_test, y_test)\n",
    "    print(\"best score on validation set: \", best_score)\n",
    "    print(\"best parameters: \", best_parameters)\n",
    "    print(\"test set score with best parameters: \", test_score)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.工业界上模型整合有三大类方式？试简述每类方式其思想？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    模型整合的几条规则：\n",
    "    群众的智慧是伟大的，集体的智慧是无穷的\n",
    "        voting投票器：\n",
    "            单个模型很难控制过拟合，多数表决，多个模型之间的差异越大，它们之间的融合度有可能更高。\n",
    "                \n",
    "        bagging\n",
    "        随机森林\n",
    "    站在巨人的肩膀上\n",
    "        模型stacking，stacking的基本思想是用一些基分类器进行分类，然后使用令一个分类器对结果进行整合，使用多次的CV会比较稳健\n",
    "        blending，Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如说10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合。说白了，就是把Stacking流程中的K-Fold CV 改成 HoldOut CV。\n",
    "        \n",
    "    一万小时定理\n",
    "        boosting\n",
    "        \n",
    "    加权表决融合\n",
    "    多数表决的融合方式默认了所有模型的重要度是一样的，但通常情况下我们会更重视表现较好的模型而需要赋予更大的权值。在加权表决的情况下，表现较差的模型只能通过与其他模型获得一样的结果来增强自己的说服力。\n",
    "    \n",
    "    对结果取平均\n",
    "    取均值的做法常常可以减少过拟合现象。在机器学习的应用上，过拟合现象是很普遍的，根本问题是训练数据量不足以支撑复杂的模型，导致模型学习到数据集上的噪音，这样产生的问题是模型很难泛化，因为模型“考虑”得过分片面。\n",
    "    但如果对结果取平均，可以在一定程度上减轻过拟合现象。图中所示，单个模型因为过拟合产生了绿色的决策边界，但事实上黑色的决策边界因为有更好的泛化能力从而有更好的效果。如果通过拟合多个模型并对模型结果取平均，对这些噪音点的考虑就会因为结果拉平均的原因而减少，决策边界也会慢慢的往黑色线靠拢。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  我们可以将xgboost的中众多参数分类为哪三类？请写出哪些参数可以用什么方式用来控制过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    xgboost的众多参数可以分为3类：通用参数/general parameters, 集成(增强)参数/booster parameters 和 任务参数/task parameter\n",
    "    通用参数/general parameters：宏观函数控制，用来控制XGBoost的宏观功能。\n",
    "        对应的参数有：booster [default=gbtree]、silent [default=0]、thread、num_pbuffer、num_feature\n",
    "        控制过拟合的方式是\n",
    "        \n",
    "        \n",
    "    集成(增强)参数/booster parameters：控制训练目标的表现。\n",
    "        对应的参数有：eta、gamma、range、max_depth、min_child_weight、max_delta_step、\n",
    "        subsample、colsample_bytree、colsample_bylevel、lambda、alpha\n",
    "        控制过拟合的方式是：\n",
    "        eta [default=0.3, 可以视作学习率]，防止过拟合是通过更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。 eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3\n",
    "        max_depth[default=6] 和GBM中的参数相同，这个值为树的最大深度。max_depth越大，模型会学到更具体更局部的样本，使模型更复杂/可能发生过拟合。需要使用CV函数来进行调优。范围：[0,∞]，典型值：3-10\n",
    "        min_child_weight[default=1] 决定最小叶子节点样本权重和。和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。范围： [0,∞]\n",
    "    \n",
    "    任务参数/task parameters：\n",
    "    对应的参数有：objective、base_score、eval_metric、poisson-nloglik、seed\n",
    "    控制过拟合的方式是：\n",
    "         seed(默认0) 随机数的种子设置它可以复现随机数据的结果，也可以用于调整参数\n",
    "         \n",
    "         \n",
    "    总之，调参的一般方法：\n",
    "    （1）选择较高的学习率（参数：eta），并选择对应此学习率理想的树数量（n_estimators） \n",
    "    （2）学习率以工具包的默认值\n",
    "    （3）XGBoost直接引用函数“cv”可以 在每次迭代中使用交叉验证，并返回理想的树数量；\n",
    "    （4）可以import两种XGBoost：一是直接引xgboost（用“cv”函数调整树的数目），二是XGBClassifier-xgboost的sklearn包（用GridSearchCV调整其他参数）\n",
    "    （5）对于给定的学习率和树数量，进行树参数调优\n",
    "    （6）正则化参数（lambda，alpha）的调优\n",
    "    （7）降低学习率，确定理想参数。\n",
    "    \n",
    "    XGBoost的优势：XGBoost算法可以给预测模型带来能力的提升。\n",
    "    1、正则化（标准GBM的实现没有像XGBoost这样的正则化步骤。正则化对减少过拟合也是有帮助的。）\n",
    "    2、并行处理（XGBoost可以实现并行处理，相比GBM有了速度的飞跃。 不过，众所周知，Boosting算法是顺序处理的，它怎么可能并行呢？每一课树的构造都依赖于前一棵树，那具体是什么让我们能用多核处理器去构造一个树呢？我希望你理解了这句话的意思。 XGBoost 也支持Hadoop实现。）\n",
    "    3、高度的灵活性（XGBoost 允许用户定义自定义优化目标和评价标准 它对模型增加了一个全新的维度，所以我们的处理不会受到任何限制。）\n",
    "    4、缺失值处理（XGBoost内置处理缺失值的规则。 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，以此来作为缺失值的取值。XGBoost在不同节点遇到缺失值时采用不同的处理方法，并且会学习未来遇到缺失值时的处理方法。）\n",
    "    5、剪枝（当分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。 XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。 这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂。）\n",
    "    6、内置交叉验证（XGBoost允许在每一轮boosting迭代中使用交叉验证。因此，可以方便地获得最优boosting迭代次数。 而GBM使用网格搜索，只能检测有限个值。）\n",
    "    7、在已有的模型基础上继续（XGBoost可以在上一轮的结果上继续训练。这个特性在某些特定的应用上是一个巨大的优势。 sklearn中的GBM的实现也有这个功能，两种算法在这一点上是一致的。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，每题25分，共计50分)\n",
    "\n",
    "- note:35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用XGBoost内置方式，导入iris数据完成分类问题（要求以不同参数设置xgboost运行并比对），最后给出实验总结\n",
    "\n",
    "-note : 直接用了sklearn接口的fitk啊?题目要求就是使用内置方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgboost的内置用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入包，载入数据\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集和测试集\n",
    "d_train = xgb.DMatrix(data=X[:120,:], label=Y[:120])\n",
    "d_test = xgb.DMatrix(data=X[120:,:], label=Y[120:])\n",
    "\n",
    "watch_list = [(d_test,'eval'), (d_train,'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.166667\ttrain-merror:0.025\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[5]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[6]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[7]\teval-merror:0.266667\ttrain-merror:0.025\n"
     ]
    }
   ],
   "source": [
    "#参数设定\n",
    "param = {'max_depth':1, 'eta':0.02, 'objective':'multi:softmax', 'num_class':3}\n",
    "        # 最大深度为5轮，学习率为0.02   分类方式为多分类\n",
    "num_round = 8  # 迭代8轮\n",
    "bst = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  2.,  1.,  2.,  2.,  1.,  1.,  2.,  1.,  2.,  2.,  2.,\n",
       "        1.,  1.,  2.,  2.,  2.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.predict(d_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:1\ttrain-merror:0\n",
      "[1]\teval-merror:1\ttrain-merror:0\n",
      "[2]\teval-merror:1\ttrain-merror:0\n",
      "[3]\teval-merror:1\ttrain-merror:0\n",
      "[4]\teval-merror:1\ttrain-merror:0\n",
      "[5]\teval-merror:1\ttrain-merror:0\n",
      "[6]\teval-merror:1\ttrain-merror:0\n",
      "[7]\teval-merror:1\ttrain-merror:0\n"
     ]
    }
   ],
   "source": [
    "# 训练集和测试集\n",
    "\n",
    "# d_train = xgb.DMatrix(data=X[:100,:], label=Y[:100])\n",
    "# d_test = xgb.DMatrix(data=X[100:,:], label=Y[100:])\n",
    "\n",
    "# watch_list = [(d_test,'eval'), (d_train,'train')]\n",
    "\n",
    "# # 如下是利用二分类\n",
    "# param2 = {'maxth_depth':4, 'eta':0.01, 'objective': 'binary:logistic'}\n",
    "# num_round = 8\n",
    "# bst2 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    比较：取不同的训练集和测试集时，得到不同的结果，差异比较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  2.,  2.,  2.,  2.,  2.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  1.,  2.,  1.,  2.,  1.,  2.,  2.,\n",
       "        1.,  1.,  2.,  1.,  2.,  2.,  2.,  1.,  1.,  2.,  2.,  2.,  1.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.predict(d_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = xgb.XGBClassifier(max_depth=3, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X[:120,:], Y[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[120:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.30811444,  0.36128399,  0.33060154],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.30811444,  0.36128399,  0.33060154],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.30811444,  0.36128399,  0.33060154],\n",
       "       [ 0.30811444,  0.36128399,  0.33060154],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31137398,  0.3613123 ,  0.32731369],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31137398,  0.3613123 ,  0.32731369],\n",
       "       [ 0.31137398,  0.3613123 ,  0.32731369],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.30811444,  0.36128399,  0.33060154],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31017882,  0.32325384,  0.36656734],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351],\n",
       "       [ 0.31012434,  0.31200218,  0.37787351]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X[120:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "实验总结：以上两种不同的参数方式，区别不是特别大。直觉上，某些部分出现了问题。需要再仔细斟酌。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用XGBoost的sklearn接口，对KaggleCredit2数据完成信用卡欺诈项目的建模及分析（要求以不同参数设置xgboost运行并比对），最后给出实验总结\n",
    "\n",
    "- KaggleCredit2数据文件 位于/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip，请勿复制或移动该文件\n",
    "\n",
    "- note: 细节还要再练习，，尤其是调参及比对，最后也没有总结 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入包，载入数据\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0                 1                              0.766127  45.0   \n",
       "1                 0                              0.957151  40.0   \n",
       "2                 0                              0.658180  38.0   \n",
       "3                 0                              0.233810  30.0   \n",
       "4                 0                              0.907239  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0   0.802982         9120.0   \n",
       "1                                   0.0   0.121876         2600.0   \n",
       "2                                   1.0   0.085113         3042.0   \n",
       "3                                   0.0   0.036050         3300.0   \n",
       "4                                   1.0   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip', 'r') as z:\n",
    "    f = z.open('KaggleCredit2.csv')\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112915, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeriousDlqin2yrs                           0\n",
       "RevolvingUtilizationOfUnsecuredLines       0\n",
       "age                                     4267\n",
       "NumberOfTime30-59DaysPastDueNotWorse       0\n",
       "DebtRatio                                  0\n",
       "MonthlyIncome                              0\n",
       "NumberOfOpenCreditLinesAndLoans            0\n",
       "NumberOfTimes90DaysLate                    0\n",
       "NumberRealEstateLoansOrLines               0\n",
       "NumberOfTime60-89DaysPastDueNotWorse       0\n",
       "NumberOfDependents                      4267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shapey = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X , y , test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将训练集与测试集变成DMtrix的数据格式\n",
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "test = xgb.DMatrix(X_test, label=y_test)\n",
    "watch_list = [(test, 'eval'), (train, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \n",
    "                          n_estimators=500,objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=1, missing=None, n_estimators=500,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9350820678018101"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "实验总结：得到的结果不是特别理想。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "本周老师讲的项目都是超级干货，需要反复消化。*✧⁺˚⁺ପ(๑･ω･)੭ु⁾⁾ 好好学习天天向上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "很好，再多来点干货！٩꒰▽ ꒱۶⁼³₌₃ 学习去咯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
