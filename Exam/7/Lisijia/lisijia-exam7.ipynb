{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第七周(机器学习-模型调优与融合)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月23日至3月25日期间完成，最晚提交时间本周日（3月25日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam7后，进行作答。例如wangwei-exam7\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/7/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>_李思佳____</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简答题(共6题，1，2题每题5分，后4题每题10分，共计50分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 机器学习任务中，通常会将给定数据切分为训练集，验证集，测试集，请回答这三类数据集各自的用途。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 训练集通过设置学习器的参数来训练模型；后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个学习模型。<br>\n",
    "2. 验证集用于模型的选择；作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。<br>\n",
    "3. 测试集用于最终对学习方法的评估；通过训练集和验证集得出最优模型后，使用测试集进行模型预测。用来衡量该最优模型的性能。即可以把测试集当做从来不存在的数据集，当已经确定模型参数后，使用测试集进行模型性能评价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.如何将数据转换成xgboost内置用法的libsvm数据格式，以及该格式数据是如何解读的?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用xgboost中DMatrix方法可以将数组或文本数据转换成libsvm数据格式。\n",
    "#### libsvm数据格式：Label 1:value 2:value 4:value….\n",
    "(1) Lable 为类别标识，如二分类问题则其取值为0或1；<br>\n",
    "(2) 数字1,2,4表示第一个特征，第二个特征,第四个特征；<br>\n",
    "(3) value则表示对应的特征值，如果特征值为0则空缺该特征（比如示例中的第三个特征）；<br>\n",
    "\n",
    "好处：可以减少内存的使用并提高做矩阵内积时的运算速度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 对于自动特征选择，通常有哪三类方法？试写出每类方式的思想，以及在sklearn中的代码实现（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 过滤式选择、包裹式选择、嵌入式选择\n",
    "过滤式和包裹式特征选择过程与学习器训练有明显的区别，而嵌入式特征选择则与学习过程融为一体。\n",
    "#### 1. 过滤式选择：也可以称为单变量分析，先对数据集进行特征选择，过滤出对结果影响程度较高的一些特征，再用过滤后的特征来训练模型。\n",
    "from sklearn.datasets import load_iris<br>\n",
    "from sklearn.feature_selection import SelectKBest<br>\n",
    "iris = load_iris()<br>\n",
    "X, y = iris.data, iris.target<br>\n",
    "X_kbBest = SelectKBest(k=2).fit_transform(X,y)\n",
    "#### 2. 包裹式选择：直接把最终要使用的学习器的性能作为特征子集的评价准则；即使用特征子集进行该学习器训练，选取出使得学习器误差最小的特征子集。\n",
    "from sklearn.feature_selection import RFE<br>\n",
    "from sklearn.ensemble import RandomForestClassifier<br>\n",
    "rf = RandomForestClassifier()<br>\n",
    "rfe = RFE(estimator=rf, n_features_to_select=2)<br>\n",
    "X_ref = rfe.fit_transform(X,y)\n",
    "#### 3. 过滤式选择：将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器过程中自动地进行了特征选择。另外可以引入L1或L2正则化，而引入L1正则化更容易获得‘稀疏’解。\n",
    "from sklearn.feature_selection import SelectFromModel <br>\n",
    "from sklearn.svm import LinearSVC <br>\n",
    "lsvc = LinearSVC(C=0.01, penalty='l1', dual=False).fit(X,y) <br>\n",
    "model = SelectFromModel(lsvc, prefit=True) <br>\n",
    "X_embedd = model.transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.网格搜索交叉验证的作用是什么，并简述网格搜索交叉验证是如何运行的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "从参数字典中选择出一组最优的超参数，同时评估该机器学习模型对训练数据集的稳定性能和泛化能力，能够避免过拟合问题。<br>\n",
    "1. 从参数字典依次选出每个参数的取值进行组合；<br>\n",
    "2. 对每一组参数取值进行交叉验证，取得评估结果的均值；<br>\n",
    "3. 选择出最大评估结果的均值所对应的一组参数取值；<br>\n",
    "4. 使用这组最佳参数的机器学习模型对整个训练集进行学习，得到最终的机器学习模型。<br>\n",
    "5. 使用最终的机器学习模型对测试集数据进行评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.工业界上模型整合有三大类方式？试简述每类方式其思想？（面试题）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 1. Voting、Bagging、Random Forest 并行模型融合\n",
    "Voting：每个基学习器使用全部样本进行训练，最后使用简单投票方式或简单平均法取得结果。<br>\n",
    "Bagging：每个基学习器使用部分样本进行训练，最后使用简单投票方式或简单平均法取得结果。<br>\n",
    "Random Forest：是一种基于树模型的Bagging的优化版本，以决策树为基学习器并使用部分样本和部分属性特征进行训练，最后使用简单投票方式或简单平均法取得结果；随机森林通常会有更好地泛化能力。\n",
    "#### 2. Stacking、Blending 并行模型融合\n",
    "Stacking：分为初级学习器和次级学习器；初级学习器通常为不同的学习模型，使用训练数据集进行训练，将输出结果作为次级学习器的输入再次进行模型训练，以次级学习器的输出作为结果。通常次级学习器是一个复杂模型（如逻辑斯蒂回归模型）。容易产生过拟合风险。<br>\n",
    "Blending：弱化版的Stacking，次级学习器使用线性加权平均或投票方式。\n",
    "#### 3. Boosting(AdaBoost、GDBT)  串行模型融合\n",
    "AdaBoost：每次仅学习一个基学习器和系数，根据前一个加法模型学习器的误差率对训练样本权重进行调整（加大评估错误的样本权重），根据调整后的样本权重（回归问题为残差）训练一个新的基学习器和系数；最终的学习器为每个乘以系数的基学习器的求和。<br>\n",
    "GDBT：先初始化一个弱学习器，然后每一轮迭代使用损失函数的负梯度在本轮学习器上的取值来拟合本轮损失的近似值，用于拟合下一个决策树（CART分类树或回归树），最终将每一轮的学习器进行叠加得到一个强学习器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  我们可以将xgboost的中众多参数分类为哪三类？请写出哪些参数可以用什么方式用来控制过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通用参数/general parameters<br>\n",
    "集成(增强)参数/booster parameters <br>\n",
    "任务参数/task parameters\n",
    "#### 可以用于控制过拟合的参数和对应方式：\n",
    "1. eta [default=0.3] 更新过程中用到的收缩步长，适当减小该值可以控制过拟合。\n",
    "2. gamma [default=0] 为了对树的叶子节点做进一步的分割而必须设置的损失减少的最小值，适当增大该值（算法越保守）可以控制过拟合。\n",
    "3. max_depth [default=6] 用于设置树的最大深度，适当减小该值可以控制过拟合。\n",
    "4. min_child_weight [default=1] 表示子树观测权重之和的最小值，适当增大该值可以控制过拟合。\n",
    "5. subsample [default=1] 表示观测的子样本的比率，按该比例随机抽取训练集进行模型学习将有助于防止过拟合现象。\n",
    "6. colsample_bytree [default=1] 表示用于构造每棵树时变量的子样本比率，按该比例随机抽取特征进行树的生长将有助于防止过拟合现象。\n",
    "7. lambda [default=1] L2 权重的L2正则化项，损失函数添加正则化项用于平衡损失函数和模型复杂度，有助于防止过拟合。\n",
    "8. alpha [default=0] L1 权重的L1正则化项，损失函数添加正则化项用于平衡损失函数和模型复杂度，有助于防止过拟合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，每题25分，共计50分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 使用XGBoost内置方式，导入iris数据完成分类问题（要求以不同参数设置xgboost运行并比对），最后给出实验总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(data=X[:120,:], label=Y[:120])\n",
    "d_test = xgb.DMatrix(data=X[120:,:], label=Y[120:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watch_list = [(d_test,'eval'), (d_train,'train')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、以参数eta作为单变量，衡量模型好坏 \n",
    "#### 1. eta=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0.016667\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':0.01, 'objective':'multi:softmax', 'num_class':3}\n",
    "num_round = 5\n",
    "b_eta_001 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.eta=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':1, 'objective':'multi:softmax', 'num_class':3}\n",
    "num_round = 5\n",
    "b_eta_1 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：参数eta取值从0.01到1，训练集误差降为0，而测试集误差不变，说明适当减少eta可以控制过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、以参数max_depth作为单变量，衡量模型好坏 \n",
    "#### 1. max_depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.166667\ttrain-merror:0.025\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0.025\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0.025\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':2, 'eta':0.01, 'objective':'multi:softmax', 'num_class':3}\n",
    "num_round = 5\n",
    "b_md_2 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0.016667\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':3, 'eta':0.01, 'objective':'multi:softmax', 'num_class':3}\n",
    "num_round = 5\n",
    "b_md_3 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. max_depth=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[1]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[2]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[3]\teval-merror:0.266667\ttrain-merror:0.016667\n",
      "[4]\teval-merror:0.266667\ttrain-merror:0.016667\n"
     ]
    }
   ],
   "source": [
    "param = {'max_depth':4, 'eta':0.01, 'objective':'multi:softmax', 'num_class':3}\n",
    "num_round = 5\n",
    "b_md_4 = xgb.train(param, d_train, num_round, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：随着参数max_dept的增大，模型在训练集上的误差越小，说明max_dept取值较小容易产生欠拟合现象，从而反过来说明取值过大容易产生过拟合现象，因此max_depth需要针对具体问题取一个合理的适中值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用XGBoost的sklearn接口，对KaggleCredit2数据完成信用卡欺诈项目的建模及分析（要求以不同参数设置xgboost运行并比对），最后给出实验总结\n",
    "\n",
    "- KaggleCredit2数据文件 位于/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip，请勿复制或移动该文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeriousDlqin2yrs</th>\n",
       "      <th>RevolvingUtilizationOfUnsecuredLines</th>\n",
       "      <th>age</th>\n",
       "      <th>NumberOfTime30-59DaysPastDueNotWorse</th>\n",
       "      <th>DebtRatio</th>\n",
       "      <th>MonthlyIncome</th>\n",
       "      <th>NumberOfOpenCreditLinesAndLoans</th>\n",
       "      <th>NumberOfTimes90DaysLate</th>\n",
       "      <th>NumberRealEstateLoansOrLines</th>\n",
       "      <th>NumberOfTime60-89DaysPastDueNotWorse</th>\n",
       "      <th>NumberOfDependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766127</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.957151</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121876</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.658180</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.085113</td>\n",
       "      <td>3042.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233810</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036050</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.907239</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>63588.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SeriousDlqin2yrs  RevolvingUtilizationOfUnsecuredLines   age  \\\n",
       "0                 1                              0.766127  45.0   \n",
       "1                 0                              0.957151  40.0   \n",
       "2                 0                              0.658180  38.0   \n",
       "3                 0                              0.233810  30.0   \n",
       "4                 0                              0.907239  49.0   \n",
       "\n",
       "   NumberOfTime30-59DaysPastDueNotWorse  DebtRatio  MonthlyIncome  \\\n",
       "0                                   2.0   0.802982         9120.0   \n",
       "1                                   0.0   0.121876         2600.0   \n",
       "2                                   1.0   0.085113         3042.0   \n",
       "3                                   0.0   0.036050         3300.0   \n",
       "4                                   1.0   0.024926        63588.0   \n",
       "\n",
       "   NumberOfOpenCreditLinesAndLoans  NumberOfTimes90DaysLate  \\\n",
       "0                             13.0                      0.0   \n",
       "1                              4.0                      0.0   \n",
       "2                              2.0                      1.0   \n",
       "3                              5.0                      0.0   \n",
       "4                              7.0                      0.0   \n",
       "\n",
       "   NumberRealEstateLoansOrLines  NumberOfTime60-89DaysPastDueNotWorse  \\\n",
       "0                           6.0                                   0.0   \n",
       "1                           0.0                                   0.0   \n",
       "2                           0.0                                   0.0   \n",
       "3                           0.0                                   0.0   \n",
       "4                           1.0                                   0.0   \n",
       "\n",
       "   NumberOfDependents  \n",
       "0                 2.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import zipfile\n",
    "with zipfile.ZipFile('/home/sxy-s3/0.Teacher/Exam/KaggleCredit2.csv.zip', 'r') as z:\n",
    "    f = z.open('KaggleCredit2.csv')\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeriousDlqin2yrs                           0\n",
       "RevolvingUtilizationOfUnsecuredLines       0\n",
       "age                                     4267\n",
       "NumberOfTime30-59DaysPastDueNotWorse       0\n",
       "DebtRatio                                  0\n",
       "MonthlyIncome                              0\n",
       "NumberOfOpenCreditLinesAndLoans            0\n",
       "NumberOfTimes90DaysLate                    0\n",
       "NumberRealEstateLoansOrLines               0\n",
       "NumberOfTime60-89DaysPastDueNotWorse       0\n",
       "NumberOfDependents                      4267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shapey = data['SeriousDlqin2yrs']\n",
    "X = data.drop('SeriousDlqin2yrs', axis=1)\n",
    "y = data['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0675085290724787, 0.0671894558574479)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.mean(),y_test.mean()\n",
    "# 测试集和训练集正负样本分布比例一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、参数max_depth作为单变量\n",
    "#### 1. max_depth=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_md_3 = xgb.XGBClassifier(max_depth=3, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_md_3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9342831897503866, 0.9416955059764868)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "model_md_3.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. max_depth=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9350931448346955, 0.9416955059764868)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_md_6 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='binary:logistic')\n",
    "model_md_6.fit(X_train,y_train)\n",
    "model_md_6.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. max_depth=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9332523378249025, 0.9416955059764868)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_md_9 = xgb.XGBClassifier(max_depth=9, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_md_9.fit(X_train,y_train)\n",
    "model_md_9.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 max_depth=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9295339076651203, 0.9416955059764868)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_md_15 = xgb.XGBClassifier(max_depth=15, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_md_15.fit(X_train,y_train)\n",
    "model_md_15.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：\n",
    "1. max_dept=3与max_dept=6相比，max_dept越大测试集评估结果更好，训练集评估结果不变；说明max_dept=6的模型效果相对更好。<br>\n",
    "2. max_dept=9与max_dept=6相比，max_dept越大测试集评估效果下降，训练集评估结果不变；<br>\n",
    "3. 当max_dept=15时，测试集评估结果再次下降；验证且说明当max_dept取值大于9时，模型产生过拟合现象。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、取值max_depth=6，n_estimators作为单变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. n_estimators=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9346513511523452, 0.9416955059764868)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ne_5 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=5, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_ne_5.fit(X_train,y_train)\n",
    "model_ne_5.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. n_estimators=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9350931448346955, 0.9416955059764868)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ne_10 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=10, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_ne_10.fit(X_train,y_train)\n",
    "model_ne_10.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. n_estimators=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9351299609748914, 0.9416955059764868)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ne_15 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_ne_15.fit(X_train,y_train)\n",
    "model_ne_15.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. n_estimators=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9348722479935203, 0.9416955059764868)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ne_20 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=20, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_ne_20.fit(X_train,y_train)\n",
    "model_ne_20.score(X_test,y_test),model.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：\n",
    "1. n_estimators=10与n_estimators=5相比，n_estimatorst越大测试集评估结果更好，训练集评估结果不变；说明n_estimators=10的模型效果相对更好。<br>\n",
    "2. n_estimators=15与n_estimators=10相比，n_estimatorst越大测试集评估结果更好，训练集评估结果不变；说明n_estimators=15的模型效果相对更好。<br>\n",
    "3. 然而当n_estimators=20时，测试集评估结果反而下降；验证且说明当n_estimators取值20或者更大时，模型产生过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、取值max_depth=6，n_estimators=15，由于该问题的正负样本比例很不均衡，于是调整scale_pos_weight参数，该参数取值大于1时，正样本比重增加；该参数取值小于1时，负样本比重增加；该问题需要增加正样本比重，并且引入混淆矩阵、AUC评估指标观察效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. scale_pos_weight=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.935130,    train:0.941696\n",
      "auc:0.935130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25165,   172],\n",
       "       [ 1590,   235]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "model_spw_1 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=1)\n",
    "model_spw_1.fit(X_train,y_train)\n",
    "print( 'test:%f,    train:%f' %(model_spw_1.score(X_test,y_test),model.score(X_train,y_train)))\n",
    "y_pred = model_spw_1.predict(X_test)\n",
    "print('auc:%f' % accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. scale_pos_weight=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.930786,   train:0.941696\n",
      "auc:0.930786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24731,   606],\n",
       "       [ 1274,   551]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spw_2 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=2)\n",
    "model_spw_2.fit(X_train,y_train)\n",
    "print( 'test:%f,   train:%f' %(model_spw_2.score(X_test,y_test),model.score(X_train,y_train)))\n",
    "y_pred = model_spw_2.predict(X_test)\n",
    "print('auc:%f' % accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. scale_pos_weight=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.903063,   train:0.941696\n",
      "auc:0.903063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[23590,  1747],\n",
       "       [  886,   939]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spw_5 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=5)\n",
    "model_spw_5.fit(X_train,y_train)\n",
    "print( 'test:%f,   train:%f' %(model_spw_5.score(X_test,y_test),model.score(X_train,y_train)))\n",
    "y_pred = model_spw_5.predict(X_test)\n",
    "print('auc:%f' % accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. scale_pos_weight=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.816803,   train:0.941696\n",
      "auc:0.816803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20913,  4424],\n",
       "       [  552,  1273]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spw_10 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=10)\n",
    "model_spw_10.fit(X_train,y_train)\n",
    "print( 'test:%f,   train:%f' %(model_spw_10.score(X_test,y_test),model.score(X_train,y_train)))\n",
    "y_pred = model_spw_10.predict(X_test)\n",
    "print('auc:%f' % accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. scale_pos_weight=14.7  负样本数量/正样本数量=14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:0.801745,   train:0.941696\n",
      "auc:0.801745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20447,  4890],\n",
       "       [  495,  1330]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spw_147 = xgb.XGBClassifier(max_depth=6, learning_rate=0.01, \\\n",
    "                          n_estimators=15, objective='binary:logistic',scale_pos_weight=14.7)\n",
    "model_spw_147.fit(X_train,y_train)\n",
    "print( 'test:%f,   train:%f' %(model_spw_147.score(X_test,y_test),model.score(X_train,y_train)))\n",
    "y_pred = model_spw_147.predict(X_test)\n",
    "print('auc:%f' % accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：\n",
    "1. 虽然随着scale_pos_weight值的增大，测试集评估结果即auc越来越低，训练集评估结果不变；说明过度增加正样本权重降低了模型预测负样本的能力，产生了过拟合现象；\n",
    "2.  随着scale_pos_weight值的增大，预测为正样本的数量（真正例）明显增多，即召回率（样本中的正例有多少被预测正确了）显著增加，一定程度上降低了fraud的风险；但是假正例数量也明显增多，即将更多不会产生欺诈行为的人预测成了会产生欺诈行为，减少了银行的信用卡客户量；\n",
    "3. 综上所述平衡各个指标，scale_pos_weight取值为[1,2]区间中的某个值可能更加合理；\n",
    "4. 因此针对实际问题，所设定的目标不同、评估准则不同，模型的参数选取也应有所侧重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本掌握机器学习的整个流程，项目实战太少，需要练习项目把所学到的知识串联起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
