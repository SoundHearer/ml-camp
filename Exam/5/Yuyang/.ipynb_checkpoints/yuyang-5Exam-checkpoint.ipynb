{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>于洋</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "- note: 16分。最后一题是为下一阶段让大家有个印象，就像学PIL库一样，学会探索很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：Hadoop的优点是可以对TB级的大文件进行分割处理，分到多个集群的节点上，处理完成再将结果返回到driver端，这样就解决了服务器无法单机完成大文件数据的解析，另外对数据分发的时候会在集群上做备份的操作，不同的节点上会存储同样的一块数据，而且两个节点上不会有多个相同的数据。\n",
    "    Spark相比于Hadoop的优点在于：Spark所有的处理都被组织成弹性分布式数据集（RDD）上的转换和动作，不需要数据分析人员自己手动写数据的分发和收集（maper-reducer）的处理操作，另外RDD可以持久化到内存中得到有效的复用，而且Spark可以省略很多编码，可以直接使用其API，还可以进行串联操作等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：MAP-REDUCE就是将大数据处理分成了分发与收集两个操作，在map阶段我们根据需要将数据解析成（key，value）对的形式，分发到不同的节点上，然后会根据key进行排序，到reduce阶段我们需要将从集群不同节点上的数据进行格式化输出（词频统计时将判断排序后的key是不是相同的决定value是否进行增加计算，最后将结果输出）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：RDD就是弹性分布式数据集（Resilient Distributed Dataset）,即一个包含很多元素、被划分到不同节点上进行并行处理的数据结合。\n",
    "RDD的两类操作：transformation（转换）：map()对所有item进行相同的操作；flatMap()对RDD中的item执行同一个操作后得到一个list，然后以平铺的方式把所有的list里所有结果组成新的list；filter()对满足条件的item进行筛选；distinct()对RDD中的item去重\n",
    "action（动作）：first()计算所有的items并返回第一个item；collect()计算所有结果并返回所有的结果到driver端（慎用，大量数据并提取出来可能导致driver端崩溃）；count()计算RDD中item的个数；reduce()对RDD中的ietms做聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：这个题真的不会，百度了一个K-means聚类算法的原理：\n",
    "Spark mllib k-means算法的声线在初始聚类点的选择上，步骤如下：\n",
    "第一步：从数据集X中随机选择一个点作为第一个初始点\n",
    "第二步：计算数据集中所有点与最新选择的中心店的距离D（x）\n",
    "第三步：选择下一个中心点使得p(x) = D(x)²/ΣxeX D(x)²最大\n",
    "第四步：重复二三步，直到K个初始点选择完成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹\n",
    "\n",
    "\n",
    "* 把本地文件test.txt test2.txt放入该文件夹\n",
    "\n",
    "\n",
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "\n",
    "\n",
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "\n",
    "\n",
    "* 把/sxy-new拷贝到/tmp下\n",
    "\n",
    "\n",
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir sxy-new\n",
    "\n",
    "haddop fs -put test.txt /sxy-new\n",
    "hadoop fs -put text2.txt /sxy-new\n",
    "\n",
    "hadoop fs -get /sxy-new/old.txt /\n",
    "\n",
    "hadoop fs -lsr /sxy-new > a.txt\n",
    "\n",
    "hadoop fs -cp /sxy-new ./tmp\n",
    "\n",
    "hadoop fs -rmr /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "- note: 35分\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put data.csv /sxy-new\n",
    "pyspark\n",
    "from pyspark import SparkContent\n",
    "sc = SparkContent('local','pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "raw_content = sc.textFile('file://' + cwd + '/sxy-new/data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "count_RDD = raw_content.count()\n",
    "print(raw_content.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "\n",
    "top_RDD = raw_content.top(5)\n",
    "print(top_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "\n",
    "sample_RDD = raw_content.takeSample(false,3)\n",
    "print(sample_RDD)\n",
    "#takeSample返回的是一个数组，所以直接打印输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "content_rdd = raw_content.map(lambda x:x.strip('\\''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note: 运行有误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字母的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "\n",
    "import re  \n",
    "flat_RDD = sc.parallelize(text)\n",
    "wordsRDD = flat_RDD.flatMap(lambda sentence:re.split(r'[#,%]',sentence.split(\" \")))\n",
    "wordsRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "\n",
    "resultRDD = (content_rdd.flatMap(lambda sentence:sentence.split(\" \"))\n",
    "             .map(lambda word:word[6].lower)\n",
    "             .map(lambda word:(word,1))\n",
    "             .reduceByKey(lambda x,y:x+y))\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "resultRDD = (content_rdd.sortBy(keyfunc = lambda(count,word):count,ascending = False)\n",
    "             .take(10))\n",
    "print(resultRDD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "\n",
    "topRDD = content_rdd.top(5).filter(lambda x:x.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：1.这周学习到的内容：首先了解了在机器学习中对大数据文件进行分析处理的方式，即使用driver-executor这种集群式的方式，通过hadoop的HDFS的map-reduce完成大数据文件的分发-聚合，通过map阶段不同节点对数据文件进行解析，形成（k，v）对的形式，然后在reduce阶段完成数据的聚合；spark相当于对map-reduce的封装，不同认为再进行map和reduce阶段的处理，而是通过现成的API函数对数据进行处理，spark的driver会将计算任务分成很多小的task，然后送到executor上执行，executor完成处理会回传。另外spark会将处理和计算任务组织成一系列的弹性分布式数据集（RDD）上的转换和动作，RDD的transformation有：map(),flatMap(),filter()等，action有top(),first(),collect()等。\n",
    "2.不足：spark的API还需要多加练习，目前使用不算熟练，不常用的API还需要查看手册；在线下课学到了利用spark进行分类任务和回归任务进行训练的实例，这部分还需要多加学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：每周两个比较重要的知识点对我么来说进度适宜，这样既有时间自己多加练习，也可以让自己充分吸收学到的东西，再通过周末的线下实训进一步巩固和拓展。之前练习题和考试直接能在notebook上直接执行，这周课程需要在服务器上运行，有时候运行不过去，所以希望老师能把这周的练习作业等相关代码整理一下以供参考，要不然自己做完作业无法验证对错。多谢老师。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
