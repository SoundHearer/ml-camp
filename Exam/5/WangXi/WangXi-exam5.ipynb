{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:王玺\n",
    "- 批改人： David\n",
    "- 最终得分:100分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "- note: 20分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "hadoop的优点：  \n",
    "1.低成本：hadoop本身是运行在普通PC服务器组成的集群中进行大数据的分发及处理工作的，这些服务器集群是可以支持数千个节点的。  \n",
    "2.高效性：这也是hadoop的核心竞争优势所在，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。  \n",
    "3.可靠性：通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。  \n",
    "4.扩展性：hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。  \n",
    "Spark的优点：  \n",
    "1.hadoop数据存储在硬盘里，所以对于迭代式数据处理性能比较差，而spark的RDD存在内存中所以处理迭代问题的时候非常有效率。  \n",
    "2.hadoop中间结果也放在HDFS文件系统中，而spark中间结果放在内存中，内存放不下了会写入本地磁盘，而不是HDFS，这也提高了效率。  \n",
    "3.hadoop抽象层次低，需要手工编写代码来完成，使用上难以上手，而spark基于RDD的抽象，实数据处理逻辑的代码非常简短。  \n",
    "4.hadoop只提供两个操作，Map和Reduce，表达力欠缺。spark提供很多转换和动作，很多基本操作如Join，GroupBy已经在RDD转换和动作中实现。  \n",
    "5.hadoop一个Job只有Map和Reduce两个阶段（Phase），复杂的计算需要大量的Job完成，依赖关系是由开发者自己管理的。而spark一个Job可以包含RDD的多个转    换操作，在调度时可以生成多个阶段（Stage），而且如果多个map操作的RDD的分区不变，是可以放在同一个Task中进行。  \n",
    "6.Spark框架为批处理（Spark Core），交互式（Spark SQL），流式（Spark Streaming），机器学习（MLlib）图计算（GraphX）提供一个统一的数据处理平台，     这相对于使用Hadoop有很大优势。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MapReduce的处理过程分为两个步骤：map和reduce。每个阶段的输入输出都是key-value的形式，key和value的类型可以自行指定。map阶段对切分好的数据进行并行处理，处理结果传输给reduce，由reduce函数完成最后的汇总。我们只需要根据业务逻辑实现Map和Reduce两个接口函数内的具体操作内容，即可完成大规模数据的并行批处理任务。  \n",
    "首先,Map 函数将输入数据经过业务逻辑计算产生若干仍旧以Key/Value形式表达的中间数据。  \n",
    "然后,MapReduce计算框架会自动将中间结果中具有相同Key值的记录聚合在一起，并将数据传送给Reduce函数内定义好的处理逻辑作为其输入值。  \n",
    "最后,Reduce 函数接收到Map阶段传过来的某个Key值及其对应的若干Value值等中间数据，函数逻辑对这个Key对应的Value内容进行处理，一般是对其进行累加、过滤、转换等操作，生成Key/Value形式的结果，这就是最终的业务计算结果。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1）RDD是弹性分布式数据集。它是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合。RDD是可以持久化到节点内存中的，不用频繁读写硬盘速度非常快，这样就可以有效地并行进行迭代操作。  \n",
    "2）Transformation用于对RDD的创建，RDD只能使用Transformation创建，同时还提供大量操作方法，包括map，filter，groupBy，join等，RDD利用这些操作生成新的RDD，但是RDD中所有的操作都是Lazy模式进行,无论多少次Transformation，在RDD中真正数据计算Action之前都不可能真正运行。  \n",
    "3）Action是数据执行部分，其通过执行count，reduce，collect等操作真正执行数据的计算部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1）.数据注入：我们收集起来供给机器学习流水线应用程序的数据可以来自于多种数据源，数据规模也是从几百GB到几TB都可以。而且，大数据应用程序还有一个特征，就是注入不同格式的数据。  \n",
    "2）.数据清洗：数据清洗这一步在整个数据分析流水线中是第一步，也是至关重要的一步，也可以叫做数据清理或数据转换，这一步主要是要把输入数据变成结构化的，以方便后续的数据处理和预测性分析。依进入到系统中的数据质量不同，总处理时间的60%-70%会被花在数据清洗上，把数据转成合适的格式，这样才能把机器学习模型应用到数据上。 数据总会有各种各样的质量问题，比如数据不完整，或者数据项不正确或不合法等。数据清洗过程通常会使用各种不同的方法，包括定制转换器等，用流水线中的定制的转换器去执行数据清洗动作。数据清洗也通常是个迭代的过程。象Trifacta、OpenRefine或ActiveClean等数据转换工具都可以用来完成数据清洗任务。  \n",
    "3）.特征抽取：在特征抽取（有时候也叫特征工程）这一步，我们会用特征哈希（Hashing Term Frequency）和Word2Vec等技术来从原始数据中抽取具体的功能。这一步的输出结果常常也包括一个汇编模块，会一起传入下一个步骤进行处理。  \n",
    "4）.模型训练：机器学习模型训练包括提供一个算法，并提供一些训练数据让模型可以学习。学习算法会从训练数据中发现模式，并生成输出模型。  \n",
    "5）.模型验证：这一步包评估和调整机器学习模型，以衡量用它来做预测的有效性。对于二进制分类模型评估指标可以用接收者操作特征（Receiver Operating Characteristic，ROC）曲线。ROC曲线可以表现一个二进制分类器系统的性能。创建它的方法是在不同的阈值设置下描绘真阳性率（True Positive Rate，TPR）和假阳性率（False Positive Rate，FPR）之间的对应关系。  \n",
    "6）.模型选择：模型选择指让转换器和估计器用数据去选择参数。这在机器学习流水线处理过程中也是关键的一步。ParamGridBuilder和CrossValidator等类都提供了API来选择机器学习模型。  \n",
    "7）.模型部署：一旦选好了正确的模型，我们就可以开始部署，输入新数据并得到预测性的分析结果。我们也可以把机器学习模型部署成网页服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    hadoop fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把本地文件test.txt test2.txt放入该文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    hadoop fs -put test.txt test2.txt /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    hadoop fs -get /sxy-new/old.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    hadoop fs -getmerge /sxy-new temp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把/sxy-new拷贝到/tmp下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    hadoop fs -mkdir /tmp \n",
    "    hadoop fs -cp /sxy-new /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    hadoop fs -rm -r /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上\n",
    "\n",
    "- note: 45分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scp d:/data.csv julyedusxy@101.201.113.229:/home/julyedusxy\n",
    "pyspark\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "raw_content = sc.textFile(\"file://\" + cwd + \"/data.csv\")\n",
    "raw_content = raw_content.map(lambda x:x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "raw_content.takeSample(False,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "def wash(x): return([xx.replace('\"','')for xx in x])\n",
    "content_rdd = raw_content.map(wash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "rdd = sc.parallelize(text)\n",
    "result = (rdd.flatMap(lambda x:x.split(\" \"))\n",
    "          .flatMap(lambda x:x.split(\",\"))\n",
    "          .flatMap(lambda x:x.split(\"#\"))\n",
    "          .flatMap(lambda x:x.split(\"%\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "package_count = content_rdd.map(lambda x: (x[6], 1)).reduceByKey(lambda a,b: a+b)\n",
    "package_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "package_count.map(lambda x: (x[1], x[0])).sortByKey(0).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "content_rdd.filter(lambda x: x[6]== 'Rcpp'or x[6]== 'ggplot2'or x[6] == 'stringi'or x[6]== 'stringr'or x[ 6]== 'plyr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
