{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>张少洋</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "- note: 20分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Hadoop的核心是HDFS和Map-Reduce，Hadoop的优点如下：\n",
    "1. 高容错性、高可靠性。在多节点上冗余地存储数据，即使节点故障，也能保证数据的持续性和一直可取性。数据校验功能、后台的连续自检数据一致性功能，都为高容错提供了可能。\n",
    "2. 高吞吐量。通过Map-Reduce计算模型，将计算移动向数据端，从而在有限的资源下提高数据的访问处理量级，从而打破带宽瓶颈。\n",
    "3. 高效率。Hadoop以并行的方式工作，通过并行处理加快处理速度。文件分块存储，读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高很多。\n",
    "4. 容量可扩充。HDFS的block信息存放到namenode上，文件的block分布到datanode上，当扩充的时候，仅仅添加datanode数量，系统可以在不停止服务的情况下做扩充，不需要人工干预。\n",
    "\n",
    "#### Spark与Hadoop相比，有如下有点：\n",
    "1. 更灵活的操作模型。Spark提供很多种的数据集操作类型，例如Transformations（转换）的map、filter、flatMap、samplehe等和 actions（动作）的count、collect、reduce等。而Hadoop提供了Map和Reduce两种操作。\n",
    "2. 更快的速度。在Spark中，使用内存（内存不够使用本地磁盘）替代使用HDFS存储中间结果，对于迭代运算效率更高。spark的60%内存用来缓存RDD，对于缓存后的rdd进行操作，节省IO，效率高。而MapReduce作业产生的数据是中间数据也需存写HDFS，且Hadoop无法缓存数据集。\n",
    "3. 更先进的框架。spark是对MapReduce计算模型的改进，核心思想是将和Reduce两个操作进一步拆分为多个元操作，Map这些元操作可以灵活组合，产生新的操作，并经过一些控制程序组装后形成一个大的DAG作业。但是套用MapReduce模型解决问题，不得不将问题 分解为若干个有依赖关系的子问题，每个子问题对应一个MapReduce作业，最终所有这些作业形成一 个DAG。\n",
    "4. 易用性高。Spark可用来完成各种各样的运算，包括SQL 查询、文本处理、机器学习等，而在 Spark 出现之前，一般需要学习各种各样的引擎来分别处理这些需求。提供更丰富的API，支持支持JAVA,Scala,Python和R四种语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MapReduce是一套从海量源数据提取分析元素最后返回结果集的编程模型。总体框架大致分为以下三个环节：\n",
    "1. Map阶段：主要完成key-value对生成，逐个文件逐行扫描，扫描的同时取出符合分析主题的内容（key,value）；\n",
    "2. Gruop by key阶段：该阶段由系统自动完成，且该阶段是进行分桶操作之后的阶段，对key-value对进行自动排序；\n",
    "3. Reduce阶段：聚合、总结、过滤或转换每个节点上的结果，并最终得到想要的结果。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RDD（Resilient Distributed Dataset）是弹性分布式数据集，RDD是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合，spark的cache缓存RDD机制可以将RDD持久化到内存中，这样就可以有效地在并行操作中复用（在机器学习这种需要反复迭代的任务中非常有效）。在节点发生错误时RDD也可以自动恢复。\n",
    "\n",
    "在Spark里，所有的处理和计算任务都会被组织成一系列RDD上的transformations(转换) 和 actions(动作)。Spark的一个核心概念是惰性计算，transformations操作不会立即执行，直到执行Action操作时，才会执行。\n",
    "\n",
    "**transformation常用的操作：**\n",
    "* **`map()` 对RDD的每一个item都执行同一个操作（一进一出）**\n",
    "* **`flatMap()` 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list（一进多出）**  \n",
    "* **`filter()` 筛选出来满足条件的item**\n",
    "* **`distinct()` 对RDD中的item去重**\n",
    "* **`sample()` 从RDD中的item中采样一部分出来，有放回或者无放回**\n",
    "* **`sortBy()` 对RDD中的item进行排序**\n",
    "\n",
    "**action常用的操作：**\n",
    "* ** collect(): 计算所有的excutor上的items并返回所有的结果到driver端，接着 `collect()`会以Python list的形式返回结果**\n",
    "* ** first(): 和上面是类似的，不过只返回第1个item**\n",
    "* ** take(n): 类似，但是返回n个item**\n",
    "* ** count(): 计算RDD中item的个数**\n",
    "* ** top(n): 返回头n个items，按照自然结果排序**\n",
    "* ** reduce(): 对RDD中的items做聚合**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用该库完成机器学习任务一般包括以下几个步骤：\n",
    "1. 加载数据\n",
    "2. 特征工程处理，提取数据特征，有监督学习需要标记数据的label\n",
    "3. 将数据集切分为训练集和测试集；\n",
    "4. 选择模型，并设置模型的参数；\n",
    "5. 将特征工程处理并标记后的数据和模型放到pipeline中\n",
    "6. 拟合模型\n",
    "7. 利用训练好的模型在测试集上做出预测\n",
    "8. 根据预测经过对模型进行评估，不同的模型有不同的评估方法\n",
    "9. 调整模型参数，重复6、7、8的步骤，直到得到理想的评估结果，就完成了机器学习任何并得到最终的机器学习模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "- note :35分\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹\n",
    "\n",
    "\n",
    "* 把本地文件test.txt test2.txt放入该文件夹\n",
    "\n",
    "\n",
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "\n",
    "\n",
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "\n",
    "\n",
    "* 把/sxy-new拷贝到/tmp下\n",
    "\n",
    "\n",
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#在hdfs根目录下新建/sxy-new文件夹\n",
    "hadoop fs -mkdir /sxy-new\n",
    "\n",
    "#把本地文件test.txt test2.txt放入该文件夹\n",
    "hadoop fs -put exerciese_2.txt exercise_1.txt  /sxy-new\n",
    "\n",
    "#从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "hadoop fs -get  /sxy-new/old.txt\n",
    "\n",
    "#从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "hadoop fs -getmerge /sxy-new  result_sxy.txt\n",
    "\n",
    "#把/sxy-new拷贝到/tmp下\n",
    "hadoop fs -cp -f /sxy-new /tmp\n",
    "\n",
    "#删除/sxy-new文件夹\n",
    "hadoop fs -rm -r /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上\n",
    "\n",
    "- note: 45分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd #'/home/julyedusxy/exam_zsy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "raw_content = sc.textFile(\"file://\" + cwd + \"/data.csv\")   #'/home/julyedusxy/exam_zsy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count() #421970条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5) #一条记录是表头，样本数据一共4条\n",
    "#raw_content.top(6) top()取出的不是文件里的前5条 take()是"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "raw_content.sample(False,0.05,3).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample: http://spark.apache.org/docs/2.1.0/api/python/pyspark.html\n",
    "\n",
    "sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "withReplacement:True-有放回抽样；False-无放回抽样\n",
    "\n",
    "raw_content.sample(False,0.05,3).count() -每次出现的数是固定的\n",
    "\n",
    "raw_content.sample(False,0.05).count() -每次出现的数不是固定的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "content_rdd = raw_content.map(lambda x : x.replace('\"',''))\n",
    "#content_rdd.top(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "tmp_rdd = sc.parallelize(text)\n",
    "def resplit(x):\n",
    "    import re\n",
    "    a = re.split('\\s|\\#|,|\\%',x)  \n",
    "    return a\n",
    "tmp_rdd.flatMap(resplit).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "package_reduce = content_rdd.map(lambda x: (x.split(',')[6],1)).reduceByKey(lambda x, y: x +y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "#sortBy\n",
    "package_top = package_reduce.sortBy(keyfunc = lambda x: x[1], ascending = False).take(10)\n",
    "#结果：\n",
    "#[(u'Rcpp', 4783), (u'ggplot2', 3913), (u'stringi', 3748), (u'stringr', 3449), (u'plyr', 3436), (u'magrittr', 3265), (u'digest', 3223), (u'reshape2', 3205), (u'RCol orBrewer', 3046), (u'scales', 3007)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sortByKey\n",
    "package_top = package_reduce.map(lambda x:(x[1],x[0])).sortByKey(False).take(10)\n",
    "#结果：\n",
    "#[(4783, u'Rcpp'), (3913, u'ggplot2'), (3748, u'stringi'), (3449, u'stringr'), (3436, u'plyr'), (3265, u'magrittr'), (3223, u'digest'), (3205, u'reshape2'), (3046, u'RColorBrewer'), (3007, u'scales')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "package_top5 = package_reduce.map(lambda x:(x[1],x[0])).sortByKey(False).map(lambda x:x[1]).take(5)\n",
    "content_rdd.filter(lambda x: x.split(',')[6] in package_top5).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 收获\n",
    "- 学习了Hadoop的核心原理，理解了Map-Reduce这一编程模型的特点；\n",
    "- 熟悉了spark的原理以及常用的transformation与action操作；\n",
    "- 学习了spark机器学习的基本流程\n",
    "\n",
    "### 不足\n",
    "- spark的常用操作还需多针对具体的应用多练习；\n",
    "- spark机器学习的模型原理还需要加强学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 反馈\n",
    "线下的课程，希望可以录屏，方便复习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
