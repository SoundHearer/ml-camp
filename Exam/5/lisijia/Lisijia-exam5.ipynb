{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>__李思佳___</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "- note: 20分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hadoop优点：\n",
    "1.高可靠性:Hadoop按位存储和处理数据的能力值得人们信赖;\n",
    "2.高扩展性:Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。\n",
    "3.高效性:Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。\n",
    "4.高容错性:Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。\n",
    "\n",
    "而与Hadoop相比，Spark优点：\n",
    "1.Spark是建立在统一抽象的RDD上，与Hadoop的大数据处理场景不同，提供了更加广泛的数据集操作，不像Hadoop仅提供Map和Reduce两种操作。\n",
    "2.Hadoop的MapReduce过程是低层次抽象，Spark是基于RDD的高层次抽象，数据处理的逻辑代码更加简洁易懂。\n",
    "3.Hadoop的一个Job只有Map和Reduce，复杂计算需要大量Job拼接；而Spark可以将一个Job分解成多个Stage和Task进行处理。\n",
    "4.与Hadoop相比，Spark是基于内存的运算，计算速度更快，所利用的计算资源更少。\n",
    "5.Hadoop需要多次进行IO操作，而Spark提供Cache机制来支持反复迭代的计算或数据共享，减少数据操作的IO开销；Hadoop计算中间结果放在HDFS文件系统中，而Spark是放在内存或本地磁盘中。\n",
    "6.Hadoop时延高，只适用于Batch数据处理，对于交互式、实时数据处理支持不够，而Spark通过将刘拆分成小的Batch提供Stream处理流数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MapReduce的基本原理：将大的数据分析分成小块逐个分析，最后再将提取出来的数据汇总分析。\n",
    "Map任务处理为读取HDFS中的文件，将每一行解析成<K,V>；然后按照K对数据进行排序（Sort），最后通过Reduce实现任务逻辑的汇总处理，产生新的<K,V>写入到HDFS中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RDD是弹性分布式数据集的简称，是只读的、一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合。\n",
    "最重要的有两类操作是transformation和action：\n",
    "transformation的主要操作：1. map 2. flatMap 3. filter 4. sample 5. union 6.groupByKey\n",
    "action:1. collect 2. reduce 3. count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.导入pyspark.ml中要使用到的工具库。\n",
    "2.使用Spark读取数据存储成RDD格式。\n",
    "3.对数据进行预处理和特征工程处理。\n",
    "4.判断该机器学习任务属于分类、回归等的有监督学习，还是聚类等的无监督学习。\n",
    "5.选取相应的机器学习模型对训练集数据进行学习和参数调优。\n",
    "6.对测试集数据进行预测和评估，对该机器学习模型进行评价。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "- note :35分\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把本地文件test.txt test2.txt放入该文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put test.txt test2.txt /sxy-new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -get /sxy-new/old.txt <本地目录，如根目录则填空>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-new <本地文件名，如merge.txt>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把/sxy-new拷贝到/tmp下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -cp /sxy-new  /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -rmr /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "-note : 45分\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.csv MapPartitionsRDD[18] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "raw_content=sc.textFile('data.csv')\n",
    "raw_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421970"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"date\",\"time\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"',\n",
       " '\"2015-12-12\",\"13:42:10\",257886,\"3.2.2\",\"i386\",\"mingw32\",\"HistData\",\"0.7-6\",\"CZ\",1',\n",
       " '\"2015-12-12\",\"13:24:37\",1236751,\"3.2.2\",\"x86_64\",\"mingw32\",\"RJSONIO\",\"1.3-0\",\"DE\",2',\n",
       " '\"2015-12-12\",\"13:42:35\",2077876,\"3.2.2\",\"i386\",\"mingw32\",\"UsingR\",\"2.0-5\",\"CZ\",1',\n",
       " '\"2015-12-12\",\"13:42:01\",266724,\"3.2.2\",\"i386\",\"mingw32\",\"gridExtra\",\"2.0.0\",\"CZ\",1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"2015-12-12\",\"00:49:35\",969050,NA,NA,NA,\"LearnBayes\",\"2.15\",\"CA\",182',\n",
       " '\"2015-12-12\",\"00:49:54\",1900184,NA,NA,NA,\"mlmRev\",\"1.0-6\",\"DE\",129',\n",
       " '\"2015-12-12\",\"00:53:39\",57423,\"3.2.2\",\"x86_64\",\"mingw32\",\"testthat\",\"0.11.0\",\"HK\",265']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "raw_content.sample(False,0.001,3).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date,time,size,r_version,r_arch,r_os,package,version,country,ip_id',\n",
       " '2015-12-12,13:42:10,257886,3.2.2,i386,mingw32,HistData,0.7-6,CZ,1',\n",
       " '2015-12-12,13:24:37,1236751,3.2.2,x86_64,mingw32,RJSONIO,1.3-0,DE,2']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "def drop_quotes(row):\n",
    "    new_row = ''\n",
    "    for i in row:\n",
    "        if i is not '\\\"':\n",
    "            new_row += i\n",
    "    return new_row\n",
    "\n",
    "content_rdd = raw_content.map(drop_quotes)\n",
    "content_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'm',\n",
       " 'n',\n",
       " 'q',\n",
       " 'r',\n",
       " 'q',\n",
       " 'w',\n",
       " 'j',\n",
       " 'r',\n",
       " 'q']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "rdd_text = sc.parallelize(text)\n",
    "rdd_text_fm = rdd_text.flatMap(lambda sentence:sentence.split(','))\n",
    "rdd_text_fm1 = rdd_text_fm.flatMap(lambda sentence:sentence.split())\n",
    "rdd_text_fm2 = rdd_text_fm1.flatMap(lambda sentence:sentence.split('%'))\n",
    "rdd_text_fm3 = rdd_text_fm2.flatMap(lambda sentence:sentence.split('#'))\n",
    "rdd_text_fm3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "import re\n",
    "from pyspark import Row\n",
    "\n",
    "PATTERN = '(\\d{4}-\\d{2}-\\d{2}),(\\d{2}:\\d{2}:\\d{2}),(\\d+),(\\S+),(\\S+),(\\S+),(\\S+),(\\S+),(\\S+),(\\S+)'\n",
    "\n",
    "def change_schemaRDD(row):\n",
    "    match = re.search(PATTERN, row)\n",
    "    if match is None:\n",
    "        return (row,0)\n",
    "    else:\n",
    "        return (Row(\n",
    "            date = match.group(1),\n",
    "            time = match.group(2),\n",
    "            size = match.group(3),\n",
    "            r_version = match.group(4),\n",
    "            r_arh = match.group(5),\n",
    "            r_os = match.group(6),\n",
    "            package = match.group(7),\n",
    "            version = match.group(8),\n",
    "            country = match.group(9),\n",
    "            ip_id = match.group(10)\n",
    "        ),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='CZ', date='2015-12-12', ip_id='1', package='HistData', r_arh='i386', r_os='mingw32', r_version='3.2.2', size='257886', time='13:42:10', version='0.7-6'),\n",
       " Row(country='DE', date='2015-12-12', ip_id='2', package='RJSONIO', r_arh='x86_64', r_os='mingw32', r_version='3.2.2', size='1236751', time='13:24:37', version='1.3-0'),\n",
       " Row(country='CZ', date='2015-12-12', ip_id='1', package='UsingR', r_arh='i386', r_os='mingw32', r_version='3.2.2', size='2077876', time='13:42:35', version='2.0-5')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaRDD = content_rdd.map(change_schemaRDD).filter(lambda s: s[1] == 1).map(lambda s: s[0]).cache()\n",
    "schemaRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8659"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_package = schemaRDD.map(lambda row: row.package)\n",
    "package_rdd = content_package.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).cache()\n",
    "package_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('HistData', 159),\n",
       " ('UsingR', 151),\n",
       " ('lme4', 1560),\n",
       " ('testthat', 1178),\n",
       " ('maps', 1586)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "package_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rcpp', 4783), ('ggplot2', 3913), ('stringi', 3748), ('stringr', 3449), ('plyr', 3436), ('magrittr', 3265), ('digest', 3223), ('reshape2', 3205), ('RColorBrewer', 3046), ('scales', 3007)]\n"
     ]
    }
   ],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "print(package_rdd.takeOrdered(10, lambda s: -1 * s[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plyr', 3436),\n",
       " ('ggplot2', 3913),\n",
       " ('stringr', 3449),\n",
       " ('Rcpp', 4783),\n",
       " ('stringi', 3748)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "package_rdd.filter(lambda s:s[1]>3430).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "通过本周课程学校，第一次接触Hadoop和Spark就有了一定基础的认识和了解，掌握一些大数据的相关知识和对大数据进行处理的能力。\n",
    "不足之处：还有很多概念不是很清楚，很多操作不熟练，需要多补充些相关知识和多练习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "无，讲得通俗易懂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
