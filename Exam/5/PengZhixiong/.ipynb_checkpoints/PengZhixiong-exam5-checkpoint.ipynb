{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>彭志雄</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？\n",
    "\n",
    "- note: 20分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- #### Hadoop的优点：  \n",
    "  - 支持超大文件   \n",
    "     一般来说，HDFS存储的文件可以支持TB和PB级别的数据。   \n",
    "  - 检测和快速应对硬件故障   \n",
    "    在集群环境中，硬件故障是常见性问题。因为有上千台服务器连在一起，故障率高，因此故障检测和自动恢复hdfs文件系统的一个设计目标。假设某一个datanode节点挂掉之后，因为数据备份，还可以从其他节点里找到。namenode通过心跳机制来检测datanode是否还存在   \n",
    "  - 流式数据访问   \n",
    "    HDFS的数据处理规模比较大，应用一次需要大量的数据，同时这些应用一般都是批量处理，而不是用户交互式处理，应用程序能以流的形式访问数据库。主要的是数据的吞吐量，而不是访问速度。访问速度最终是要受制于网络和磁盘的速度，机器节点再多，也不能突破物理的局限，HDFS不适合于低延迟的数据访问，HDFS的是高吞吐量。    \n",
    "  - 简化的一致性模型   \n",
    "    对于外部使用用户，不需要了解hadoop底层细节，比如文件的切块，文件的存储，节点的管理。一个文件存储在HDFS上后，适合一次写入，多次写出的场景once-write-read-many。因为存储在HDFS上的文件都是超大文件，当上传完这个文件到hadoop集群后，会进行文件切块，分发，复制等操作。如果文件被修改，会导致重新出发这个过程，而这个过程耗时是最长的。所以在hadoop里，不允许对上传到HDFS上文件做修改（随机写），在2.0版本时可以在后面追加数据。但不建议。   \n",
    "  - 高容错性  \n",
    "    数据自动保存多个副本，副本丢失后自动恢复。可构建在廉价机上，实现线性（横向）扩展，当集群增加新节点之后，namenode也可以感知，将数据分发和备份到相应的节点上。   \n",
    "  - 商用硬件   \n",
    "    Hadoop并不需要运行在昂贵且高可靠的硬件上，它是设计运行在商用硬件的集群上的，因此至少对于庞大的集群来说，节点故障的几率还是非常高的。HDFS遇到上述故障时，被设计成能够继续运行且不让用户察觉到明显的中断。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Spark与Hadoop相比又有哪些优点\n",
    "  - 首先，Spark对分散的数据集进行抽样，创新地提出RDD(ResilientDistributedDataset)的概念，所有的统计分析任务被翻译成对RDD的基本操作组成的有向无环图(DAG)。RDD可以被驻留在RAM中，往后的任务可以直接读取RAM中的数据；同时分析DAG中任务之间的依赖性可以把相邻的任务合并，从而减少了大量不准确的结果输出，极大减少了硬盘I/O，使复杂数据分析任务更高效。从这个推算，如果任务够复杂，Spark比Hadoop快一到两倍。\n",
    "\n",
    "  - 其次，Spark是一个灵活的运算框架，适合做批次处理、工作流、交互式分析、流量处理等不同类型的应用，因此Spark也可以成为一个用途广泛的运算引擎，并在未来取代Hadoop的地位。\n",
    "\n",
    "  - 最后，Spark可以与Hadoop生态系统的很多组件互相操作。Spark可以运行在新一代资源管理框架YARN上，它还可以读取已有并存放在Hadoop上的数据，这是个非常大的优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- MapReduce处理问题的思想：\n",
    "  - Input:  \n",
    "  先将海量文件上传到HDFS中，HDFS是将这些海量数据分成若干数据块，每个块放在不同的数据节点里；\n",
    "  - Map:  \n",
    "  做一个map函数，该函数可以被jobtracker进程分配到各个节点中，具体是对分在该节点的数据进行map（map函数可根据具体的实际情况编写），指定节点的map只对本节点的数据进行处理，不需要其他节点的map来协作；因此，海量数据就此被若干数据节点分担，成为若干个小的工作。大任务=>小工作；\n",
    "  - Shuffle:  \n",
    "  实际是一个预处理的过程（可以没有），可以把map的输出按照某种key值进行排序、分组、拷贝，把key值符合某种范围的输出送到特定的reduce那里去处理，简化reduce过程；\n",
    "  - Reduce：  \n",
    "  将map阶段处理后的结果进行汇总计算出最终的结果，具体如下：\n",
    "  对全部的结果进行合并，用merge实现，合并到一个大表里面；  \n",
    "  reduce函数提取需要的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 简述RDD的含义\n",
    "  - RDD（弹性数据集）是Spark提供的最重要的抽象的概念，它是一种有容错机制的特殊集合，可以分布在集群的节点上，以函数式编操作集合的方式，进行各种并行操作。可以将RDD理解为一个具有容错机制的特殊集合，它提供了一种只读、只能有已存在的RDD变换而来的共享内存，然后将所有数据都加载到内存中，方便进行多次重用。\n",
    "  - RDD是分布式的，可以分布在多台机器上，进行计算。\n",
    "  - RDD是弹性的，计算过程中内存不够时它会和磁盘进行数据交换。\n",
    "  - 这些特性可以极大的降低自动容错开销，其实质是一种更为通用的迭代并行计算框架，用户可以显示控制计算的中间结果，然后将其自由运用于之后的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transformation:\n",
    "  - map() ：对RDD的每一个item都执行同一个操作\n",
    "  - flatMap() ：对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list\n",
    "  - filter()： 筛选出来满足条件的item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action\n",
    "  - collect()：计算所有的items并返回所有的结果到driver端，接着 collect()会以Python list的形式返回结果\n",
    "  - first()：和上面是类似的，不过只返回第1个item\n",
    "  - take(n)：类似，但是返回n个item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 准备训练和测试数据集\n",
    "- 预处理（包括特征提取与缩放、特征选择、降维、采样）\n",
    "- 学习（包括模型选择、交叉验证、评估指标、超参数优化）\n",
    "- 评估\n",
    "- 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "- note:35分\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹\n",
    "\n",
    "\n",
    "* 把本地文件test.txt test2.txt放入该文件夹\n",
    "\n",
    "\n",
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "\n",
    "\n",
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "\n",
    "\n",
    "* 把/sxy-new拷贝到/tmp下\n",
    "\n",
    "\n",
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "答：\n",
    "#在hdfs根目录下新建/sxy-new文件夹\n",
    "hadoop fs -mkdir /sxy-new\n",
    "\n",
    "#把本地文件test.txt test2.txt放入该文件夹\n",
    "hadoop fs -put test.txt test2.txt /sxy-new\n",
    "\n",
    "#从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "hadoop fs -get /sxy-new/old.txt\n",
    "\n",
    "#从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "hadoop fs -getmerge /sxy-new local.txt\n",
    "\n",
    "#把/sxy-new拷贝到/tmp下\n",
    "hadoop fs -cp /sxy-new /tmp/\n",
    "\n",
    "#删除/sxy-new文件夹\n",
    "hadoop fs -rmr /sxy-new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "- note: 45分\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 环境准备\n",
    "  - （1）下载预构建好的Spark压缩包\n",
    "  - （2）解压压缩包\n",
    "  - （3）添加环境变量\n",
    "  - （4）测试配置情况：执行pyspark命令，出现大大的Spark version 2.1.1即表示环境配置成功\n",
    "  - （5）将data.csv文件传到服务器的/home/julyedusxy/hadoop_mapreduce/pengzx/目录下\n",
    "- 启动：执行 pyspark 命令"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "raw_content = sc.textFile(\"file://\" + \"/home/julyedusxy/hadoop_mapreduce/pengzx/data.csv\").map(lambda sentence: sentence.split(','))\n",
    "#为了得到与下面题目要求的数据格式一致，将将每条item按逗号分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```421970```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    [\n",
    "    [u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "    [u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "    [u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2'], \n",
    "    [u'\"2015-12-12\"', u'\"13:42:35\"', u'2077876', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"UsingR\"', u'\"2.0-5\"', u'\"CZ\"', u'1'], \n",
    "    [u'\"2015-12-12\"', u'\"13:42:01\"', u'266724', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"gridExtra\"', u'\"2.0.0\"', u'\"CZ\"', u'1']\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "raw_content.takeSample(True,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    [\n",
    "    [u'\"2015-12-12\"', u'\"11:51:29\"', u'605286', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"scales\"', u'\"0.3.0\"', u'\"HK\"', u'1261'], \n",
    "    [u'\"2015-12-12\"', u'\"17:11:09\"', u'75592', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"bc3net\"', u'\"1.0.2\"', u'\"CN\"', u'41'], \n",
    "    [u'\"2015-12-12\"', u'\"09:47:09\"', u'92004', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"cuttlefish.model\"', u'\"1.0\"', u'\"KR\"', u'511']\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "import re\n",
    "content_rdd = raw_content.map(lambda item:map((lambda x:re.sub('\"', \"\", x)),item))\n",
    "content_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    [\n",
    "    [u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "    [u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "    [u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "    [u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1'], \n",
    "    [u'2015-12-12', u'13:42:01', u'266724', u'3.2.2', u'i386', u'mingw32', u'gridExtra', u'2.0.0', u'CZ', u'1']\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "sentencesRDD = sc.parallelize(text)\n",
    "wordsRDD = sentencesRDD.flatMap(lambda sentence: re.split(\"[ ,#%]\",sentence))\n",
    "wordsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'm', 'n', 'q', 'r', 'q', 'w', 'j', 'r', 'q']```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "resultRDD=content_rdd.map(lambda x:x[6]).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "resultRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[(u'SIS', 24), (u'StatMethRank', 15), (u'dbmss', 54), (u'searchable', 14), (u'RcmdrPlugin.TextMining', 3)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "sortRDD=resultRDD.sortBy(lambda x: x[1],ascending=False)\n",
    "sortRDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[(u'Rcpp', 4783), (u'ggplot2', 3913), (u'stringi', 3748), (u'stringr', 3449), (u'plyr', 3436), (u'magrittr', 3265), (u'digest', 3223), (u'reshape2', 3205), (u'RColorBrewer', 3046), (u'scales', 3007)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "keys=sortRDD.map(lambda x:x[0]).take(5)\n",
    "top5RDD=content_rdd.filter(lambda x:x[6] in keys)\n",
    "top5RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    [\n",
    "    [u'2015-12-12', u'20:56:29', u'392860', u'3.1.3', u'x86_64', u'mingw32', u'plyr', u'1.8.3', u'US', u'10'], \n",
    "    [u'2015-12-12', u'20:55:19', u'512', u'NA', u'NA', u'NA', u'plyr', u'1.6', u'CN', u'23'], \n",
    "    [u'2015-12-12', u'20:56:29', u'35401', u'3.2.2', u'x86_64', u'mingw32', u'stringr', u'1.0.0', u'US', u'10'], \n",
    "    [u'2015-12-12', u'20:56:28', u'2370487', u'3.2.2', u'i386', u'mingw32', u'Rcpp', u'0.12.2', u'US', u'10'], \n",
    "    [u'2015-12-12', u'20:56:27', u'3643527', u'3.2.1', u'x86_64', u'linux-gnu', u'stringi', u'1.0-1', u'US', u'10']\n",
    "    ]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "        对MapReduce思想有一定的理解，了解到Hadoop和spark在大数据处理上的优势；完成作业用时过长"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "        spark平台搭建不完善，在命令行练习很不方便，要是能搭建一套实际应用的环境就更好了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
