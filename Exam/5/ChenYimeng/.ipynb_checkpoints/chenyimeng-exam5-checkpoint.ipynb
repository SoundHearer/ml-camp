{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>_陈益梦_</u>  \n",
    "- 批改人： \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hadoop优点\n",
    "（一）高可靠性\n",
    "\n",
    "     Hadoop按位存储和处理数据的能力值得人们信赖;\n",
    "     \n",
    "（二）可扩展性\n",
    "\n",
    "     Hadoop是在可用的计算机节点间分配数据并完成计算任务的，集群可以方便地扩展到数以千计的节点中。\n",
    "     \n",
    "（三）高效性\n",
    "\n",
    "     Hadoop并行的方式工作，基于移动计算不移动数据的运算模式，加快数据处理速度。\n",
    "     \n",
    "（四）高容错性\n",
    "\n",
    "     Hadoop能够根据配置自动保存数据的多个副本，并且能够自动将失败的任务重新分配。\n",
    "    \n",
    "Spark优点\n",
    "\n",
    " （1）运算速度快\n",
    "\n",
    "    高效利用内存计算，减少大量磁盘IO，使计算速度提升\n",
    "    Spark 的有向无环图计算模式，提高运算速度\n",
    "    任务调度模式，在Spark中一个应用程序包含多个job任务，在MapReduce中，一个job任务就是一个应用\n",
    "    Spark Streaming 可以支持实时流式运算，MapReduce 更加适合处理离线数据\n",
    " （2）容错性高\n",
    "\n",
    "    Spark 的有向无环图计算模式，提高容错性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MAP-REDUCE编程模型:\n",
    "\n",
    "概况：该模型适合处理大量的离线数据，在不移动数据的情况下移动计算，提高运算效率。\n",
    "\n",
    "计算方式：\n",
    "\n",
    "    Map: 将数据读取到内存，每个 partition 根据mapper 计算后得到 KV对 模式数据，存入磁盘\n",
    "    Combiner: 在map端对输出先做一次合并，以减少传输到reducer的数据量(可不设置)\n",
    "    shuffle: 将mapper 的结果copy 到reduce 需要计算的地方，准备数据\n",
    "    reducer: 将shuffle过程准备好的数据读取到内存进行指定的 reducer 计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RDD: Resilient Distributed Dataset (弹性分布式数据集)\n",
    "RDD的惰性计算注定会有两个类型的算子，转换类型和行动类型\n",
    "### transformation:\n",
    "画蓝图\n",
    "\n",
    "* **`map()` 对RDD的每一个item都执行同一个操作**\n",
    "* **`flatMap()` 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list**\n",
    "* **`filter()` 筛选出来满足条件的item**\n",
    "* **`distinct()` 对RDD中的item去重**\n",
    "* **`sample()` 从RDD中的item中采样一部分出来，根据参数设置 有放回或者无放回**\n",
    "* **`sortBy()` 对RDD中的item进行排序**\n",
    "\n",
    "\n",
    "### action:\n",
    "实现蓝图\n",
    "\n",
    "* **`collect()`  计算所有的items并返回所有的结果到driver端，接着 `collect()`会以Python list的形式返回结果，数据量大时不推荐使用\n",
    "* **`first()`  返回第1个item，类似collect\n",
    "* **`take(n)`  返回n个item，类似collect\n",
    "* **`count()`  计算RDD中item的个数\n",
    "* **`top(n)`  返回头n个items，按照自然结果排序\n",
    "* **`reduce()`  对RDD中的items做聚合\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 概述\n",
    "推荐技术和算法中，最被大家广泛认可和采用的就是基于协同过滤的推荐方法\n",
    "用户对物品或者信息的偏好，根据应用本身的不同，可能包括用户对物品的评分、用户查看物品的记录、用户的购买记录等。其实这些用户的偏好信息可以分为两类：\n",
    "\n",
    "### 大致步骤\n",
    "要实现协同过滤的推荐算法，要进行以下四个步骤：\n",
    "\n",
    "　　了解算法  ->>>   收集数据     ->>>   训练模型    ->>>  进行推荐\n",
    "  \n",
    "\n",
    "#### 一、了解算法\n",
    "MLlib目前支持基于协同过滤的模型，在这个模型里，用户和产品被一组可以用来预测缺失项目的潜在因子来描述。特别是我们实现交替最小二乘（ALS）算法来学习这些潜在的因子，在 MLlib 中的实现有如下参数：\n",
    "\n",
    "l  numBlocks是用于并行化计算的分块个数（设置为-1时 为自动配置）；\n",
    "\n",
    "l  rank是模型中隐性因子的个数；\n",
    "\n",
    "l  iterations是迭代的次数；\n",
    "\n",
    "l  lambda是ALS 的正则化参数；\n",
    "\n",
    "l  implicitPrefs决定了是用显性反馈ALS 的版本还是用隐性反馈数据集的版本；\n",
    "\n",
    "l  alpha是一个针对于隐性反馈 ALS 版本的参数，这个参数决定了偏好行为强度的基准。\n",
    "\n",
    "#### 二、收集数据\n",
    "1.装载样本数据\n",
    "\n",
    "2.清洗数据\n",
    "\n",
    "3.特征工程\n",
    "\n",
    "#### 三、训练模型\n",
    "1.将样本数据切分成3个部分，分别用于训练 (60%，并加入用户评分), 校验 (20%), and 测试 (20%)\n",
    "\n",
    "2.训练不同参数下的模型，并再校验集中验证，获取最佳参数下的模型\n",
    "\n",
    "#### 四、推荐\n",
    "1.用最佳模型预测测试集的评分，计算和实际评分之间的均方根误差\n",
    "\n",
    "2.根据用户评分的数据，推荐前十"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹\n",
    "\n",
    "    hdfs dfs -mkdir /sxy-new\n",
    "    \n",
    "\n",
    "* 把本地文件test.txt test2.txt放入该文件夹\n",
    "\n",
    "    hdfs dfs -put test.txt test2.txt /sxy-new\n",
    "    \n",
    "\n",
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "\n",
    "    hdfs dfs -get /sxy-new/old.txt .\n",
    "    \n",
    "\n",
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "\n",
    "    hdfs dfs -getmerge /sxy-new/* result.txt\n",
    "    \n",
    "\n",
    "* 把/sxy-new拷贝到/tmp下\n",
    "\n",
    "    hdfs dfs -cp -p /sxy-new /tmp\n",
    "    \n",
    "\n",
    "* 删除/sxy-new文件夹\n",
    "\n",
    "    hdfs dfs -rm -r /sxy-new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上传数据 data.csv 到hdfs 上\n",
    "hdfs dfs -put /home/julyedusxy/data.csv /sxydata/output/homework_chenyimeng/exam5\n",
    "#在服务器上执行 \n",
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中  data(MapPartitionsRDD)\n",
    "raw_content = sc.textFile(\"hdfs://emr-header-1.cluster-52678:9000/hadoop_deng/data.csv\")\n",
    "#获取第一条\n",
    "data.first()\n",
    "#获取5条\n",
    "raw_content.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "# 　　第一个参数如果为true,放回抽样，可能会有重复的元素； 如果为false，不放回抽样，不会有重复的元素；\n",
    "# 　　第二个参数取值为[0,1]，最后的数据个数大约等于第二个参数乘总数；\n",
    "# 　　第三个参数为随机因子。随机种子值为3（即可能以1 2 3的其中一个起始值）\n",
    "raw_content.sample(False,0.0001,3).take(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "content_rdd = data.map(lambda x: x.replace('\"', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "rdd = sc.parallelize(text)\n",
    "import re\n",
    "rdd.flatMap(lambda x : re.split('[, #%]', x))   #多分隔符"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "package_count = content_rdd.map(lambda x: (x.split(',')[6],1)).reduceByKey(lambda x,y :  x + y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "package_count_top10 = package_count.map(lambda x: (x[1], x[0])).sortByKey().top(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "top5 = sc.parallelize(package_count.map(lambda x: (x[1], x[0])).sortByKey().top(5)).map(lambda x : x[1]).take(5)\n",
    "data.filter(lambda x : x.split(',')[6].replace('\"','') in top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "课程总结：第五周学习的大数据相关知识，涉及到 Hadoop 和 Spark 最基础的操作\n",
    "Hadoop：hdfs的操作\n",
    "        MapReduce 的编写\n",
    "Spark: 基本的transformation 和 action 操作\n",
    "    \n",
    "对Hadoop 比较生疏，相对而言，Spark 会比较方便，可以联系到 pandas 的 DataFrame 的操作。操作不一样，思想基本一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "针对平台部分，可能会影响到整体的学习过程，在实操的时候有障碍。课件和课程相对比较全。辛苦老师了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
