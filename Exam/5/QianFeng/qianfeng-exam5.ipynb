{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>___钱峰___</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "-note:18分。。除最后一题外，其它都甚好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### (1) hadoop优点:\n",
    "* 低成本\n",
    "     hadoop本身是运行在普通PC服务器组成的集群中进行大数据的分发及处理工作的，这些服务器集群是可以支持数千个节点的。\n",
    "* 高效性\n",
    "    这也是hadoop的核心竞争优势所在，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。\n",
    "* 可靠性\n",
    "    通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。\n",
    "* 扩展性\n",
    "    hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) spark的优点:\n",
    "#### Spark是MapReduce 模式的最好实现。\n",
    "#### 1 程序抽象的角度来看：\n",
    "*  他抽象出MapReduce两个阶段来支持tasks的任意DAG。大多数计算通过依赖将maps和reduces映射到一起(Most computation maps (no pun intended) into many maps and reduces with dependencies among them. )。而在Spark的RDD编程模型中，将这些依赖弄成DAG 。通过这种方法，更自然地表达出计算逻辑。\n",
    "*  通过更好的语言来集成到模型中的数据流，他抛弃了Hadoop MapReduce中要求的大量样板代码。通常情况下，当你看一个的Hadoop MapReduce的程序，你很难抽取出这个程序需要做的事情，因为 the huge amount of boiler plates，而你阅读Spark 程序的时候你会感觉到很自然。\n",
    "*  由于Spark的灵活编程模型，Hadoop MapReduce 中必须和嵌入的操作现在直接在应用程序的环境中。也就是应用程序可以重写shuffle 或者aggregation 函数的实现方式。而这在MapReduce是不可能的！虽然不是绝大部分的应用程序会重写这些方法，但是这种机制可以使得某些人基于特定的场景来重写相关的函数，从而使得计算得到最优。\n",
    "*  最后，应用程序可以将数据集缓存到集群的内存中。这种内置的机制其实是很多应用程序的基础，这些应用程序在短时间内需要多次方法访问这些数据集，比如在机器学习算法中。\n",
    "\n",
    "#### 2 系统的高层次来看：\n",
    "* Spark通过快速的RPCs 方式来调度作业\n",
    "* Spark在线程池中来运行task，而不是一系列的JVM进程。上面两个计算结合起来，使得Spark可以在毫秒级别的时间内调度task。然而在MP调度模型中，需要花费数秒甚至是数分钟（繁忙的集群）来调度task。\n",
    "* Spark不仅支持基于checkpointing(checkpointing-based)的容错(这种方式也是Hadoop MP采用的)，也支持基于血统( lineage-based )的容错机制。错误是很常见的，基于血统( lineage-based )的容错机制可以快速地从失败者恢复！\n",
    "* 部分也是由于学术方面的原因，Spark社区常常有新的思维，其中一个例子就是，在Spark中采用BT协议来广播数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "基本的思想就是讲任务分成很多小任务并行执行, 每个阶段执行的目的不同.\n",
    "\n",
    "map阶段是将数据进行分类，Shuffle阶段排序进行归类，reduce阶段基于已归类数据进行数据处理.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "rdd是弹性分布式数据库的简称，对于用户就是一种数据结构, 数据处理时实际上跑在excutor节点上, 但是对用户透明.\n",
    "* transformation:1. map 2. flatmap 3. filter 4. sortBy 5. distince\n",
    "* action: 1 collect 2 first 3 count 4 top 5 reduce\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 查阅资料后也不甚了解...\n",
    "* 选择数据: 整合数据，将数据规范化成一个数据集，收集起来.\n",
    "* 数据预处理: 数据格式化，数据清理，采样等.\n",
    "* 数据转换: 这个阶段做特征工程.特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后才能得到最终的model。\n",
    "* 数据建模（Model Data）: 建立模型，评估模型并逐步优化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把本地文件test.txt test2.txt放入该文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -put test.txt test2.txt /sxy-new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -get /sxy-new/old.txt ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-new ./result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把/sxy-new拷贝到/tmp下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -cp /sxy-new  /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -rmr /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上\n",
    "\n",
    "- note:45分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mkdir exam_qianfeng\n",
    "cd exam_qianfeng\n",
    "ls -lt  # 当前目录下有data.csv文件\n",
    "# 直接可以使用\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext('local', 'pyspark')\n",
    "import os\n",
    "cwd=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "raw_content = sc.textFile(\"file://\" + cwd + \"/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count() # 421970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note: 好骚的操作，明明可以直接抽样3个，结果还用了列表推导 ：）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "\n",
    "\n",
    "# list of list\n",
    "threeRecord = [raw_content.takeSample(withReplacement=False, num=1) for i in range(3)]  \n",
    "print(threeRecord)\n",
    "\n",
    "# [output]: [[u'\"2015-12-12\",\"16:41:52\",60855,\"3.2.3\",\"x86_64\",\"linux-gnu\",\"SimHaz\",\"0.1\",\"KR\",4989'], \n",
    "#           [u'\"2015-12-12\",\"20:17:09\",18777,NA,NA,NA,\"evaluate\",\"0.8\",\"US\",7946'], \n",
    "#           [u'\"2015-12-12\",\"12:12:36\",2176088,\"3.2.2\",\"x86_64\",\"mingw32\",\"metafor\",\"1.9-8\",\"GB\",668']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "\n",
    "# type of rdd element is str\n",
    "content_rdd = raw_content.map(lambda x: x.replace('\"', ''))\n",
    "\n",
    "# 如果想以下面的rdd展示形式打印的话, 可以执行下面的采样输出 list of list, not a str\n",
    "newThreeRecord = [content_rdd.takeSample(withReplacement=False, num=1) for i in range(3)]  \n",
    "print(newThreeRecord)\n",
    "\n",
    "# [output]: [[u'2015-12-12,02:48:37,36519,3.2.2,x86_64,mingw32,bitops,1.0-6,US,11554'], \n",
    "#          [u'2015-12-12,12:23:54,50872,3.2.3,x86_64,mingw32,Flury,0.1-3,GB,548'], \n",
    "#          [u'2015-12-12,11:35:23,1318953,3.2.3,x86_64,mingw32,rmarkdown,0.8.1,GB,15926']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "rdd = sc.parallelize(text)   \n",
    "\n",
    "(\n",
    "rdd.flatMap(lambda x: x.split(\" \"))\n",
    "    .flatMap(lambda x: x.split(\"#\"))\n",
    "    .flatMap(lambda x: x.split(\",\"))\n",
    "    .flatMap(lambda x: x.split(\"%\"))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "\n",
    "(\n",
    "content_rdd.map(lambda astr: astr.split(',')[6].strip())\n",
    "\t\t\t.filter(lambda x: x != 'package')\n",
    "\t\t\t.map(lambda word: (word, 1))\n",
    "\t\t\t.reduceByKey(lambda x, y: x + y) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "\n",
    "(\n",
    "content_rdd.map(lambda astr: astr.split(',')[6].strip())\n",
    "\t\t\t.filter(lambda x: x != 'package')   # \"package是列名, 因此要过滤\n",
    "\t\t\t.map(lambda word: (word, 1))\n",
    "\t\t\t.reduceByKey(lambda x, y: x + y) \n",
    "\t\t\t.sortBy(keyfunc=lambda (word, count): count, ascending=False)\n",
    "\t\t\t.take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "\n",
    "top5 = (\n",
    "content_rdd.map(lambda astr: astr.split(',')[6].strip())\n",
    "\t\t\t.filter(lambda x: x != 'package')\n",
    "\t\t\t.map(lambda word: (word, 1))\n",
    "\t\t\t.reduceByKey(lambda x, y: x + y) \n",
    "\t\t\t.sortBy(keyfunc=lambda (word, count): count, ascending=False)\n",
    "\t\t\t.take(5)\n",
    ")\n",
    "\n",
    "top5_packages = [x[0] for x in top5]\n",
    "\n",
    "content_rdd.filter(lambda astr: astr.split(',')[6].strip() in top5_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本周主要主要了解了分布式计算的基本原理, hadoop/spark工具的简单实用, 对海量数据处理有了初步的了解.\n",
    "收获和之前的每个阶段都一样, 就是对每一个模块都有的了初步的认识, 有利于下一步的继续学习.\n",
    "需要努力的就是在实际项目中逐步加深细节理解, 最终形成良好的知识体系."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "目前我觉得老师难易程度蛮好的. 初学者上课时会有点疑惑, 课后再看下基本上很快就能上手了. 至于稍微进阶点的, 我个人初学阶段也没有精力去深究, \n",
    "用于学习的时间真的很有限. 课上提到留个印象就好.\n",
    "我对老师的期望一直都是, 多讲讲自己的理解, 学习方法, 简单讲讲自己的一些经历. 寒老师讲的很不错 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
