{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第五周(大数据Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月9日至3月11日期间完成，最晚提交时间本周日（3月11日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam5后，进行作答。例如wangwei-exam5\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/4/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>__廖莹__</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题5分，共计20分)\n",
    "\n",
    "- note: 14分，2，4有些苗条得过分：）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？而Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hadoop优点：\n",
    "1.支持超大文件，可存储的数据达到PB级别\n",
    "2.简化的数据处理模型，用户不需要关心数据文件的存储、切块，以及节点管理\n",
    "3.服务器集群模式可以快速应对硬件故障，当副本数据丢失，可以快速从其他节点回复数据\n",
    "4.可以在节点之间动态移动数据，保持节点动态平衡\n",
    "\n",
    "Spark与之相比的优点：\n",
    "1.spark处理数据比较高效。spark可以把数据存储在内存中处理，而mapreduce则过多依赖磁盘I/O。\n",
    "2.Spark 是一个通用引擎，可用它来完成各种各样的运算，包括 SQL 查询、文本处理、机器学习\n",
    "3.spark易用性相比Hadoop好，用户不需要关心对数据排序，只需要关注数据计算本身。\n",
    "4.spark社区很活跃，方便用户查询学习相关知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "map是映射，reduce是归约，\n",
    "简单理解是，map是把数据分类，reduce是把map分类之后的数据进行相同类别归类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RDD的含义：\n",
    "RDD是一个包含诸多元素、被划分到不同节点上进行并行处理的数据集合，可以将RDD持久化到内存中，这样就可以有效地在并行操作中复用（在机器学习中这种需要反复迭代的任务中非常有效）。在节点发生错误时RDD也可以自动恢复。\n",
    "\n",
    "RDD的transformation操作：\n",
    "1.map()\n",
    "2.flatmap()\n",
    "3.filter()\n",
    "4.distinct()\n",
    "5.sample()\n",
    "6.sortBy()\n",
    "\n",
    "RDD的actions操作：\n",
    "1.reduce()\n",
    "2.collect()\n",
    "3.count()\n",
    "4.take(n)\n",
    "5.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "加载数据，特征提取，模型训练，预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计80分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(35分)\n",
    "\n",
    "- note:35\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "* 在hdfs根目录下新建/sxy-new文件夹\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 把本地文件test.txt test2.txt放入该文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "echo 'this is the test.txt'>> test.txt\n",
    "echo 'this is the test2.txt'>> test2.txt(本地没有就先创建带数据的文件)\n",
    "hadoop fs -put test.txt /sxy-new\n",
    "hadoop fs -put test2.txt /sxy-new\n",
    "hadoop fs -lsr /sxy-new (查看一下目录里面的文件)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -touchz /sxy-new/old.txt(集群上没有就先创建)\n",
    "hadoop fs -get /sxy-new/old.txt\n",
    "ls (查看本地是否有old.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-new local.txt\n",
    "cat local.txt(查看合并后的内容)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 把/sxy-new拷贝到/tmp下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -cp  /sxy-new /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hadoop fs -rm -r  /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上\n",
    "\n",
    "- note:45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "先通过xftp上传数据data.csv到服务器目录/home/julyedusxy/Exam-liaoying\n",
    "执行命令cd /home/julyedusxy/Exam-liaoying ,然后执行命令pyspark，启动spark环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    ">>> import os\n",
    ">>> cwd=os.getcwd() #当前路劲在/home/julyedusxy/Exam-liaoying\n",
    ">>>raw_content = sc.textFile(\"file://\" + cwd + \"/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    ">>> raw_content.count()#输出：421970    \n",
    "'''输出如下：\n",
    "421970  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    ">>> raw_content.take(5)\n",
    "'''输出如下：\n",
    "[u'\"date\",\"time\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"', \n",
    "u'\"2015-12-12\",\"13:42:10\",257886,\"3.2.2\",\"i386\",\"mingw32\",\"HistData\",\"0.7-6\",\"CZ\",1',\n",
    "u'\"2015-12-12\",\"13:24:37\",1236751,\"3.2.2\",\"x86_64\",\"mingw32\",\"RJSONIO\",\"1.3-0\",\"DE\",2',\n",
    "u'\"2015-12-12\",\"13:42:35\",2077876,\"3.2.2\",\"i386\",\"mingw32\",\"UsingR\",\"2.0-5\",\"CZ\",1',\n",
    "u'\"2015-12-12\",\"13:42:01\",266724,\"3.2.2\",\"i386\",\"mingw32\",\"gridExtra\",\"2.0.0\",\"CZ\",1']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    ">>> raw_content.takeSample(False,3,1)\n",
    "'''输出如下：\n",
    "[u'\"2015-12-12\",\"06:43:14\",639360,NA,NA,NA,\"XLConnect\",\"0.2-7\",\"CN\",14775', \n",
    "u'\"2015-12-12\",\"15:52:09\",107921,\"3.2.3\",\"x86_64\",\"darwin13.4.0\",\"ridge\",\"2.1-3\",\"EU\",3722',\n",
    "u'\"2015-12-12\",\"20:41:13\",302617,\"3.2.2\",\"i386\",\"mingw32\",\"VDA\",\"1.3\",\"US\",4438']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    ">>> tmp=raw_content.map(lambda x:x.split(','))\n",
    ">>> content_rdd=tmp.map(lambda x:map(lambda y:y.strip('\"'),x))\n",
    ">>> content_rdd.take(5)#输出前5行看一下结果\n",
    "'''输出如下:\n",
    "[[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'],\n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1'],\n",
    "[u'2015-12-12', u'13:42:01', u'266724', u'3.2.2', u'i386', u'mingw32', u'gridExtra', u'2.0.0', u'CZ', u'1']]\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "\n",
    ">>>text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    ">>> textrdd=sc.parallelize(text)\n",
    ">>> textrdd.flatMap(lambda x:x.split(' ')).flatMap(lambda x:x.split('#')).flatMap(lambda x:x.split(',')).flatMap(lambda x:x.split('%')).collect()\n",
    "'''输出如下:\n",
    "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'm', 'n', 'q', 'r', 'q', 'w', 'j', 'r', 'q']\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    ">>> package=content_rdd.map(lambda x:x[6])\n",
    ">>> prdd=package.map(lambda x:(x,1))#转化成pair-rdd\n",
    ">>> prdd.take(5)#查看pair-rdd\n",
    "'''输出如下:\n",
    "[(u'package', 1), (u'HistData', 1), (u'RJSONIO', 1), (u'UsingR', 1), (u'gridExtra', 1)]\n",
    "'''\n",
    ">>> prdd.reduceByKey(lambda x,y:x+y).take(20)#对具有相同键的值进行合并计数并查看20条结果\n",
    "'''输出如下:\n",
    "[(u'SIS', 24), (u'StatMethRank', 15), (u'dbmss', 54), (u'searchable', 14), (u'RcmdrPlugin.TextMining', 3), (u'bear', 11), (u'SunterSampling', 18), (u'pcadapt', 17), (u'glmdm', 20), (u'ProfileLikelihood', 19), (u'entropy', 55), (u'RArcInfo', 39), (u'schumaker', 12), (u'mosaicData', 58), (u'TruncatedNormal', 13), (u'vmsbase', 16), (u'RegressionFactory', 15), (u'gpmap', 14), (u'blm', 19), (u'xts', 598)]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    ">>> prdd.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).take(10)\n",
    "'''输出如下:\n",
    "[(u'Rcpp', 4783), (u'ggplot2', 3913), (u'stringi', 3748), (u'stringr', 3449), (u'plyr', 3436), (u'magrittr', 3265), (u'digest', 3223), (u'reshape2', 3205), (u'RColorBrewer', 3046), (u'scales', 3007)]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    ">>> top5list=prdd.reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).keys().take(5)#取出数量top5的package \n",
    ">>> top5list  #top5 package 列表\n",
    "\n",
    "'''输出如下:\n",
    "[u'Rcpp', u'ggplot2', u'stringi', u'stringr', u'plyr']\n",
    "'''\n",
    ">>> content_rdd.filter(lambda x:x[6] in map(lambda y:str(y),top5list)).take(5)#过滤出top5 数量的package对应的数据记录，并查看5行数据记录\n",
    "'''输出如下:\n",
    "[[u'2015-12-12', u'20:56:29', u'392860', u'3.1.3', u'x86_64', u'mingw32', u'plyr', u'1.8.3', u'US', u'10'], \n",
    "[u'2015-12-12', u'20:55:19', u'512', u'NA', u'NA', u'NA', u'plyr', u'1.6', u'CN', u'23'], \n",
    "[u'2015-12-12', u'20:56:29', u'35401', u'3.2.2', u'x86_64', u'mingw32', u'stringr', u'1.0.0', u'US', u'10'],\n",
    "[u'2015-12-12', u'20:56:28', u'2370487', u'3.2.2', u'i386', u'mingw32', u'Rcpp', u'0.12.2', u'US', u'10'], \n",
    "[u'2015-12-12', u'20:56:27', u'3643527', u'3.2.1', u'x86_64', u'linux-gnu', u'stringi', u'1.0-1', u'US', u'10']]\n",
    "'''\n",
    ">>> content_rdd.filter(lambda x:x[6] in map(lambda y:str(y),top5list)).count()#过滤出来的记录数目\n",
    "'''输出如下:\n",
    "19329 \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(5分送分项，非必答)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "本周学习的关于大数据的工具第一次接触，尤其是对数据处理这方面从来没接触过，感觉很陌生，尤其hadoop的map-reduce这个编程模型，\n",
    "老师给的练习map-reduce作业，基本不会做。但是spark这块，相比map-reduce操作简单一些，网上也很多总结的资料可以查找，所以spark操作做起来相对容易掌握。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "希望老师在课程中可以多提供一些这些工具比较好的资料网站，方便学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
