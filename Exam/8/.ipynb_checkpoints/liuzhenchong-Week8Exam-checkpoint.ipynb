{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam8\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>刘振冲</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "\n",
    "- NOTE:每题都回答的很认真仔细 ，如果是自己理解的就太棒了。如果是COPY的，后边还要再努力变成自己的理解\n",
    "- 50分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    深度学习通过一层层的网络迭代抽取出特征，然后进行建模分析，得出的训练和测试的结果。是机器学习中一种对数据进行表征学习的方法。深度学习的好处是用无监督或者半监督方式的特征学习或者分层特征提取高效算法来代替手工提取特征。不断增加层数和神经元数量，让系统运行大量数据，进行训练\n",
    "    \n",
    "    \n",
    "    传统机器学习需要人工事先抽取特征，选取模型，进行训练和测试，用算法真正解析数据，不断学习，然后对世界中发生的事做出判断和预测。\n",
    "    \n",
    "    相同之处：\n",
    "        都依赖于海量的数据，体现了对特征处理的重要性\n",
    "    不同之处：\n",
    "        两者的侧重点不一样，机器学习侧重于特征工程预处理，对特征的抽取，关键词：有监督学习与无监督学习，分类、回归，密度估计、聚类，深度学习，Sparse DBN\n",
    "        \n",
    "        深度学习侧重于之后的网络层的搭建，建模\n",
    "        Deep Learning Algorithm 的核心思想：\n",
    "        把learning hierarchy 看做一个network，则\n",
    "        ①无监督学习用于每一层网络的pre-train；\n",
    "        ②每次用无监督学习只训练一层，将其训练结果作为其higher一层的输入；\n",
    "        ③用监督学习去调整所有层 \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1522469327523&di=220aef3b1b81f44d24c4ce91ce1358f1&imgtype=0&src=http%3A%2F%2Fimg.mp.sohu.com%2Fupload%2F20170612%2Fa7b3e225c2f04756b155fbab77e9d756_th.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    Keras既可以用TensorFlow作为backend，也可以用theano作为backend，方便简洁，快速上手，但是TensorFlow对于初学者来说比较难理解；\n",
    "    Keras把大量的内部细节给隐藏了，可扩展性依然很强；但是入门了之后，如果不读源码，或者不用TensorFlow自己动手跑一个模型的话，很难明白里面的细节和原理；\n",
    "    效率依然很不错。 \n",
    "    \n",
    "    任务流程：\n",
    "\n",
    "    1 引入库，初始化“模型架子”\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    2 通过add来添加层\n",
    "    比如添加的都是全连接层\n",
    "    from keras.layers import Dense, Activation\n",
    "    \n",
    "    model.add(Dense(units=64, input_dim=100))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(units=10))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "  \n",
    "    \n",
    "    3 通过compile来编译模型\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    编译模型时必须指明损失函数和优化器，也可以自己定制损失函数。Keras里也封装好了很多优化器和损失函数。\n",
    "    \n",
    "    from keras.optimizers import SGD\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    4 把数据灌进来训练\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "    \n",
    "    也可以手动选择一批一批数据训练\n",
    "    model.train_on_batch(x_batch, y_batch)\n",
    "    \n",
    "    5 在测试集上评估效果\n",
    "    loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "    \n",
    "    6 预测\n",
    "    classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？\n",
    "\n",
    "- note:其它方式那部分，，应该出现在最后一题里的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    SGD（stochastic gradient descent）每一次计算mini-batch的梯度，然后更新参数，是优化深度学习模型最常用的技术，SGD完全依赖于当前batch的梯度。\n",
    "    算法思路是每次选取m个样本进行计算梯度，进行模型参数优化。\n",
    "    算法中ϵϵ是学习率参数，在实际应用中，学习率更新常常采用如下公式ϵk=(1−α)ϵ0+αϵτ，其中α=kτϵk=(1−α)ϵ0+αϵτ，其中α=kτ，即每一轮循环中学习率都衰减，直到第ττ轮，通常ϵτ=1%ϵ0ϵτ=1%ϵ0\n",
    "    通常采用过度误差（Excess error）度量算法收敛速度，J(θ)−minJ(θ)J(θ)−minJ(θ)，即当前步骤离最优解的距离，SGD的收敛速率为O(1k√)\n",
    "    \n",
    "    SGD方法在机器学习模型优化中常常被采用，主要优点有：\n",
    "    （1）采用批量或者全量样本进行优化，能够得到准确的梯度，但不是线性关系。例如每次训练时喂入10000个样本和每次100个样本，虽然有100倍关系，但是准确度只能提升大概10倍左右。\n",
    "    （2）采用minibatch方法可以更方便并行，处理更大的样本集合。\n",
    "    （3）能够起到正则化的左右几种方法比较\n",
    "    \n",
    "    其他方式有：\n",
    "    （1）Momentum 即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力\n",
    "    （2）Nesterov Momentum\n",
    "    是对传统momentum方法的一项改进。\n",
    "    首先，按照原来的更新方向更新一步，然后在该位置计算梯度值，用这个梯度值修正最终的更新方向\n",
    "    （3）Adagrad\n",
    "    Adagrad自适应地为各个参数分配不同学习率的算法。对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢\n",
    "    （4）Adadelta\n",
    "    Adadelta是基于Adagrad算法，为了完美解决Adagrad算法学习率是单调递减、手工设置一个全局初始学习率等问题产生\n",
    "    \n",
    "    \n",
    "    \n",
    "    备注：\n",
    "    minibatch算法是指在优化J(θ)J(θ)的过程中，每次随机选择m个训练样本进行优化，直到达到最优。 \n",
    "    \n",
    "        算法模型：\n",
    "        while True:\n",
    "            data_batch = sample_training_data(data, 256)\n",
    "            weights_grad = grandient(loss_fun, data_batch, weights)\n",
    "            weights += - step_size * weight_grad\n",
    "    \n",
    "    Karpathy做了一个这几个方法在MNIST上性能的比较，其结论是： \n",
    "    1、adagrad相比于sgd和momentum更加稳定，即不需要怎么调参。\n",
    "    2、精调的sgd和momentum系列方法无论是收敛速度还是precision都比adagrad要好一些。\n",
    "    3、在精调参数下，一般Nesterov优于momentum优于sgd。\n",
    "    4、adagrad一方面不用怎么调参，另一方面其性能稳定优于其他方法。\n",
    "    \n",
    "    对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值  \n",
    "    SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠  \n",
    "    如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。  \n",
    "    Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 \n",
    "    在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如应用于下面的模型："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1522469706179&di=f8a5a9b011fa5c17e6f7025989e8a01c&imgtype=0&src=http%3A%2F%2Fn1.itc.cn%2Fimg8%2Fwb%2Frecom%2F2017%2F03%2F14%2F148946261880533764.GIF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    BP算法的基本思想:学习过程由信号的正向传播与误差的反向传播两个过程组成。\n",
    "    1）正向传播：输入样本－>输入层－>各隐层（处理）－>输出层\n",
    "    注1：若输出层实际输出与期望输出（教师信号）不符，则转入2）（误差反向传播过程）。\n",
    "    2）误差反向传播：输出误差（某种形式）－>隐层（逐层）－>输入层 其主要目的是通过将输出误差反传，将误差分摊给各层所有单元，从而获得各层单元的误差信号，进而修正各单元的权值（其过程，是一个权值调整的过程）。\n",
    "    BP神经网络是应用最多的一种神经网络，其精髓在于误差反向传播,实质是求取误差函数最小值问题。但是经过多年的研究显示也存在着很多瓶颈，比如学习速度过慢，学习率难以确定，可能进入局部极小点，以及过拟合问题等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[备注：](https://blog.csdn.net/u013007900/article/details/50118945)\n",
    "\n",
    "    权值调整的过程，也就是网络的学习训练过程（学习也就是这么的由来，权值调整）。\n",
    "    1）初始化\n",
    "    2）输入训练样本对，计算各层输出\n",
    "    3）计算网络输出误差\n",
    "    4）计算各层误差信号\n",
    "    5）调整各层权值\n",
    "    6）检查网络总误差是否达到精度要求\n",
    "    满足，则训练结束；不满足，则返回步骤2。\n",
    "    1）易形成局部极小（属贪婪算法，局部最优)而得不到全局最优；\n",
    "    2）训练次数多使得学习效率低下，收敛速度慢（需做大量运算）；\n",
    "    3）隐节点的选取缺乏理论支持；\n",
    "    4）训练时学习新样本有遗忘旧样本趋势。\n",
    "    如下图显示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://s8.sinaimg.cn/bmiddle/6cfbbd8bt93df774f76d7&690)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    早停法经常用来缓解“BP算法过拟合”。\n",
    "    早停：将数据分为训练集和验证集，训练集用于计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "\n",
    "[如下流程：](https://deeplearning4j.org/img/earlystopping.png)\n",
    "\n",
    "    1）将数据分为定型集和测试集\n",
    "    2）每个epoch结束后（或每N个epoch后）：\n",
    "        用测试集评估网络性能\n",
    "        如果网络性能表现优于此前最好的模型：保存当前这一epoch的网络副本\n",
    "    3）将测试性能最优的模型作为最终网络模型\n",
    "    \n",
    "        早停法可以限制模型最小化代价函数所需的训练迭代次数。通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。\n",
    "    \n",
    "    \n",
    "    备注：\n",
    "    另一种缓解BP过拟合的方法是 正则化：在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    第一种解决过拟合的方法就是调整衡量网络拟合能力的误差准则函数,添加一项含有网络权值和偏置值的均方值。\n",
    "        E=VEe+(1-V)Ew\n",
    "        隐含层神经元的数目可以根据下式来确定   nH=(nI+nO)1/2+l\n",
    "        其中:nH,nI和nO分别为隐含层、输入层和输出层的神经元数目;l一般为1～10之间的一个整数(特别情况可以大于10)\n",
    "    一般用网络均方误差做为衡量网络拟合能力的误差准则函数[1],即  E=Ee=1N∑Ni=1(ei)2=1\n",
    "    其中:N为对应层神经元数目;ti为输出样本值;oi为神经元输出。\n",
    "    \n",
    "    第二种是 提前停止法 \n",
    "    将训练样本分为3个子集,第1个子集做为训练子集,用来计算梯度和网络的权值及偏置值。第2个子集称为确认子集,在训练过程中它用来检验网络误差(称之为确认误差)。在训练的初始阶段,确认误差将减小,但当网络开始出现过拟合时,确认误差将会上升,当确认误差在一定次数的迭代过程中均上升时,网络停止训练,此时可以得到在确认误差最小时网络的权值和偏置值。第3个子集是测试子集,用于比较不同子集划分时网络的适应性。如果测试子集产生的误差达到最小时的迭代次数与确认子集相差很大,则表明子集的划分是不合适的,需要重新进行划分。这种方法称为提前停止法,所谓提前,是指在到达误差目标之前可能会结束训练过程，提前停止法可以应用于任何算法,只需要在训练过程中用确认子集检验网络误差。用该方法对前述网络进行训练,得到如图4所示的结果。从网络的响应可以看出,网络没有出现过拟合现象,但其输出不如调整法平滑,这是提前停止法的缺点。（见上一题）\n",
    "    \n",
    "    第三种是 隐层节点自生成法\n",
    "    设计网络的思想为:先设计一个仅含有一个隐层节点的网络;在训练过程中,根据误差下降的速率和验证误差的变化来确定是否增加一个隐层节点,即当误差下降的速率低于阈值,并且验证误差已经到达最小值时,增加一个隐层节点;当验证误差的最小值在增加节点后变大,或者网络训练达到误差目标时,停止网络训练\n",
    "    \n",
    "    第四种是 dropout\n",
    "    例如训练多个神经网络，然后选取预测结果出现次数最多的结果。例如我们训练得到5个神经网络，对于数字“8”，加入三个网络认为是8，而两个不是，那么我们将其分为“8”。在网络训练更新的过程中，我们随机抑制一些隐层神经元的表达（比如随机抑制50%的神经元），在每一个batch的训练过程中，先根据删去后的网络预测结果，然后用BP算法更新网络参数。训练很多epochs后，相当于得到了非常多的神经网络（可以说无穷多个），所以drop out会非常有效防止过拟合。另一种解释是，网络在丢失掉一些参数后仍然坚持朝着代价函数减小的方向移动，这样得到的模型更加健壮，所以结果更好。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[备注](https://blog.csdn.net/w326639619/article/details/53072140)\n",
    "[原理](https://blog.csdn.net/qq_27248897/article/details/76933986)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    \n",
    "    激活函数（Activation functions）对于人工神经网络模型去学习、理解非常复杂和非线性的函数来说具有十分重要的作用。它们将非线性特性引入到我们的网络中。其主要目的是将A-NN模型中一个节点的输入信号转换成一个输出信号。该输出信号现在被用作堆叠中下一个层的输入。\n",
    "    \n",
    "    应用激活函数f（x），以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。因此，使用非线性激活函数，我们便能够从输入输出之间生成非线性映射。几乎我们可以想到的任何过程都可以表示为神经网络中的函数计算。\n",
    "    \n",
    "    常用的激活函数：\n",
    "        sigmoid\n",
    "        Leaky Relu\n",
    "        Maxout\n",
    "        tanh\n",
    "    如图所示：    \n",
    "    具体函数分析见备注网址\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://7pn4yt.com1.z0.glb.clouddn.com/blog-ac1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[备注：](http://www.sohu.com/a/145367458_468740)\n",
    "\n",
    "        如果我们不运用激活函数的话，则输出信号将仅仅是一个简单的线性函数。线性函数一个一级多项式。现如今，线性方程是很容易解决的，但是它们的复杂性有限，并且从数据中学习复杂函数映射的能力更小。一个没有激活函数的神经网络将只不过是一个线性回归模型（Linear regression Model）罢了，它功率有限，并且大多数情况下执行得并不好。我们希望我们的神经网络不仅仅可以学习和计算线性函数，而且还要比这复杂得多。同样是因为没有激活函数，我们的神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。这就是为什么我们要使用人工神经网络技术，诸如深度学习（Deep learning），来理解一些复杂的事情，一些相互之间具有很多隐藏层的非线性问题，而这也可以帮助我们了解复杂的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    卷积神经网络依旧是层级网络， 但层的功能和形式做了变化\n",
    "    其层级结构包括：\n",
    "    数据输入层/  Input layer\n",
    "        常见的图像数据处理方式：去均值（CNN往往用的处理方式），归一化，PCA降维\n",
    "    卷积计算层 / CONV layer，\n",
    "        局部关联，把每一个神经元看做一个filter，通过对窗口(recepient field)做滑动操作，filter对局部数据计算。\n",
    "        涉及到深度（depth）、填充值（zero padding）、步长（stride）的概念\n",
    "    激励层 / ReLU layer\n",
    "        把卷积层输出结果做非线性映射，激励函数见上题\n",
    "    池化层 / Pooling layer\n",
    "        夹在连续的卷积层之间，作用是压缩数据和参数的量，减少过拟合\n",
    "    全连接层 / FC layer\n",
    "        两层所有神经元之间都有权重连接，通常在CNN的尾部。主要目的是最大可能的利用现在经过窗口滑动和池化后保留下的少量的信息还原原来的输入信息。\n",
    "        \n",
    "        常见的结构：\n",
    "        1）INPUT  \n",
    "        2) [[CONV -> RELU]*N -> POOL?]*M    \n",
    "        3) [FC -> RELU]*K 或FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    备注：\n",
    "   [ CNN训练注意事项](https://blog.csdn.net/zdy0_2004/article/details/70237049)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    图例：一个具有7（输入层+c1+s2+c3+s4+c5+v）层网络结构的字母识别的CNN网络\n",
    "![](http://img.blog.csdn.net/20140508101034093)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？\n",
    "\n",
    "- note: 决策树模型？ 题目限定了是深度学习了吧\n",
    "- note: 而且像非线性变换，正则化，dropout都这些深层过拟合的方法都需要列出来 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    考虑方向：\n",
    "    样本本身有无问题\n",
    "    （1）建模样本抽取错误，包括（但不限于）样本数量太少，抽样方法错误，抽样时没有足够正确考虑业务场景或业务特点，等等导致抽出的样本数据不能有效足够代表业务逻辑或业务场景； \n",
    "    （2）样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系； \n",
    "    \n",
    "    数据量大小有无关系\n",
    "    如果出现过拟合，则训练集量不够大\n",
    "   \n",
    "    \n",
    "    模型的选取是否恰当\n",
    "    （3）建模时的“逻辑假设”到了模型应用时已经不能成立了。任何预测模型都是在假设的基础上才可以搭建和应用的，常用的假设包括：假设历史数据可以推测未来，假设业务环节没有发生显著变化，假设建模数据与后来的应用数据是相似的，等等。如果上述假设违反了业务场景的话，根据这些假设搭建的模型当然是无法有效应用的。 \n",
    "    （4）参数太多、模型复杂度高 \n",
    "    （5）决策树模型。如果我们对于决策树的生长没有合理的限制和修剪的话，决策树的自由生长有可能每片叶子里只包含单纯的事件数据(event)或非事件数据（no event），可以想象，这种决策树当然可以完美匹配（拟合）训练数据，但是一旦应用到新的业务真实数据时，效果是一塌糊涂。 \n",
    "    （6）神经网络模型。 \n",
    "    a.由于对样本数据,可能存在隐单元的表示不唯一,即产生的分类的决策面不唯一.随着学习的进行, BP算法使权值可能收敛过于复杂的决策面,并至极致. \n",
    "    b.权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解\n",
    "\n",
    "- note: 这个没有写思考 ，直接就写几种不同的学习率设置了。 应该是第3题的后部分写在这儿的。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    AdaDelta基本思想是用一阶的方法，近似模拟二阶牛顿法。\n",
    "    局部最小值\n",
    "    从多个数据集情况来看，AdaDelta在训练初期和中期，具有非常不错的加速效果。但是到训练后期，进入局部最小值雷区之后，AdaDelta就会反复在局部最小值附近抖动。主要体现在验证集错误率上，脱离不了局部最小值吸引盆。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[备注](https://www.cnblogs.com/neopenx/p/4768388.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "- note: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 训练数据 60000张手写图片，28*28*1\n",
    "# 测试数据 10000张手写图片，28*28*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# 归一化到0-1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# to_categorical(y, nb_classes=None)\n",
    "# 将类别向量(从0到nb_classes的整数向量)映射为二值类别矩阵, 用于应用到以categorical_crossentropy为目标函数的模型中.\n",
    "# y: 类别向量; nb_classes:总共类别数\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Dense层:即全连接层\n",
    "# keras.layers.core.Dense(output_dim, init='glorot_uniform', activation='linear', weights=None,\n",
    "#      W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 20s 330us/step - loss: 0.2430 - acc: 0.9251 - val_loss: 0.1129 - val_acc: 0.9649\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 18s 300us/step - loss: 0.1011 - acc: 0.9690 - val_loss: 0.0846 - val_acc: 0.9745\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 18s 301us/step - loss: 0.0754 - acc: 0.9780 - val_loss: 0.0856 - val_acc: 0.9762\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 19s 309us/step - loss: 0.0606 - acc: 0.9818 - val_loss: 0.0904 - val_acc: 0.9754\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 19s 319us/step - loss: 0.0497 - acc: 0.9854 - val_loss: 0.0892 - val_acc: 0.9769\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 18s 298us/step - loss: 0.0448 - acc: 0.9864 - val_loss: 0.0829 - val_acc: 0.9796\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 19s 310us/step - loss: 0.0377 - acc: 0.9884 - val_loss: 0.0841 - val_acc: 0.9799\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 17s 290us/step - loss: 0.0349 - acc: 0.9897 - val_loss: 0.0826 - val_acc: 0.9824\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 17s 286us/step - loss: 0.0317 - acc: 0.9910 - val_loss: 0.0913 - val_acc: 0.9811\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0290 - acc: 0.9919 - val_loss: 0.0917 - val_acc: 0.9822\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0258 - acc: 0.9927 - val_loss: 0.0892 - val_acc: 0.9832\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.0260 - acc: 0.9929 - val_loss: 0.0938 - val_acc: 0.9819\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 15s 254us/step - loss: 0.0237 - acc: 0.9933 - val_loss: 0.1077 - val_acc: 0.9816\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0249 - acc: 0.9935 - val_loss: 0.0996 - val_acc: 0.9828\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s 253us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.1086 - val_acc: 0.9818\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 17s 279us/step - loss: 0.0210 - acc: 0.9942 - val_loss: 0.1047 - val_acc: 0.9826\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 18s 295us/step - loss: 0.0210 - acc: 0.9945 - val_loss: 0.1036 - val_acc: 0.9834\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 18s 294us/step - loss: 0.0198 - acc: 0.9949 - val_loss: 0.1162 - val_acc: 0.9831\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 15s 258us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.1172 - val_acc: 0.9824\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.0178 - acc: 0.9958 - val_loss: 0.1235 - val_acc: 0.9828\n",
      "-------evaluate--------\n",
      "10000/10000 [==============================] - 1s 108us/step\n",
      "Test score: 0.123480460241\n",
      "Test accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# 激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递activation参数实现。\n",
    "# 以下两行等价于：model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Dropout  需要断开的连接的比例\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 打印出模型概况\n",
    "print('model.summary:')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "# 在训练模型之前，通过compile来对学习过程进行配置\n",
    "# 编译模型以供训练\n",
    "# 包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']\n",
    "# 如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics={'ouput_a': 'accuracy'}\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "# Keras以Numpy数组作为输入数据和标签的数据类型\n",
    "# fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)\n",
    "# nb_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为\"number of\"的意思\n",
    "# verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "# shuffle：布尔值，表示是否在训练过程中每个epoch前随机打乱输入样本的顺序。\n",
    "\n",
    "# fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "\n",
    "# evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n",
    "# 按batch计算在某些输入数据上模型的误差\n",
    "print('-------evaluate--------')\n",
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    有些知识点是自己查到的，与题目不符合，但是为了便于以后的复习，作为“备注”而存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
