{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam8\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>___李建其__</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "\n",
    "- note: 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：传统机器学习会有这样的过程：特征提取、特征选择、特征处理、模型选择、模型调参、模型评估等，对于特征需要进行单独的提取、选择、处理，根据任务来选择使用哪种算法模型来训练，训练时调整哪些参数会提高效果。深度学习在训练的过程中，大部分是在做类似于特征抽取的事情，属于嵌入型的特征选择，还有就是调整参数方面没有机器学习那么直观、方便。传统机器学习在不是特别大的数据集上就可以完成比较好的学习结果，而深度学习需要大量的数据集、进行多轮迭代才会有一个比较明显的效果。目前来看，除了在图像处理、计算机视觉领域，深度学习远远优胜于传统机器学习外，其他领域来看对传统机器学习没有特别大的优势，反而有些场景传统机器学习比深度学习有更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：Keras框架是一个站在巨人肩膀上的深度学习框架，其后端可以由不同的深度学习框架来支撑，包括Tensorflow、cntk、theano等。Keras中有Sequential（序贯）模型和Functional（函数式）模型。序贯模型可以理解为函数式模型的一种特殊情况。    \n",
    "针对Sequential模型的基本流程是初始化Sequential模型实例，add层（Dense、Activitaion、Conv2D、MaxPooling、Dropout等），通过compile来编译模型、使用数据进行模型训练、在测试集上评估效果、调参、再次在测试集上评估效果等。    \n",
    "针对Function模型，它的使用场景是搭建多输入、多输出的模型，每一个层都可以指定输入和输出，首先建立一个或多个输入Input，之后建立层（Dense、Conv2D、MaxPooling、Dropout），为每一层指定输入和输出，使用Model建立一个模型，通过compile编译模型，使用数据进行模型训练，在测试记上评估效果等。函数式模型一个重要的特点是可以重用，即把一个训练好的模型可以直接拿过来当做一层使用。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：训练深度学习模型时，计算全量（all samples）的梯度来进行一次参数的更新，在比较小的数据量级时，还可以接受，但是如果数据量比较大，那么每次参数的迭代都要遍历全量数据，来做一个梯度的计算，进而更新参数。第一是硬件成本比较高，有时无法满足计算需求；第二是即使硬件满足需求，消耗的时间成本也是巨大的。使用mini-batch来计算梯度，更新参数对硬件的要求没有那么高，并且计算速度比较快，时间成本和硬件成本都会降低。如果使用全量的数据进行梯度的计算，loss函数很容易掉入局部最小点，但是局部最小点又不是全局最小点，并不一定是最好的结果。使用mini-batch，梯度就会有一定的波动性，有比较大的几率跳出局部最小点，进而找到全局最小点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：信号正向传播，输入信号从输入层传入，经各隐层逐层处理后，传向输出层。如果输出层的实际输出与期望的输出有差别，则转入误差反向传播阶段。    \n",
    "误差反向传播，将输出以某种形式通过隐层向输入层逐层反传，根据每个隐层的误差，对该层的参数进行调整，进而达到是误差变小的效果，最终得到一个比较好的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：使用Deep Learning进行训练的过程，大部分可以理解为是在做特征抽取，迭代轮数越多，那么特征越多，描述的越全面，模型训练的越好。但是如果迭代轮次过多会将训练集一些特有的属性学习进来，会造成在训练集的效果越来越好，而超过某个轮次之后，测试集或者验证集的效果反而越来越差。这个时候可以使用早停法，即在验证集或者测试集效果开始变差的轮次，停止迭代，保证模型在测试集或验证集上比较好的效果。早停法可以防止过拟合的情况出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：使用early stoping，及时的停止训练、引入更多的数据进行训练，来保证样本空间的多样性和普遍性、使用dropout增加神经网络的健壮性、引入正则化项进行约束。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：激活函数是为了引入非线性变换，增加神经网络在传递过程中的复杂性，使得隐层神经元可以逼近任意函数。如果不使用激活函数，那么输入层和输出层是线性的关系，表示拟合能力和学习能力有限。常用的激活函数有sigmiod、tanh、ReLU、ReLU的变体（LReLU、PReLU）等。而sigmiod激活函数由于在离0稍微远一点的地方，梯度接近于0，使得参数无法更新，所以会采用ReLU函数，而ReLU的变体是为了解决ReLU在小于0的时候梯度恒为0的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：基本架构有卷积层（Conv2D）、池化层（MaxPooling）、全连接层（Dense）以及输出层（比如softmax），当然中间会加入Dropout层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：第一点从数据的角度来考虑，是否是训练样本过少，不足以表征数据的分布，所以在训练集上的效果很好，而更换数据集进行测试后，效果比较差，此时可以考虑增加数据集的大小，通过平移、翻转等手段。    \n",
    "第二点可以考虑神经网络的训练太精确、专一，导致测试集和训练集有一点点的差异就会引起比较大的loss，此时可以考虑引入Dropout来增加参数训练的多样性，进而增加神经网络的鲁棒性，来防止过拟合。    \n",
    "第三点就是调参，batch size的选取过大导致，通过调小每次反向传播的batch size，引入样本扰动，增加训练的多样性，进而增加模型的鲁棒性，来防止过拟合。    \n",
    "第四点就是使用early stoping，考虑迭代轮次过高，而导致训练样本独有的特性或者噪声学习进去，而导致过拟合，在验证集或测试集效果开始降低时，提前终止训练。    \n",
    "第五点就是模型选择，是否是模型建立有问题，可以考虑使用一些已经训练好的模型作为基础，站在巨人的肩膀上，说不定就有一个比较好的效果。    \n",
    "其他就是考虑损失函数的选取是否正确、激活函数的选取是否正确、还有可以通过调节Optimizer参数来防止过拟合    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：学习率决定了每次参数迭代的步长，过大导致震荡、不收敛；过小导致收敛速度慢、并且有肯能收敛于局部最优解。学习率的调整路线是：前期大学习率搜索，后期小学习率调优。SGD（随机梯度下降）一般基于mini-batch（即一次计算比较小的样本梯度），在前期保证收敛速度，后期小学习率时会大概率跳出局部最优解，最终收敛到近似全局最优解的情况。方法有Adagrad、Rmsprop、Adadelta等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "\n",
    "- note: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 58s - loss: 0.4685 - acc: 0.8529    \n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 67s - loss: 0.1774 - acc: 0.9468    \n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 104s - loss: 0.1256 - acc: 0.9619   \n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 119s - loss: 0.1007 - acc: 0.9696   \n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 80s - loss: 0.0888 - acc: 0.9733    \n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 56s - loss: 0.0760 - acc: 0.9766    \n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 56s - loss: 0.0687 - acc: 0.9792    \n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 56s - loss: 0.0622 - acc: 0.9808    \n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 59s - loss: 0.0564 - acc: 0.9827    \n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.0513 - acc: 0.9849    \n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0479 - acc: 0.9851    \n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.0436 - acc: 0.9864    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d37c2a908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 0.030707651961327066\n",
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把optimizer换成Adagrad，训练集准确率由0.9864提高到0.9944，测试集准确率由0.99提高到0.9913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = Adagrad(lr=0.01, epsilon=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=ada,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.1594 - acc: 0.9685    \n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 65s - loss: 0.0479 - acc: 0.9856    \n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 65s - loss: 0.0390 - acc: 0.9873    \n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.0337 - acc: 0.9895    \n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 65s - loss: 0.0303 - acc: 0.9903    \n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.0265 - acc: 0.9914    \n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0243 - acc: 0.9919    \n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 59s - loss: 0.0234 - acc: 0.9924    \n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 56s - loss: 0.0214 - acc: 0.9930    \n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 59s - loss: 0.0199 - acc: 0.9938    \n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 65s - loss: 0.0185 - acc: 0.9939    \n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 64s - loss: 0.0169 - acc: 0.9944    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d0874ffd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 0.029436930847338227\n",
      "Test accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变batch_size=256,训练集准确率提高到了0.9966，测试集提高到了0.9923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 61s - loss: 0.0149 - acc: 0.9953    \n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0144 - acc: 0.9951    \n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0140 - acc: 0.9951    \n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0145 - acc: 0.9951    \n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0136 - acc: 0.9956    \n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0134 - acc: 0.9958    \n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0118 - acc: 0.9963    \n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 61s - loss: 0.0118 - acc: 0.9961    \n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0120 - acc: 0.9961    \n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 62s - loss: 0.0128 - acc: 0.9957    \n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0112 - acc: 0.9964    \n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.0110 - acc: 0.9966    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d25921b38>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 0.028984473573532888\n",
      "Test accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变迭代轮次为16,训练集准确率提高到了0.9974，测试集下降到了0.9922，怀疑有过拟合的风险"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "60000/60000 [==============================] - 93s - loss: 0.0107 - acc: 0.9967    \n",
      "Epoch 2/16\n",
      "60000/60000 [==============================] - 91s - loss: 0.0103 - acc: 0.9966    \n",
      "Epoch 3/16\n",
      "60000/60000 [==============================] - 88s - loss: 0.0109 - acc: 0.9968    \n",
      "Epoch 4/16\n",
      "60000/60000 [==============================] - 92s - loss: 0.0100 - acc: 0.9970    \n",
      "Epoch 5/16\n",
      "60000/60000 [==============================] - 93s - loss: 0.0099 - acc: 0.9968    \n",
      "Epoch 6/16\n",
      "60000/60000 [==============================] - 93s - loss: 0.0097 - acc: 0.9967    \n",
      "Epoch 7/16\n",
      "60000/60000 [==============================] - 85s - loss: 0.0097 - acc: 0.9969    \n",
      "Epoch 8/16\n",
      "60000/60000 [==============================] - 80s - loss: 0.0102 - acc: 0.9965    \n",
      "Epoch 9/16\n",
      "60000/60000 [==============================] - 53s - loss: 0.0096 - acc: 0.9969    \n",
      "Epoch 10/16\n",
      "60000/60000 [==============================] - 74s - loss: 0.0100 - acc: 0.9967    \n",
      "Epoch 11/16\n",
      "60000/60000 [==============================] - 86s - loss: 0.0083 - acc: 0.9975    \n",
      "Epoch 12/16\n",
      "60000/60000 [==============================] - 90s - loss: 0.0089 - acc: 0.9970    \n",
      "Epoch 13/16\n",
      "60000/60000 [==============================] - 89s - loss: 0.0082 - acc: 0.9973    \n",
      "Epoch 14/16\n",
      "60000/60000 [==============================] - 87s - loss: 0.0085 - acc: 0.9970    \n",
      "Epoch 15/16\n",
      "60000/60000 [==============================] - 87s - loss: 0.0080 - acc: 0.9974    \n",
      "Epoch 16/16\n",
      "60000/60000 [==============================] - 88s - loss: 0.0081 - acc: 0.9974    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8d2588fc50>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          epochs=16,\n",
    "          batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0s\n",
      "Test loss: 0.03063230253819561\n",
      "Test accuracy: 0.9922\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, batch_size=256)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：本周课程切入深度学习，听起来比较吃力，基本的概念和框架已经理解，在Keras的应用上比较生疏，对图像分类、计算机视觉方向比较模糊。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：在学习过深度学习第八期的前提下，听本周课程会有更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
