{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam8\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>_张学飞_</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "\n",
    "- note:55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答：  深度学习主要特点是使用深度神经网络：深度卷积网络、深度循环网络、递归网络等。深度学习算法擅长分析高维度的数据。比如图像、语音等。以图片为例，一张图片像素可能几十上百万，相当于特征向量维度达到几十上百万，而且像素点与像素点之间的关系又不是特别明显。这种时候用卷积神经网络能很有效的处理这种问题，基本很精确的抓取出图片的特征。但是每个维度的权重可解释性极弱，调参方向很不明朗（神经元数量、隐含层层数等）。\n",
    "\n",
    "传统机器学习一般指的是像决策树、逻辑回归、支持向量机、xgboost等。普通机器学习比较擅长分析维度较低，可解释性很强的任务。比如数据挖掘、推荐算法。他们的特点是一般情况下采集的数据维度都不高，以广告推送任务为例，一般分析的数据维度只会包含性别、年龄、学历、职业等。可解释性很强，调参方向较为明确。\n",
    "\n",
    "深度学习是近几年才发展起来的。传统机器学习算法大都来源于概率论，信息学。对于程序编写的话，传统机器学习模型基本上都集成在sklearn这个包里面，深度学习可以用tensorflow作为框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Keras是基于Theano的一个深度学习框架，它的设计参考了Torch，用Python语言编写，是一个高度模块化的神经网络库。主要包含如下包：\n",
    "\n",
    "1.Models包：keras.models。这是Keras中最主要的一个模块，用于对各个组件进行组装。\n",
    "\n",
    "2.Layers包：keras.layers：该模块主要用于生成神经网络层，包含多种类型，如Core layers、Convolutional layers、recurrent layers、advanced_activations layers、normalization layers、embeddings layers等。其中Core layers里面包含了flatten（CNN的全连接层之前需要把二维特征图flatten成为一维的）、reshape（CNN输入时将一维的向量弄成二维的）、dense（隐藏层）。\n",
    "\n",
    "3.Initializations包：keras.initializations。该模块主要负责对模型参数（权重）进行初始化，初始化方法包括：uniform、lecun_uniform、normal、orthogonal、zero、glorot_normal、he_normal等。\n",
    "\n",
    "4.Activations包：keras.activations、keras.layers.advanced_activations（新激活函数。该模块主要负责为神经层附加激活函数，如linear、sigmoid、hard_sigmoid、tanh、softplus、softmax、relu以及LeakyReLU、PReLU等比较新的激活函数。\n",
    "\n",
    "5.Objectives包：keras.objectives。该模块主要负责为神经网络附加损失函数，即目标函数。如mean_squared_error，mean_absolute_error ，squared_hinge，hinge，binary_crossentropy，categorical_crossentropy等，其中binary_crossentropy，categorical_crossentropy是指logloss。\n",
    "\n",
    "6.Optimizers包：keras.optimizers。该模块主要负责设定神经网络的优化方法，如最基本的随机梯度下降SGD,另外还有Adagrad、Adadelta、RMSprop、Adam，一些新的方法以后也会被不断添加进来。\n",
    "\n",
    "7.Preprocessing包：keras.preprocessing。数据预处理模块，包括序列数据的处理、文本数据的处理和图像数据的处理等。对于图像数据的处理，keras提供了ImageDataGenerator函数,实现数据集扩增，对图像做一些弹性变换，比如水平翻转，垂直翻转，旋转等。\n",
    "\n",
    "8.metrics包：keras.metrics。与sklearn中metrics包基本相同，主要包含一些如binary_accuracy、mae、mse等的评价方法。predict = model.predict_classes(test_x)       #输出预测结果。keras.metrics.binary_accuracy(test_y, predict)  #计算预测精度\n",
    "\n",
    "\n",
    "\n",
    "--kernes使用流程--\n",
    "\n",
    "第一步，我们需要根据模型fit（训练）时需要的数据格式来构造数据的shape，用numpy构造两个矩阵：一个是数据矩阵，一个是标签矩阵.\n",
    "\n",
    "第二步，我们来构造一个神经网络模型。在这一步中可以add多个层，也可以merge合并两个模型。\n",
    "\n",
    "第三步，我们编译上一步构造好的模型，并指定一些模型的参数，比如目标函数、优化器等。\n",
    "\n",
    "第四步，传入要训练的数据和标签，并指定训练的一些参数，然后进行模型训练。\n",
    "\n",
    "第五步，用测试数据测试已经训练好的模型，并可以获得测试结果，从而对模型进行评估。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在GD和SGD中，都会在每次迭代中更新模型的参数，使得代价函数变小。在GD中，每次迭代都要用到全部训练数据。在对代价函数求偏导时，是需要用到全部的训练数据的。在SGD中，每次迭代可以只用一个（mini-batch）训练数据来更新参数。\n",
    "\n",
    "当训练数据过大时，用GD可能造成内存不够用，那么就可以用SGD了，SGD其实可以算作是一种online-learning。另外SGD收敛会比GD快."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BP算法：BP算法的学习过程由信号的正向传播与误差的反向传播两个过程组成。由于多层前馈网络的训练经常采用误差反向传播算法，人们也常把将多层前馈网络直接称为BP网络。\n",
    "\n",
    "BP算法由信号的正向传播和误差的反向传播两个过程组成。\n",
    "正向传播时，输入样本从输入层进入网络，经隐层逐层传递至输出层，如果输出层的实际输出与期望输出不同，则转至误差反向传播；如果输出层的实际输出与期望输出(导师信号)相同，结束学习算法。\n",
    "反向传播时，将输出误差(期望输出与实际输出之差)按原通路反传计算，通过隐层反向，直至输入层，在反传过程中将误差分摊给各层的各个单元，获得各层各单元的误差信号，并将其作为修正各单元权值的根据。这一计算过程使用梯度下降法完成，在不停地调整各层神经元的权值和阈值后，使误差信号减小到最低限度。\n",
    "\n",
    "\n",
    "权值和阈值不断调整的过程，就是网络的学习与训练过程，经过信号正向传播与误差反向传播，权值和阈值的调整反复进行，一直进行到预先设定的学习训练次数，或输出误差减小到允许的程度。\n",
    "\n",
    "\n",
    "BP算法的问题：\n",
    "1）梯度越来越稀疏，从顶层往下，误差校正信号越来越小。\n",
    "2）易陷入局部极小值：BP算法可以使网络权值收敛到一个最终解，但它并不能保证所求为误差超平面的全局最优解，也可能是一个局部极小值。\n",
    "3）训练时间较长：对于某些特殊的问题，运行时间可能需要几个小时甚至更长。\n",
    "4）只能对有标签的数据进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "为了获得性能良好的神经网络，网络定型过程中需要进行许多关于所用设置（超参数）的决策。超参数之一是定型周期（epoch）的数量：亦即应当完整遍历数据集多少次（一次为一个epoch）？如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。\n",
    "\n",
    "早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。\n",
    "\n",
    "早停法可以限制模型最小化代价函数所需的训练迭代次数。早停法通常用于防止训练中过度表达的模型泛化性能差。如果迭代次数太少，算法容易欠拟合（方差较小，偏差较大），而迭代次数太多，算法容易过拟合（方差较大，偏差较小）。早停法通过确定迭代次数解决这个问题，不需要对特定值进行手动设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "提前终止（当验证集上的效果变差的时候）\n",
    "正则化（Regularization）\n",
    "L1正则化\n",
    "L2正则化\n",
    "数据集扩增（Data augmentation）\n",
    "Dropout\n",
    "\n",
    "参数绑定与参数共享\n",
    " bagging 和其他集成方法\n",
    " 辅助分类节点(auxiliary classifiers)\n",
    " Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在神经网络中，每一层输出的都是上一层输入的线性函数，所以无论网络结构怎么搭，输出都是输入的线性组合。为了避免单纯的线性组合，我们在每一层的输出后面都添加一个激活函数（sigmoid、tanh、ReLu等等）。\n",
    "激活函数是用来加入非线性因素的，解决线性模型不能解决的问题。\n",
    "\n",
    "sigmod函数   tanh函数   ReLU函数   ELU函数   PReLU函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "卷积神经网络是一个多层的神经网络，每层都是一个变换（映射），常用卷积convention变换和pooling池化变换，每种变换都是对输入数据的一种处理，是输入特征的另一种特征表达；每层由多个二维平面组成，每个平面为各层处理后的特征图（feature map）\n",
    "\n",
    "\n",
    "层级结构包括：数据输入层/  Input layer，卷积计算层 / CONV layer， ReLU 激励层 / ReLU layer，池化层 / Pooling layer，全连接层 / FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.选择合适的损失函数。\n",
    "2.选择较小的batch ，一次epoch运行多次，可以减少局部特征的干扰。\n",
    "3.不同深度可以得到不同的结果，过深的layers会导致overfitting，可以尝试不同的深度。\n",
    "4.随着epochs减少learnign rates。\n",
    "5.momentum\n",
    "6.早停法。检测training & tesitng 的loss 曲线。\n",
    "7.在maxout中效果佳，dropout rate随着epoch降低。\n",
    "8.weight decay，traning date在input前就可以做些权重比值，对背景干扰赋予低系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解\n",
    "\n",
    "- note:只介绍了几种，自适应学习率，了解呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "AdaDelta基本思想是用一阶的方法，近似模拟二阶牛顿法。\n",
    "在模型的初期的时候，往往设置为较大的学习速率比较好，因为距离极值点比较远，较大的学习速率可以快速靠近极值点；而，后期，由于已经靠近极值点，模型快收敛了，此时，采用较小的学习速率较好，较大的学习速率，容易导致在真实极值点附近来回波动，就是无法抵达极值点。\n",
    "\n",
    "AdaDelta的缺陷：局部最小值\n",
    "\n",
    "从多个数据集情况来看，AdaDelta在训练初期和中期，具有非常不错的加速效果。\n",
    "\n",
    "但是到训练后期，进入局部最小值雷区之后，AdaDelta就会反复在局部最小值附近抖动。\n",
    "\n",
    "主要体现在验证集错误率上，脱离不了局部最小值吸引盆。\n",
    "\n",
    "这时候，切换成动量SGD，如果把学习率降低一个量级，就会发现验证集正确率有2%~5%的提升，\n",
    "\n",
    "这与常规使用动量SGD，是一样的。\n",
    "\n",
    "之后再切换成AdaDelta，发现正确率又退回去了。\n",
    "\n",
    "再切换成动量SGD，发现正确率又回来了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "\n",
    "- note: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 63s - loss: 0.3334 - acc: 0.8987 - val_loss: 0.0766 - val_acc: 0.9778\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 92s - loss: 0.1156 - acc: 0.9658 - val_loss: 0.0532 - val_acc: 0.9831\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 120s - loss: 0.0855 - acc: 0.9746 - val_loss: 0.0418 - val_acc: 0.9860\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 129s - loss: 0.0727 - acc: 0.9788 - val_loss: 0.0385 - val_acc: 0.9880\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 121s - loss: 0.0625 - acc: 0.9815 - val_loss: 0.0339 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 123s - loss: 0.0556 - acc: 0.9838 - val_loss: 0.0347 - val_acc: 0.9883\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 122s - loss: 0.0511 - acc: 0.9845 - val_loss: 0.0326 - val_acc: 0.9888\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 131s - loss: 0.0488 - acc: 0.9855 - val_loss: 0.0299 - val_acc: 0.9901\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 157s - loss: 0.0457 - acc: 0.9866 - val_loss: 0.0295 - val_acc: 0.9892\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 175s - loss: 0.0419 - acc: 0.9877 - val_loss: 0.0286 - val_acc: 0.9909\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 132s - loss: 0.0388 - acc: 0.9883 - val_loss: 0.0306 - val_acc: 0.9907\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 139s - loss: 0.0378 - acc: 0.9888 - val_loss: 0.0277 - val_acc: 0.9910\n",
      "Test loss: 0.027748237101843553\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "# 使用Sequential模型\n",
    "from keras.models import Sequential\n",
    "# 导入Dense，Dropout，Flatten，Conv2D，MaxPooling2D层\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "# 调用后端接口\n",
    "from keras import backend as K\n",
    "\n",
    "# batch大小，每处理128个样本进行一次梯度更新\n",
    "batch_size = 128\n",
    "# 类别数\n",
    "num_classes = 10\n",
    "# 迭代次数\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "# 28x28 图像\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# -x_train.shape:(6000, 28, 28)\n",
    "# -x_test.shape:(1000, 28, 28)\n",
    "\n",
    "# tf或th为后端，采取不同参数顺序\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    # -x_train.shape[0]=6000\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    # -x_train.shape:(60000, 1, 28, 28)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    # x_test.shape:(10000, 1, 28, 28)\n",
    "    # 单通道灰度图像,channel=1\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# 数据转为float32型\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# 归一化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# 标签转换为独热码\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "# 第一层为二维卷积层\n",
    "# 32 为filters卷积核的数目，也为输出的维度\n",
    "# kernel_size 卷积核的大小，3x3\n",
    "# 激活函数选为relu \n",
    "# 第一层必须包含输入数据规模input_shape这一参数，后续层不必包含\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "# 再加一层卷积，64个卷积核\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# 加最大值池化\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# 加Dropout，断开神经元比例为25%\n",
    "model.add(Dropout(0.25))\n",
    "# 加Flatten，数据一维化\n",
    "model.add(Flatten())\n",
    "# 加Dense，输出128维\n",
    "model.add(Dense(128, activation='relu'))\n",
    "# 再一次Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# 最后一层为Softmax，输出为10个分类的概率\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 配置模型，损失函数采用交叉熵，优化采用Adadelta，将识别准确率作为模型评估\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 训练模型，载入数据，verbose=1为输出进度条记录\n",
    "# validation_data为验证集\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# 开始评估模型效果\n",
    "# verbose=0为不输出日志信息\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
