{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam8\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>张少洋</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "- note: 60\n",
    "- note: perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 对深度学习的理解，以及它与传统机器学习的关系\n",
    "深度学习是机器学习的特定分支，它源于人工神经网络的研究，深度学习模型的典型例子是前馈深度网络和多层感知机，深度学习可以从原始数据中提取较高层次、较为抽象的特征，将原始数据表示为嵌套的层次概念体系，可以通过简单的概念构建复杂的概念。例如，计算机难以理解像素值集合的图像，将一组图像映射到对象标识的函数非常复杂，如果直接处理，学习和评估次映射似乎是不可能的，深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层次描述）来解决这一难题。\n",
    "\n",
    "#### 深度学习vs机器学习\n",
    "##### 相同点：\n",
    "- 两者都属于人工智能范畴，且深度学习是机器学习的分支，两者都属于学习算法；\n",
    "- 两者都从经验中学习，从经验数据中提取特征；\n",
    "- 两者都包含有监督学习和无监督学习。\n",
    "\n",
    "##### 不同点：\n",
    "- 相对比机器学习，深度学习可以提取更高层次、更为抽象的特征，例如从图像中提取特征，且深度学习涉及的模型复杂度更高；\n",
    "- 深度学习解决了那些对机器学习来说不可能完成的任务，例如语音识别、人脸识别；\n",
    "- 机器学习有严格的数学、统计学等理论基础，而深度学习在这方面较为缺乏；\n",
    "- 两者解决问题的方式不同，深度学习根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来来定义，层次化的概念可以让计算机构建较简单的概念来学习复杂概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### keras简介\n",
    "Keras一个深度学习框架，其后端可以用不同的深度学习框架支撑，比如theano、Tensorflow和微软的CNTK，keras最大的特点是为快速实验而生。Keras的核心数据结构是“模型”，模型是一种组织网络层的方式，keras包括两种模型，即序贯（Sequential）模型和函数式（Functional）模型，序贯模型是多个网络层的线性堆叠，也就是“一条路走到黑”，函数式模型接口是用户定义多输出模型、非循环有向模型或具有共享层的模型等复杂模型的途径。\n",
    "\n",
    "#### 基本流程\n",
    "- 1.引入库，初始化“模型架子”（Sequential模型或者Functional模型）；\n",
    "- 2.利用序贯模型或函数式模型来构建深度学习网络，序贯模型通过add来添加层，指定输入数据的shape；\n",
    "- 3.通过compile来编译模型，指定优化器optimizer、损失函数loss和指标列表metrics；\n",
    "- 4.把数据灌进来训练（fit），可指定batch_size和epochs；\n",
    "- 5.在测试集上评估效果（evaluate）\n",
    "- 6.实际预测（predict）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "梯度下降算法是使用最为广泛的参数寻优方法，参数的更新有两种方式：\n",
    "- 批量梯度下降算法：遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。\n",
    "- 随机梯度下降算法：每拿到一个样本就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。\n",
    "\n",
    "小批的梯度下降（mini-batch gradient decent）是为了克服以上两种方法的缺点而采用的是一种折中手段，优点如下：\n",
    "- 这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性；\n",
    "- 因为批的样本数与整个数据集相比小了很多，减少了计算量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "通常说“BP网络”时，一般是指用BP算法训练的多层前馈神经网络。多层前馈神经网络的每层神经元与下一层神经元全互连，神经元之间不存在同层链接，也不存在跨层链接，其中输入层神经元接收外界输入，隐藏层和输出层神经元对信号进行加工，最终结果由输出层神经元输出。\n",
    "\n",
    "#### 信号正向传播过程\n",
    "- 1.初始化网络权重和阈值，每个神经元的每个连接都有一个权重，且每个神经元都有一个相关联的偏倚（bias），偏倚充当阈值，用来改变神经元的活性；\n",
    "- 2.训练样本提供给网络的输入层，输入通过输入神经，不发生变化，即，对于输入层神经元来讲，输出值等于输入值；\n",
    "- 3.计算隐藏层或输出层的每个神经元的净输入。隐藏层或输出层的每个神经元的每个连接都有一个权重，对于计算隐藏层或输出层某神经元J，连接该神经元的每个输入（该输入来自于上一层的输出）都乘以其对应的权重，然后求和，即得到该神经元的净输入；\n",
    "- 4.计算隐藏层或输出层的每个神经元的输出。隐藏层或输出层的每个神经元取其净输入与偏倚的差，然后将激活函数作用于它，得到输出。\n",
    "\n",
    "#### 误差反向传播过程\n",
    "BP算法基于梯度下降策略，以目标的负梯度方向对参数进行调整。\n",
    "- 1.计算输出层的神经元的误差（或称目标）；\n",
    "- 2.计算隐藏层神经元的误差（或称目标）；\n",
    "- 3.计算输出层神经元的梯度项，并更新权重和阈值。给定学习率，对于输出层某神经元，利用梯度下降算法，以目标的负梯度方向更新该神经元所连接的权重和关联的阈值；\n",
    "- 4.计算隐藏层神经元的梯度项，给定学习率，利用梯度下降算法该层神经元的权重和阈值；\n",
    "- 5.重复以上步骤直到达到停止条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "由于神经网络有强大的表示功能，经常容易早于过拟合，当训练误差持续降低，但测试误差却上升的情况下，可使用早停法。使用早停法可以缓解神经网络的过拟合。\n",
    "\n",
    "早停法（earyly stoping）将数据分成训练集和验证集，训练集用来计算梯度、更新权重和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 1.早停法（earyly stoping）。早停法将数据分成训练集和验证集，训练集用来计算梯度、更新权重和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "- 2.权重衰减（Weight Decay）。到训练的后期，通过衰减因子使权重的梯度下降地越来越缓。\n",
    "- 3.Dropout。Dropout是正则化的一种处理，以一定的概率关闭神经元的通路，阻止信息的传递。由于每次关闭的神经元不同，从而得到不同的网路模型，最终对这些模型进行融合。\n",
    "- 4.调整网络结构（Network Structure）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 解决的问题\n",
    "激活函数是人工神经网络的一个极其重要的特征，它决定一个神经元是否应该被激活，激活代表神经元接收的信息与给定的信息有关。激活函数可以带来非线性处理，对输入信息进行非线性变换， 然后将变换后的输出信息作为输入信息传给下一层神经元，使其能够学习和执行更复杂的任务。我们希望我们的神经网络能够处理复杂任务，如语言翻译和图像分类等，线性变换永远无法执行这样的任务。\n",
    "\n",
    "#### 常用的激活函数\n",
    "- Sigmoid函数：f(x)=1/(1+e^-x)。这是一个平滑函数，处理二分类问题，且具有连续性和可微性。与线性函数相比，它的最大优点就是非线性。但该函数的两端很缓，易发生学不动的情况，产生梯度弥散。\n",
    "- ReLU(Rectified Linear Unit):f(x)=max(0,x)。ReLU是如今设计神经网络时使用最广泛的激活函数，该函数为非线性映射，且简单，可缓解梯度弥散。\n",
    "- ReLU的改良版：例如𝐿𝑒𝑎𝑘𝑦𝑅𝑒𝐿𝑈和𝑃𝑎𝑟𝑎𝑚𝑒𝑡𝑟𝑖𝑐𝑅𝑒𝐿𝑈。\n",
    "- Softmax函数：处理多分类问题，softmax函数将压缩每个类在0到1之间，并除以输出总和。它实际上可以表示某个类的输入概率，softmax函数最好在分类器的输出层使用。\n",
    "- Maxout\n",
    "- Tanh(双曲正切)\n",
    "- 等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "- 数据输入层（Input layer）：样本数据提供给数据输入层，需要做一些处理，例如去均值（把输入数据各个维度都中心化到0）、归一化（幅度归一化到同样的范围）、PCA/白化（用PCA降维；白化是对数据每个特征轴上的幅度归一化）。\n",
    "- 卷积计算层（CONV layer）：gaiceng：卷积层可以解决DNN中全连接层参数过多带来的问题。卷积层通过以下方法降低参数量：\n",
    "    - 局部关联，将每个神经元看做一个filter，每个神经元只关注一个特性；\n",
    "    - 使用参数共享机制，固定住神经元与窗口之间连接的权重（卷积核），滑动窗口对输入遍历，抽取特征，一组固定的权重和不同窗口内数据做卷积计算。\n",
    "- 激励层（Activation layer）：使用激活函数把卷积层输出结果做非线性映射，激活函数对卷积层输出结果的每个点进行运算。常用的激活函数见上一题。\n",
    "- 池化层（Pooling layer）：池化层夹在连续的卷积层中间，可压缩数据和参数的量，减小过拟合，可通过Max pooling或average pooling进行处理，达到降维和减小过拟合的风险。\n",
    "- 全连接层（FC layer） ：两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。\n",
    "- Batch Normalization层(可能有)\n",
    "\n",
    "#### 典型的CNN结构\n",
    "- INPUT\n",
    "- [[CONV -> RELU]*N -> POOL?]*M\n",
    "- [FC -> RELU]*K\n",
    "- FC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 如果模型的训练效果不好，可先考察以下几个方面是否有可以优化的地方。\n",
    "#### (1)选择合适的损失函数（choosing proper loss ）\n",
    "神经网络的损失函数是非凸的，有多个局部最低点，目标是找到一个可用的最低点。非凸函数是凹凸不平的，但是不同的损失函数凹凸起伏的程度不同，例如下述的平方损失和交叉熵损失，后者起伏更大，且后者更容易找到一个可用的最低点，从而达到优化的目的。\n",
    "- Square Error（平方损失）\n",
    "- Cross Entropy（交叉熵损失）\n",
    "\n",
    "#### (2)选择合适的Mini-batch size\n",
    "采用合适的Mini-batch进行学习，使用Mini-batch的方法进行学习，一方面可以减少计算量，一方面有助于跳出局部最优点。因此要使用Mini-batch。更进一步，batch的选择非常重要，batch取太大会陷入局部最小值，batch取太小会抖动厉害，因此要选择一个合适的batch size。\n",
    "\n",
    "#### (3)选择合适的激活函数（New activation function）\n",
    "使用激活函数把卷积层输出结果做非线性映射，但是要选择合适的激活函数。\n",
    "- Sigmoid函数是一个平滑函数，且具有连续性和可微性，它的最大优点就是非线性。但该函数的两端很缓，会带来猪队友的问题，易发生学不动的情况，产生梯度弥散。\n",
    "- ReLU函数是如今设计神经网络时使用最广泛的激活函数，该函数为非线性映射，且简单，可缓解梯度弥散。\n",
    "\n",
    "#### (4)选择合适的自适应学习率（apdative learning rate）\n",
    "- 学习率过大，会抖动厉害，导致没有优化提升\n",
    "- 学习率太小，下降太慢，训练会很慢\n",
    "\n",
    "#### (5)使用动量（Momentum）\n",
    "在梯度的基础上使用动量，有助于冲出局部最低点。\n",
    "\n",
    "\n",
    "#### 如果以上五部分都选对了，效果还不好，那就是产生过拟合了，可使用第6题使用的方法来防止过拟合，分别是\n",
    "\n",
    "- 1.早停法（earyly stoping）。早停法将数据分成训练集和验证集，训练集用来计算梯度、更新权重和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。\n",
    "- 2.权重衰减（Weight Decay）。到训练的后期，通过衰减因子使权重的梯度下降地越来越缓。\n",
    "- 3.Dropout。Dropout是正则化的一种处理，以一定的概率关闭神经元的通路，阻止信息的传递。由于每次关闭的神经元不同，从而得到不同的网路模型，最终对这些模型进行融合。\n",
    "- 4.调整网络结构（Network Structure）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "学习率决定了参数移动到最优值的速度快慢。如果学习率过大，很可能会越过最优值；如果学习率过小，优化的效率可能过低，长时间算法无法收敛。所以学习率对于算法性能的表现至关重要。\n",
    "\n",
    "学习率是神经网络难以设置的超参数之一，因为它对模型的性能有显著的影响。损失通常高度敏感与参数空间的某些方向，而不敏感与其他。如果我们认为方向敏感在某种程度是轴对齐的，那么每个参数设置不同的学习率，在整个学习过程中自动适应这些学习率是有道理的。以下列举了几种学习率算法和思想。\n",
    "- Delta-bar-delta算法是早期的在训练时适应模型参数各自学习率的启发式方法，该方法认为，如果损失对于某个给定模型的偏导保持相同的符号，那么学习率应该增加。如果对于该参数的偏导变化了符号，那么学习率应该减小，这种方法应用于全批量优化中。\n",
    "- AdaGrad算法，独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降，效果是在参数空间中，更为平缓的倾斜方向会取得更大的进步。AdaGrad算法旨在应用于凸问题时快速收敛。\n",
    "- RMSProp算法修改AdaGrad以非凸设定下效果更好，改变梯度积累为指数加权的移动平均。RMSProp算法使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛。\n",
    "- Adam算法，在Adam算法中，动量直接并入了梯度一阶矩的估计，且Adam包括偏置修正。Adam算法被认为对超参数的选择相当鲁棒。<br>\n",
    "\n",
    "目前最流行并且使用很高的优化算法包括SGD、RMSProp等等，选择哪一种算法主要取决于使用者对算法的熟悉程度（以便调节超参数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "- note: 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 57s - loss: 0.3381 - acc: 0.8975 - val_loss: 0.0827 - val_acc: 0.9736\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 62s - loss: 0.1201 - acc: 0.9652 - val_loss: 0.0520 - val_acc: 0.9838\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 65s - loss: 0.0895 - acc: 0.9735 - val_loss: 0.0434 - val_acc: 0.9857\n",
      "Test loss: 0.04340015971554676\n",
      "Test accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：经过三轮的迭代，最后的准确率是0.9857"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 收获：通过对深度学习的课程学习，发现深度学习非常有意思，产生了很大的兴趣，且感叹于大神在第一次解决图像问题的脑洞，实在佩服。\n",
    "- 欠缺：本周差点跟不上节奏，好多视频之前没有时间看，导致连续好几天都处于恶补的状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "关于深度学习的内容，寒老师讲课深入浅出，点赞。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
