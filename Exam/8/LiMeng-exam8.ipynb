{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam8\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>__李猛___</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "\n",
    "- note:58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。\n",
    "\n",
    "- note:字不少，但有些点没点到。。。你点到了应用层上的不同，主要是图像多媒体语言这类。。还有一些，如数据量要求，非线性表达 这些。可以参考下其它同学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "机器学习是数据驱动，在获得大量数据后，通过数据构建模型从而完成预测，分类等任务。机器学习包含较多方面。  \n",
    "深度学习是一种特殊的机器学习，通过学习将世界使用嵌套的概念层次来表示并实现巨大的功能和灵活性，其中每个概念都定义为与简单概念相关联，而更为抽象的表示则以较不抽象的方式来计算。深度学习和机器学习的区别是，深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。同机器学习方法一样，深度机器学习方法也有监督学习与无监督学习之分．不同的学习框架下建立的学习模型很是不同．例如，卷积神经网络就是一种深度的监督学习下的机器学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Keras是基于Theano的一个深度学习框架，它的设计参考了Torch，用Python语言编写，是一个高度模块化的神经网络库，支持GPU和CPU。  \n",
    "Keras的API相比TensorFlow更加简洁，使用更加方便。但是效率和灵活性有所降低。  \n",
    "基本流程：  \n",
    "1.导入包  \n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()  \n",
    "2.将一些网络层通过.add()堆叠起来，就构成了一个模型：  \n",
    "from keras.layers import Dense, Activation  \n",
    "\n",
    "model.add(Dense(units=64, input_dim=100))  \n",
    "model.add(Activation(\"relu\"))  \n",
    "model.add(Dense(units=10))  \n",
    "model.add(Activation(\"softmax\"))  \n",
    "\n",
    "3.完成模型的搭建后，我们需要使用.compile()方法来编译模型：  \n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])  \n",
    "编译模型时必须指明损失函数和优化器，如果你需要的话，也可以自己定制损失函数。Keras的一个核心理念就是简明易用，同时保证用户对Keras的绝对控制力度，用户可以根据自己的需要定制自己的模型、网络层，甚至修改源代码。  \n",
    "from keras.optimizers import SGD  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))  \n",
    "\n",
    "4.使用一行代码对我们的模型进行评估  \n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)  \n",
    "5.预测  \n",
    "classes = model.predict(x_test, batch_size=128)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "当训练数据太多时，利用整个数据集更新往往时间上不显示。batch的方法可以减少机器的压力，并且可以更快地收敛。\n",
    "当训练集有很多冗余时（类似的样本出现多次），batch方法收敛更快。以一个极端情况为例，若训练集前一半和后一半梯度相同。那么如果前一半作为一个batch，后一半作为另一个batch，那么在一次遍历训练集时，batch的方法向最优解前进两个step，而整体的方法只前进一个step。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "BP神经网络基本思想由两部分组成：输入样本前向传播并输出结果、误差的反向传播更新网络权值。  \n",
    "样本数据前向传播时，输入样本由输入层传入，经隐含层处理后传到输出层，若输出层的实际输出与期望输出不符，则进入误差反向传播更新网络权值阶段。\n",
    "误差的反向传播更新网络权值是将输出误差以某种形式通过隐含层向输入层逐层反转，并将误差分配给各层神经元各个神经单元。\n",
    "这种信号正向传播与误差反向传播的各层权值调整过程循环进行，权值也不断调整，也就是网络的学习过程。此过程一直进行到网络输出的误差减少到可以接受的程度，或进行到预先设定的学习次数为止。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在使用梯度下降法对损失函数求解时，可以使用早停法防止过拟合。当验证集上误差停止下降然后开始上升时停止训练，防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.early stoping  \n",
    "2.正则项约束L1,L2  \n",
    "3.扩大训练集  \n",
    "4.drop out(随机丢弃)  \n",
    "5.Batch Normalization  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "激活函数是用来加入非线性因素的，因为线性模型的表达能力不够。激活函数可以引入非线性因素，解决线性模型所不能解决的问题。  \n",
    "激活函数有tanh,sigmoid,ReLU,softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "卷积神经网络是一个多层的神经网络，每层都是一个变换，常用卷积convention变换和pooling池化变换，每种变换都是对输入数据的一种处理，是输入特征的另一种特征表达；每层由多个二维平面组成，每个平面为各层处理后的特征图.  \n",
    "输入层为训练数据，即原始数据，网络中的每一个特征提取层都紧跟着一个二次提取的计算层，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。具体C层和S层的个数不确定，依据具体案例而定；最后一个S，即完成了对原始数据的特征提取后，把S层的特征数据进行向量化，然后连接到相应分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？\n",
    "\n",
    "- note:侧重点是解决方法了，可以先从方向上进行归纳问题，再找解决办法，这样理解更船长。。比如数据层，网络层，训练时"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1.加入范数惩罚项  \n",
    "范数正则化是一种非常普遍的方法,也是最常用的方法  \n",
    "2.数据增强  \n",
    "有限的数据容易造成过拟合，针对有限的数据，可以人为的创造一些假数据添加到训练集中  \n",
    "3.early stoping  \n",
    "随着模型的能力提升,训练集的误差会先减小再增大,这样可以提前终止算法减缓过拟合现象  \n",
    "4.Dropout  \n",
    "Dropout提供了一种廉价的Bagging集成近似,能够训练和评估指数级数量的神经网络。dropout可以随机的让一部分神经元失活,这样仿佛是bagging的采样过程,因此可以看做是bagging的廉价的实现.  \n",
    "5.Batch Normalization  \n",
    "Batch Normalization就是在神经网络的训练过程中对每层的输入数据加一个标准化处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "深度学习率可以通过自适应方法调节。随机梯度下降里的自适应学习率方法.  \n",
    "学习率决定了参数空间搜索的步长，过大导致解振动、不收敛；过小导致收敛速度慢，更早收敛于局部最优的差解。理想的学习率设计是，前期大学习率搜索，后期小学习率调优。以及对参数的个性化调整，优化频率高的参数小学习率调整，优化频率低的参数大学习率调整。         \n",
    "由于随机梯度下降，一般基于mini-batch（即一次计算一小批样本的梯度），是对所有样本的估计，所以一般梯度下降会带有惯性，以避免mini-batch带来的梯度的反复波动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "- note:40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.6202    \n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.3098    \n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.2492    \n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.2108    \n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.1827    \n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1633    \n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1483    \n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 11s - loss: 0.1362    \n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1261    \n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 10s - loss: 0.1159    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c08432f28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建模\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 转化为一维数据\n",
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_test = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "# uint不能有负数，我们先转为float类型\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#归一化\n",
    "X_train = (X_train) / 255\n",
    "X_test = (X_test) / 255\n",
    "\n",
    "nb_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#搭建神经网络\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(784,), kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Dense(512, kernel_initializer='he_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2)) \n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "#训练\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9216/10000 [==========================>...] - ETA: 0s0.0982269696444273\n"
     ]
    }
   ],
   "source": [
    "#评估及预测\n",
    "loss_and_metrics = model.evaluate(X_test, y_test, batch_size=128)\n",
    "\n",
    "classes = model.predict(X_test, batch_size=128)\n",
    "\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
