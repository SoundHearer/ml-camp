{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习实训营三期第八周(深度学习)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年3月30日至4月1日期间完成，最晚提交时间本周日（4月1日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试方式：请同学<font color=red><b>拷贝</b></font>该试卷至自己姓名的目录后，将文件更名为同学姓名拼音-exam8后，进行作答。例如wangwei-exam7\n",
    "- 提交格式：请同学新建自己姓名全拼的文件夹，将该试卷，数据文件，zip文件等相关考试文件，放置此目录下。将该目录<b>移动</b>至/0.Teacher/Exam/8/目录下\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名:<u>石国瑞</u>  \n",
    "- 批改人： David\n",
    "- 最终得分:85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、简答题(共10题，1-8题每题5分，最后两题每题10分。共计60分)\n",
    "\n",
    "- note：50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.试写您对深度学习的理解，以及它与传统机器学习的关系，相同与不同之处。\n",
    "\n",
    "- note: 有一些可以继续补充，如可解释性的差异，训练集要求海量数据等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "深度学习基于各种神经网络，比如深度神经网络（DNN）、卷积神经网络（CNN）、循环神经网络（RNN）等，模仿了人脑的部分原理，每个神经网络由很多层组成，每层可以有很多神经元，每个神经元可以做各种计算和变换，可以吞吐大量的数据。在机器学习中都是人工抽取数据特征然后训练模型进行预测或者分类，但是对于图像这种数据人工抽取特征效果不是很好，比如卷积神经网络可以捕获大量抽象特征，因而在语音、图像、视频等领域有很大作为。\n",
    "\n",
    "\n",
    "相同：作用类似，都需要用样本数据去训练模型，都需要抽取特征和处理数据，做预测或者分类、聚类等等。都是对实际业务场景进行建模分析，找出规律。\n",
    "\n",
    "不同：神经网络依靠神经元及连接来提取特征，可以吞吐大量的数据，同时需要有更强的硬件支持，比如GPU、TPU等。而机器学习更多基于统计理论及一些算法，去做业务场景的建模分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.简要介绍下您了解的keras框架? 以及进行一个任务的基本流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- keras基于tensorflow这样的后端(可替换为theano，CNTK)做了高度封装，以层作为主要抽象而不是某个运算。根据官方文档，框架模型主要分为Sequential（序贯模型）和Model（函数式模型）两种实现方式，可以用Dense Layer（全连接层）、Convolutional Layer（卷积层）、Pooling Layer、Recurrent等各种抽象好的层（或者自定义的层）去搭建神经网络，然后可以配置各种优化及损失函数等参数去构建模型。\n",
    "- 流程：引入相应的库 --> 导入数据 --> 数据预处理 --> 用序贯或者函数式模型搭建神经网络(配置相应的层和参数) --> 编译神经网络（配置好优化器、损失函数、评估标准等）--> 训练模型 --> 用模型预测或分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.工业界在训练深度学习模型时，采用训练方式多为SGD（mini-batch），请简述这种方式较其它方式的优点？\n",
    "\n",
    "- note:硬件利用率不高？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 1.SGD非常直观，实现简单，就是随机拿一个或几个数据做个梯度下降；\n",
    "- 2.训练速度快,对于很大的数据集,也能够以较快的速度收敛。\n",
    "- 3.串行执行，硬件利用率不高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  请简述神经风格中的BP模型的信号正向传播与误差反向传播的过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- BP算法由信号的正向传播和误差的反向传播两个过程组成。\n",
    "- 正向传播时，输入样本从输入层进入网络，经隐层逐层传递至输出层，如果输出层的实际输出与期望输出(导师信号)不同，则转至误差反向传播；如果输出层的实际输出与期望输出(导师信号)相同，结束学习算法。\n",
    "- 反向传播时，将输出误差(期望输出与实际输出之差)按原通路反传计算，通过隐层反向，直至输入层，在反传过程中将误差分摊给各层的各个单元，获得各层各单元的误差信号，并将其作为修正各单元权值的根据。这一计算过程使用梯度下降法完成，在不停地调整各层神经元的权值和阈值后，使误差信号减小到最低限度。\n",
    "- 权值和阈值不断调整的过程，就是网络的学习与训练过程，经过信号正向传播与误差反向传播，权值和阈值的调整反复进行，一直进行到预先设定的学习训练次数，或输出误差减小到允许的程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  在什么情况下，会使用到早停法earyly stoping? 使用早停法可以防止什么情况发生？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在机器学习中，早停法，是一种正则化的形式，用来避免训练模型时候的过拟合，即在训练模型时使用一种迭代方法，比如梯度下降。这些方法可以用来更新（训练）模型，使得它可以在每次迭代中更好的拟合训练数据。训练模型直到到达一个点，这会改善（模型）在训练集之外的表现。过了这个点（最优点），在改善模型（表现的）过程中，会带来更多的错误。\n",
    "\n",
    "基于验证集的过拟合\n",
    "这种过拟合规则主要用于将原始训练数据集切分成新的训练集和验证集时。当过拟合的时候，验证集上的错误率 被用来代替错误（作为评判标准）。\n",
    "\n",
    "主要作用：防止过拟合\n",
    "\n",
    "https://en.wikipedia.org/wiki/Early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  训练多层神经网络时可以采用哪些方式防止过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- dropout随机失活，比如全连接每一个神经元都会和上一层的每一个神经元建立连接，失活其中一部分，就会使得它们看到的东西有所差异，去一定程度上防止过拟合；\n",
    "- 加正则化，和机器学习类似，也可以使用L1、L2正则化去防止过拟合；\n",
    "- 增加样本数据，样本集不足，远少于特征值，会导致严重过拟合，可以收集更多样本数据，比如上午的示例，对图片做偏移，翻转，取局部等处理，将样本数据扩大；\n",
    "- 样本均衡处理，不同类样本比例不同时，让样本均衡可以防止过拟合，比如某些类别数据少，可以重复多次使用，或者等比例采样等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  进行深度学习任务时，使用激活函数是为了解决什么问题？ 常用的激活函数有哪些？\n",
    "\n",
    "- note:重点的点没说到呢~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "在神经网络中，一个节点的激活函数，就是将对给定输入或输入集定义了一个输出。可以开关的标准电脑芯片电路可以被看做和神经网络的激活函数，这个货和神经网络中的线性感知器非常像。人工神经网络中会被称为传递函数（transfer function）。\n",
    "\n",
    "知乎或者博客好多说的比较片面，看完维基百科感觉还是英文说的比较好，翻译水平有限，翻译过来感觉特别奇怪。。。详见：https://en.wikipedia.org/wiki/Activation_function\n",
    "\n",
    "激活函数可以将一个输入转化为开关型输出，如（0，1）或（-1，1），转化为固定较为有规律的数或取值范围，比如relu\n",
    "\n",
    "常用的激活函数：\n",
    "   - sigmoid\n",
    "   - relu（包括特殊的relu，比如小于0斜率非0的）\n",
    "   - tanh\n",
    "   - softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8 . 请简要说明CNN网络的框架结构\n",
    "\n",
    "- note: input可以为conv层，? & 全连接层?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 1.input（输入层）\n",
    "- 2.conv（卷积层）\n",
    "- 3.activation（激活层）\n",
    "- 4.pooling（池化层） \n",
    "- 5.output(dense层)\n",
    "\n",
    "\n",
    "- 注：\n",
    "    - input可以为conv层，\n",
    "    - conv可以和activation放在一起作为卷积层，\n",
    "    - 卷积、池化等可以重复构建很多\n",
    "    - 可添加dropout进行随机失活\n",
    "    - 输出到dense时需要flatten展平"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.请简述应当从哪些方向上思考和解决深度学习中出现的的over fitting问题？\n",
    "\n",
    "- note: 解决办法，还有一些~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. 样本数据量 小于或者远远小于 特征数量，因为模型能力强，样本较为简略，为了匹配训练集，而做了太多特定的事情，丧失了一般性，因为样本少无法代表真实世界规律 或者 无法反映真实业务场景的各种细节。\n",
    "2. 噪声数据，同决策树采样思想类似，采取部分采样或者有放回采样，可以避免模型对噪声规律的妥协，所以可以dropout，让噪声只影响一部分神经元\n",
    "3. 样本比例不合适，比如分类问题，A类有1倍样本，B类有100倍样本，模型训练时肯定会更多以B类为参考，获得的特征或者规律肯定更偏向B类。\n",
    "\n",
    "解决办法还是：\n",
    "- dropout随机失活，比如全连接每一个神经元都会和上一层的每一个神经元建立连接，失活其中一部分，就会使得它们看到的东西有所差异，去一定程度上防止过拟合；\n",
    "- 加正则化，和机器学习类似，也可以使用L1、L2正则化去防止过拟合；\n",
    "- 增加样本数据，样本集不足，远少于特征值，会导致严重过拟合，可以收集更多样本数据，比如上午的示例，对图片做偏移，翻转，取局部等处理，将样本数据扩大；\n",
    "- 样本均衡处理，不同类样本比例不同时，让样本均衡可以防止过拟合，比如某些类别数据少，可以重复多次使用，或者等比例采样等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 谈谈您对深度学习中的自适应学习率的了解\n",
    "\n",
    "- note:自适应学习率具体针对性，写得有些泛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "    自适应学习：自适应学习通常是指给学习中提供相应的学习的环境、实例或场域，通过学习者自身在学习中发现总结，最终形成理论并能自主解决问题的学习方式。\n",
    "\n",
    "\n",
    "    自适应学习率应该是参考了人类学习的方式，根据对模型在学习中发现的问题、学习结果等进行总结，从而用形成一些比较合适的算法或者理论，来解决模型的学习问题（或者叫模型训练问题）。\n",
    "\n",
    "\n",
    "    比如SGD（随机梯度下降）是在某个梯度或者某几个梯度进行下降，来寻求最低点，但是由于串行，硬件利用率不高等缺点，所以很多人进行了改进，比如使用动量，使得它可以越过局部最低点去寻求更低点。\n",
    "    \n",
    "    \n",
    "    AdaGrad就是总结了SGD等的一些问题，借鉴L2正则化，训练过程中对梯度进行缩放，前期梯度下降学习率较大，下降速度快快，后期根据训练情况降低下降梯度，可以更稳妥地寻找相对最低点。\n",
    "    \n",
    "    \n",
    "    AdaDelta在此基础上又进行了改进，比如使用更精确的近似，使用当前t开始的前w个梯度状态的期望值，同样是基于Gradient的正则化，不过只取最近的w个状态，这样不会让梯度被惩罚至0。\n",
    "    \n",
    "    \n",
    "    AdaGrad、AdaDelta就是根据训练过程中，借鉴了一些优秀的解决问题的思想，去动态调整学习率，让模型在训练过程中对学习率进行了自适应。\n",
    "    详见：http://www.cnblogs.com/neopenx/p/4768388.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、实验题(共1题，共计40分)\n",
    "\n",
    "- note: 35  下次做记得把训练，评估，预测这些流程的都标注清楚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 请使用keras框架，构建CNN网络完成对MNIST数据集的训练，评估及预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data: \n",
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n",
      "preprocessing data...\n",
      "(60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "#加载数据\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('input data: ')\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "#input image dimensions（输入数字维度）\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "\n",
    "print('preprocessing data...')\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape) \n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train x_test: (60000, 28, 28, 1) (10000, 28, 28, 1)\n",
      "y_train y_test: (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import keras.utils\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train x_test:', x_train.shape, x_test.shape)\n",
    "\n",
    "num_classes = 10 #10个数字，10分类\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('y_train y_test:', y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "#单层卷积运行快\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#多层卷积训练慢，GPU上可跑\n",
    "# model.add(Conv2D(filters=64, kernel_size=(3,3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#显示当前模型状态\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#编译模型，设置参数\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1791 - acc: 0.9464 - val_loss: 0.0926 - val_acc: 0.9725\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1743 - acc: 0.9468 - val_loss: 0.0908 - val_acc: 0.9727\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1764 - acc: 0.9466 - val_loss: 0.0907 - val_acc: 0.9720\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1727 - acc: 0.9489 - val_loss: 0.0895 - val_acc: 0.9731\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 14s - loss: 0.1710 - acc: 0.9488 - val_loss: 0.0879 - val_acc: 0.9728\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1670 - acc: 0.9500 - val_loss: 0.0860 - val_acc: 0.9743\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1667 - acc: 0.9500 - val_loss: 0.0858 - val_acc: 0.9736\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1623 - acc: 0.9504 - val_loss: 0.0865 - val_acc: 0.9731\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1634 - acc: 0.9503 - val_loss: 0.0836 - val_acc: 0.9756\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1633 - acc: 0.9507 - val_loss: 0.0819 - val_acc: 0.9749\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1594 - acc: 0.9527 - val_loss: 0.0809 - val_acc: 0.9752\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1562 - acc: 0.9535 - val_loss: 0.0812 - val_acc: 0.9753\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1538 - acc: 0.9535 - val_loss: 0.0804 - val_acc: 0.9748\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1520 - acc: 0.9534 - val_loss: 0.0784 - val_acc: 0.9759\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1495 - acc: 0.9545 - val_loss: 0.0782 - val_acc: 0.9768\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1503 - acc: 0.9549 - val_loss: 0.0779 - val_acc: 0.9759\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1461 - acc: 0.9553 - val_loss: 0.0762 - val_acc: 0.9776\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1469 - acc: 0.9555 - val_loss: 0.0751 - val_acc: 0.9774\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1442 - acc: 0.9568 - val_loss: 0.0753 - val_acc: 0.9767\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 15s - loss: 0.1436 - acc: 0.9566 - val_loss: 0.0733 - val_acc: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6720bacc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20 #12轮, 减少轮次，较快执行完\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict_classes(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本阶段课程意见反馈题(非必答，不送分)\n",
    "#### 请同学围绕以下两点进行回答：\n",
    "- 自身总结：请您对您自己在本周课程的学习，收获，技能掌握等方面进行一次总结 ，也包括有哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 课程反馈：请就知识点，进度，难易度，教学方式，考试方式及难易度等方面向我们反馈，督促我们进行更有效的改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
