{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop经典案例：词频统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/mr-flow.png)\n",
    "我们前面提完了用hadoop完成大数据处理的一些基本知识，这次实验咱们一起来学习一下，如何编写Hadoop的map/reduce任务，完成大数据的处理。\n",
    "\n",
    "这是一个非常经典的例子，几乎在任何的hadoop教材上都会看到它，即使如此，它依旧是最经典最有代表性的案例，学习大数据处理，可以从先理解清楚它入手。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总体流程\n",
    "咱们来看看对特别大的文件统计，整个过程是如何分拆的。<br>\n",
    "大家想想词频统计的过程，如果是单机完成，我们需要做的事情是维护一个计数器字典，对每次出现的词，词频+1.但是当数据量非常大的时候，没办法在内存中维护这么大的一个字典，我们就要换一种思路来完成这个任务了，也就是我们所谓的map-reduce过程。\n",
    "\n",
    "大体的过程画成图是下面这个样子：\n",
    "![](./images/word_count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大概是分成下面几个环节：\n",
    "\n",
    "- **Map阶段**\n",
    "    - 主要完成key-value对生成，这里是每看到一个单词，就输出(单词，1)的kv对\n",
    "- **排序阶段**\n",
    "    - 对刚才的kv对进行排序，这样相同单词就在一块儿了\n",
    "- **Reduce阶段**\n",
    "    - 对同一个单词的次数进行汇总，得到(词，频次)对"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map阶段代码\n",
    "流程大家都看清楚啦，咱们来看看用代码如何实现，你猜怎么着，有了hadoop streaming，咱们可以用python脚本完成map和reduce的过程，然后把整个流程跑起来！\n",
    "\n",
    "比如咱们map阶段要做的就是把每一个单词和出现1次的信息输出来！所以我们写一个mapper.py文件，具体内容如下：\n",
    "\n",
    "```python\n",
    "#coding: utf-8\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "# 从标准输入过来的数据\n",
    "for line in sys.stdin:\n",
    "    # 把首位的空格去掉\n",
    "    line = line.strip()\n",
    "    # 把这一行文本切分成单词(按照空格)\n",
    "    words = line.split()\n",
    "    # 对见到的单词进行次数标注(出现1次)\n",
    "    for word in words:\n",
    "        print '%s\\t%s' % (word, 1)\n",
    "```\n",
    "\n",
    "对，就这么简单，你看到了，对于输入进来的每一行，我们做完切分之后，都会输出(单词，1)这样一个kv对，表明这个单词出现过。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排序阶段\n",
    "中间会有一个对上述结果进行排序的过程，以保证所有相同的单词都在一起，不过不用担心，这个过程是系统会自动完成的，因此不用我们编写额外的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce阶段\n",
    "接下来就是对map排序后的结果进行汇总了，这个阶段我们可以用一个reducer.py的python脚本来完成，具体完成的任务，就是：\n",
    "\n",
    "对于读入的(单词，1)对\n",
    "\n",
    "- 如果这个单词还没有结束（排序后所有相同的单词都在一起了），我们就对单词的次数+1\n",
    "- 如果遇到新单词了，那重新开始对新单词计数\n",
    "\n",
    "基于上面的想法，我们可以完成以下的reducer.py脚本：\n",
    "\n",
    "```python\n",
    "#coding: utf-8\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# 依旧是标准输入过来的数据\n",
    "for line in sys.stdin:\n",
    "    # 去除左右空格\n",
    "    line = line.strip()\n",
    "\n",
    "    # 按照tab键对行切分，得到word和次数1\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # 你得到的1是一个字符串，需要对它进行类型转化\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        #如果不能转成数字，输入有问题，调到下一行\n",
    "        continue\n",
    "\n",
    "    # 如果本次读取的单词和上一次一样，对次数加1\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # 输出统计结果\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# 不要忘了最后一个词哦，也得输出结果\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)\n",
    "```\n",
    "\n",
    "怎么样，2个脚本是不是很好懂？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地模拟测试代码\n",
    "一般情况下，我们不会一遍遍用hadoop streaming执行任务，去测试脚本写得对不对，这个过程太麻烦了。\n",
    "\n",
    "有没有本地可以测试的办法？有！\n",
    "\n",
    "我们可以利用linux管道模拟map-reduce的过程！比如我们可以下面这样测试：\n",
    "\n",
    "```python\n",
    "# very basic test\n",
    "user@ubuntu:~$ echo \"foo foo quux labs foo bar quux\" | /home/hduser/mapper.py\n",
    "foo     1\n",
    "foo     1\n",
    "quux    1\n",
    "labs    1\n",
    "foo     1\n",
    "bar     1\n",
    "quux    1\n",
    "\n",
    "user@ubuntu:~$ echo \"foo foo quux labs foo bar quux\" | /home/hduser/mapper.py | sort -k1,1 | /home/hduser/reducer.py\n",
    "bar     1\n",
    "foo     3\n",
    "labs    1\n",
    "quux    2\n",
    "\n",
    "# 用一本英文电子书作为输入测试一下！比如可以在http://www.gutenberg.org/etext/20417下载到！\n",
    "user@ubuntu:~$ cat /tmp/gutenberg/20417-8.txt | /home/hduser/mapper.py\n",
    " The     1\n",
    " Project 1\n",
    " Gutenberg       1\n",
    " EBook   1\n",
    " of      1\n",
    "\n",
    "# 后面的sort和reducer过程是一样的，自己试一下！\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop集群运行案例\n",
    "如果测试通过了，我们就可以在集群上运行我们的案例了，我们先从下面3个链接拉取3本电子书。\n",
    "[The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson](http://www.gutenberg.org/etext/20417)<br>\n",
    "[The Notebooks of Leonardo Da Vinci](http://www.gutenberg.org/etext/5000)<br>\n",
    "[Ulysses by James Joyce](http://www.gutenberg.org/etext/4300)<br>\n",
    "\n",
    "我们把它们下载到一个本地路径下，比如/tmp/gutenberg\n",
    "\n",
    "```\n",
    "user@ubuntu:~$ ls -l /tmp/gutenberg/\n",
    "total 3604\n",
    "-rw-r--r-- 1 hduser hadoop  674566 Feb  3 10:17 pg20417.txt\n",
    "-rw-r--r-- 1 hduser hadoop 1573112 Feb  3 10:18 pg4300.txt\n",
    "-rw-r--r-- 1 hduser hadoop 1423801 Feb  3 10:18 pg5000.txt\n",
    "user@ubuntu:~$\n",
    "```\n",
    "\n",
    "### 拷贝文件到HDFS上\n",
    "我们前面给大家准备好的HDFS命令小抄在这里派上用场了！执行下面的命令：\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -copyFromLocal /tmp/gutenberg /user/hduser/gutenberg\n",
    "hduser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls\n",
    "Found 1 items\n",
    "drwxr-xr-x   - user supergroup          0 2016-05-08 17:40 /user/hduser/gutenberg\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls /user/hduser/gutenberg\n",
    "Found 3 items\n",
    "-rw-r--r--   3 hduser supergroup     674566 2016-05-10 11:38 /user/hduser/gutenberg/pg20417.txt\n",
    "-rw-r--r--   3 hduser supergroup    1573112 2016-05-10 11:38 /user/hduser/gutenberg/pg4300.txt\n",
    "-rw-r--r--   3 hduser supergroup    1423801 2016-05-10 11:38 /user/hduser/gutenberg/pg5000.txt\n",
    "user@ubuntu:/usr/local/hadoop$\n",
    "```\n",
    "\n",
    "### 执行map-reduce任务\n",
    "下面我们就可以用hadoop streaming执行map-reduce任务了，命令行执行\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar \\\n",
    "-file /home/hduser/mapper.py    -mapper /home/hduser/mapper.py \\\n",
    "-file /home/hduser/reducer.py   -reducer /home/hduser/reducer.py \\\n",
    "-input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output\n",
    "```\n",
    "\n",
    "你甚至可以用-D去指定reducer的个数：\n",
    "\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -D mapred.reduce.tasks=16 ...\n",
    "```\n",
    "\n",
    "运行的结果过程输出的信息大概是下面这个样子：\n",
    "\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop jar contrib/streaming/hadoop-*streaming*.jar -mapper /home/hduser/mapper.py -reducer /home/hduser/reducer.py -input /user/hduser/gutenberg/* -output /user/hduser/gutenberg-output\n",
    " additionalConfSpec_:null\n",
    " null=@@@userJobConfProps_.get(stream.shipped.hadoopstreaming\n",
    " packageJobJar: [/app/hadoop/tmp/hadoop-unjar54543/]\n",
    " [] /tmp/streamjob54544.jar tmpDir=null\n",
    " [...] INFO mapred.FileInputFormat: Total input paths to process : 7\n",
    " [...] INFO streaming.StreamJob: getLocalDirs(): [/app/hadoop/tmp/mapred/local]\n",
    " [...] INFO streaming.StreamJob: Running job: job_200803031615_0021\n",
    " [...]\n",
    " [...] INFO streaming.StreamJob:  map 0%  reduce 0%\n",
    " [...] INFO streaming.StreamJob:  map 43%  reduce 0%\n",
    " [...] INFO streaming.StreamJob:  map 86%  reduce 0%\n",
    " [...] INFO streaming.StreamJob:  map 100%  reduce 0%\n",
    " [...] INFO streaming.StreamJob:  map 100%  reduce 33%\n",
    " [...] INFO streaming.StreamJob:  map 100%  reduce 70%\n",
    " [...] INFO streaming.StreamJob:  map 100%  reduce 77%\n",
    " [...] INFO streaming.StreamJob:  map 100%  reduce 100%\n",
    " [...] INFO streaming.StreamJob: Job complete: job_200803031615_0021\n",
    " [...] INFO streaming.StreamJob: Output: /user/hduser/gutenberg-output\n",
    "user@ubuntu:/usr/local/hadoop$\n",
    "```\n",
    "\n",
    "### 查看执行结果\n",
    "上面的信息告诉我们任务执行成功了，结果文件存储在hdfs上的/user/hduser/gutenberg-output目录下，我们来看一眼(是的，HDFS小抄又可以用上了)。\n",
    "\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls /user/hduser/gutenberg-output\n",
    "Found 1 items\n",
    "/user/hduser/gutenberg-output/part-00000     &lt;r 1&gt;   903193  2017-03-21 13:00\n",
    "user@ubuntu:/usr/local/hadoop$\n",
    "```\n",
    "\n",
    "还可以直接查看结果的内容：\n",
    "\n",
    "```\n",
    "user@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -cat /user/hduser/gutenberg-output/part-00000\n",
    "\"(Lo)cra\"       1\n",
    "\"1490   1\n",
    "\"1498,\" 1\n",
    "\"35\"    1\n",
    "\"40,\"   1\n",
    "\"A      2\n",
    "\"AS-IS\".        2\n",
    "\"A_     1\n",
    "\"Absoluti       1\n",
    "[...]\n",
    "user@ubuntu:/usr/local/hadoop$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
